{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Kurs\u00fcbersicht","text":""},{"location":"#kursinhalte","title":"Kursinhalte:","text":""},{"location":"#vorkurs","title":"Vorkurs","text":"Thema Beschreibung FastAPI Einf\u00fchrung in FastAPI: Erstellung von APIs und Routen. Pydantic Datenvalidierung und Modellierung mit Pydantic. SQLModel Integration von SQLModel f\u00fcr den Umgang mit Datenbanken in FastAPI. Visualisierungen und Plots \u00dcbersicht zu Grafiken und die Erstellung in Python."},{"location":"#eda","title":"EDA","text":"Thema Beschreibung Begriffe Einf\u00fchrung in FastAPI: Erstellung von APIs und Routen. Data Cleaning Datenvalidierung und Modellierung mit Pydantic. Feature Engineering Integration von SQLModel f\u00fcr den Umgang mit Datenbanken in FastAPI. Rollen in einem Data Science Projekt \u00dcbersicht zu Grafiken und die Erstellung in Python."},{"location":"#projekte","title":"Projekte:","text":"<p>Hier ist Platz f\u00fcr gemeinsame Projekte, welche in diesem Kurs bearbeitet werden:</p>"},{"location":"#projektideen","title":"Projektideen:","text":"<ul> <li>PDF Scanner</li> <li>B\u00f6rsenkurse analysieren</li> <li>Foren analysieren</li> <li>Energiedaten</li> </ul>"},{"location":"KW2/goals/","title":"Ziele","text":"<p>Ich kann...</p> <p>... erkl\u00e4ren, was ein Entscheidungsbaum ist. ... einen Entscheidungsbaum in python verwenden. ... im Ansatz erkl\u00e4ren, wie ein Entscheidungsbaum funktioniert. ... erkl\u00e4ren, wie eine lineare Regression funktioniert. ... lineare Regressionen in Python erstellen. ... den Unterschied zwischen Regression, Klassifikation und Clustering erkl\u00e4ren. ... den Unterschied zwischen \u00fcberwachtem und un\u00fcberwachtem Lernen erkl\u00e4ren. ... den Test-Trainingssplit erkl\u00e4ren. ... den Unterschied zwischen Test- und Trainingsdaten erkl\u00e4ren.</p>"},{"location":"KW2/assignments/car_features/","title":"Car Features and MSRP","text":"<p>Auf der Seite https://www.kaggle.com/datasets/CooperUnion/cardataset/data befindet sich ein Dataset \u00fcber verschiedene Automodelle und deren Eigenschaften. Wir werden diesen Datensatz nutzen, um die Themen Klassifikation, Regression und Clustering zu behandeln. Wir gehen davon aus, dass die Preise in $ vorliegen.</p>"},{"location":"KW2/assignments/car_features/#aufgabe-1-daten-verarbeiten-und-eda","title":"Aufgabe 1 - Daten verarbeiten und EDA:","text":"<ol> <li>Lade den Datensatz in Python und f\u00fchre eine erste EDA durch. Welche Features gibt es? Welche Datentypen haben die Features? Gibt es fehlende Werte? Wie ist die Verteilung der Merkmale?</li> <li>Beantworte die nachfolgenden Fragen: <ol> <li>Wie viele verschiedene Automarken gibt es in dem Datensatz?</li> <li>Wie hat sich die Popularit\u00e4t der Marke BMW im Laufe der Zeit ver\u00e4ndert?</li> <li>Wie hat sich der Durchschnittspreis von Fahrzeugen \u00fcber die Jahre entwickelt?</li> <li>Bei welchen Marktklassen sind die Preise am st\u00e4rksten gestiegen?</li> <li>Gibt es einen Zusammenhang zwischen Preis und Population?</li> <li>Definiere 3 weitere Fragen, welche du beantwortest. Bereite diese so vor, dass du sie vorstellen kannst.</li> </ol> </li> </ol>"},{"location":"KW2/assignments/car_features/#aufgabe-2-klassifikation","title":"Aufgabe 2 - Klassifikation:","text":"<p>Ziel dieser Aufgabe ist es, die Marke eines Fahrzeuges vorherzusagen. </p> <ol> <li>F\u00fchre bei bedarf ein Feature Engineering und Data Cleaning durch.</li> <li>Identifiziere Relevante Spalten f\u00fcr deine sp\u00e4tere Vorhersage.</li> <li>F\u00fchre einen Test-Train Split durch. Verwende den random_state 73 und ein Test-Train-Verh\u00e4ltnis von 80/20.</li> <li>Verwende einen Entscheidungsbaum, um die Marke eines Fahrzeuges vorherzusagen</li> <li>Bewerte dein Modell.</li> <li>Entferne die Spalte \"Popularity\" aus dem Datensatz. Wie ver\u00e4ndert sich der Score deines Modells?</li> <li>Entferne die Spalte \"Model\" und alle davon abgeleiteten Spalten. Vergleiche die Performance deiner Modelle.</li> </ol>"},{"location":"KW2/assignments/car_features/#aufgabe-3-regression","title":"Aufgabe 3 - Regression:","text":"<p>Ziel dieser Aufgabe ist es, die UVP (MRSP) eines Fahrzeuges vorherzusagen.</p> <ol> <li>F\u00fchre bei bedarf ein Feature Engineering und Data Cleaning durch.</li> <li>Identifiziere Relevante Spalten f\u00fcr deine sp\u00e4tere Vorhersage.</li> <li>F\u00fchre einen Test-Train Split durch. Verwende den randon_state 73 und ein Test-Train-Verh\u00e4ltnis von 80/20.</li> <li>Verwende Regressionsmodell, um die UVP eines Fahrzeuges vorherzusagen</li> <li>Bewerte dein Modell.</li> <li>Versuche weitere Modelle (SVR, Lineare Regression, Decision Tree Regressor)</li> </ol>"},{"location":"KW2/decision_trees/intro/","title":"Einf\u00fchrung","text":"<p>Entscheidungsb\u00e4ume \u2013 Eine Einf\u00fchrung</p> <p>Entscheidungsb\u00e4ume sind ein Werkzeug im Bereich des \u00fcberwachten maschinellen Lernens. Sie dienen dazu, Entscheidungsprozesse in einer baumartigen Struktur abzubilden. Dabei k\u00f6nnen Entscheidungsb\u00e4ume sowohl f\u00fcr Klassifikationsaufgaben, bei denen Kategorien vorhergesagt werden, als auch f\u00fcr Regressionsaufgaben, bei denen kontinuierliche Werte vorhergesagt werden, eingesetzt werden.</p> <p>Das Besondere an Entscheidungsb\u00e4umen ist ihre einfache und intuitive Struktur. Sie bestehen aus einem Stammknoten, mehreren inneren Knoten (Entscheidungsknoten) und Blattknoten (Endknoten). Jeder Knoten repr\u00e4sentiert dabei eine Frage oder Bedingung zu einem bestimmten Merkmal des Datensatzes, und die Verzweigungen geben die m\u00f6glichen Antworten auf diese Frage an. Die Blattknoten stehen schlie\u00dflich f\u00fcr die Ergebnisse oder Vorhersagen.</p> <p></p>"},{"location":"KW2/decision_trees/intro/#motivation","title":"Motivation","text":"<p>Entscheidungsb\u00e4ume bieten eine M\u00f6glichkeit, komplexe Entscheidungsprozesse auf eine klare und interpretierbare Weise abzubilden. Ihre visuelle Baumstruktur erm\u00f6glicht es, Entscheidungslogiken leicht zu verstehen und bietet Einblicke in die Daten, ohne komplexe mathematische Modelle zu fordern. Diese Flexibilit\u00e4t macht Entscheidungsb\u00e4ume zu einem beliebten Werkzeug in verschiedenen Anwendungsbereichen, von der Klassifikation bis zur Regression. Mit ihrer F\u00e4higkeit, sowohl numerische als auch kategoriale Daten zu handhaben, erfordern sie weniger Datenvorbereitungsaufwand und bieten gleichzeitig Robustheit gegen\u00fcber unterschiedlichen Skalierungen der Merkmale. Die Leichtigkeit ihrer Anpassung an nichtlineare Zusammenh\u00e4nge und ihre Effizienz bei gro\u00dfen Datens\u00e4tzen machen sie zu einer motivierenden Wahl f\u00fcr datengetriebene Entscheidungsfindung.</p>"},{"location":"KW2/decision_trees/intro/#begriffe","title":"Begriffe","text":"<p>Wurzelknoten: Der oberste Knoten im Baum, der die gesamte Bev\u00f6lkerung darstellt, die in Untergruppen aufgeteilt werden soll.</p> <p>Zweige: Ein Zweig repr\u00e4sentiert eine m\u00f6gliche Antwort auf eine Funktion.</p> <p>Blattknoten: Ein Blattknoten repr\u00e4sentiert eine Entscheidung oder ein Ergebnis. Es gibt keine untergeordneten Knoten.</p> <p>Interne Knoten: Ein interner Knoten repr\u00e4sentiert eine Funktion, die eine Entscheidung trifft, wo der n\u00e4chste Knoten zu finden ist.</p> <p>Tiefe des Baumes: Die L\u00e4nge des l\u00e4ngsten Pfades von der Wurzel bis zu einem Blatt.</p>"},{"location":"KW2/decision_trees/intro/#lernprozess","title":"Lernprozess","text":"<p>Der Lernprozess von Entscheidungsb\u00e4umen basiert auf einer \"Teile-und-Herrsche-Strategie\", die durch eine \"gierige\" Suche optimale Teilungspunkte innerhalb des Baums ermittelt. Diese Aufteilung erfolgt rekursiv von oben nach unten, bis die meisten Datenpunkte bestimmten Klassen zugeordnet sind. Die Komplexit\u00e4t des Baums beeinflusst, ob alle Datenpunkte homogene Mengen oder gemischte Klassen repr\u00e4sentieren. Kleine B\u00e4ume neigen dazu, reine Blattknoten zu erreichen, w\u00e4hrend gr\u00f6\u00dfere B\u00e4ume zur Fragmentierung und m\u00f6glicher \u00dcberanpassung neigen. Entsprechend dem Prinzip der Sparsamkeit in Occams Razor sollten Entscheidungsb\u00e4ume die Komplexit\u00e4t nur dann erh\u00f6hen, wenn n\u00f6tig, da die einfachste Erkl\u00e4rung oft die beste ist.</p> <p>Um \u00dcberanpassung zu vermeiden, wird \u00fcblicherweise \"Zurechtschneiden\" (Pruning) angewendet, indem unwichtige Zweige entfernt werden. Die Modellbewertung erfolgt oft durch Kreuzvalidierung. Ein weiterer Ansatz zur Erhaltung der Genauigkeit ist die Verwendung von Ensemble-Methoden wie dem Random-Forest-Algorithmus, der pr\u00e4zisere Vorhersagen erm\u00f6glicht, insbesondere wenn die einzelnen B\u00e4ume unkorreliert sind. Dieser Ansatz unterstreicht die Vielseitigkeit und Anpassungsf\u00e4higkeit von Entscheidungsb\u00e4umen in datengetriebenen Entscheidungsfindungsprozessen.</p>"},{"location":"KW2/decision_trees/intro/#wie-funktionieren-entscheidungsbaume","title":"Wie funktionieren Entscheidungsb\u00e4ume?","text":"<p>Entscheidungsb\u00e4ume nutzen das Prinzip \u201eTeile und herrsche\u201c. Sie zerlegen die Daten schrittweise in immer kleinere Teilmengen, bis diese m\u00f6glichst homogen sind. Die grundlegende Idee ist es, bei jedem Schritt das Merkmal auszuw\u00e4hlen, das die Daten am besten in Klassen oder Wertebereiche trennt. Diese Trennung erfolgt durch sogenannte Split-Kriterien, wie etwa:</p> <ul> <li>Gini-Index: Misst die \u201eUnreinheit\u201c eines Knotens.</li> <li>Informationsgewinn: Bewertet, wie viel Unsicherheit durch den Split reduziert wird.</li> <li>Varianzreduktion: Wird bei Regressionsb\u00e4umen verwendet, um die Streuung der Zielvariablen zu minimieren.</li> </ul> <p>Ein Entscheidungsbaum startet immer beim Stammknoten, der alle Daten enth\u00e4lt, und spaltet diese rekursiv auf. Der Algorithmus stoppt, wenn entweder alle Knoten homogen sind oder eine festgelegte Bedingung erreicht ist, wie die maximale Tiefe des Baums oder eine minimale Anzahl von Datenpunkten pro Knoten.</p>"},{"location":"KW2/decision_trees/intro/#ein-kleines-beispiel","title":"Ein kleines Beispiel","text":"<p>Angenommen, wir haben einen Datensatz, der die folgenden Merkmale enth\u00e4lt: - Wetter (sonnig, bew\u00f6lkt, regnerisch) - Temperatur (hoch, mittel, niedrig) - Luftfeuchtigkeit (hoch, niedrig) - Wind (stark, schwach)</p> <p>Die Zielvariable ist, ob wir Fu\u00dfball spielen (\u201eja\u201c oder \u201enein\u201c). Ein Entscheidungsbaum k\u00f6nnte wie folgt aufgebaut sein:</p> <ol> <li>Der Stammknoten entscheidet zuerst, ob das Wetter \u201esonnig\u201c, \u201ebew\u00f6lkt\u201c oder \u201eregnerisch\u201c ist.</li> <li>Wenn es \u201esonnig\u201c ist, wird der n\u00e4chste Knoten nach der Luftfeuchtigkeit gefragt.</li> <li>Wenn es \u201eregnerisch\u201c ist, entscheidet der Baum anhand der Windst\u00e4rke.</li> <li>Die Bl\u00e4tter repr\u00e4sentieren die Entscheidungen: \u201eja\u201c oder \u201enein\u201c.</li> </ol> <p>Das Entscheidungsverfahren k\u00f6nnte dann etwa lauten: \u201eWenn das Wetter regnerisch und der Wind stark ist, spielen wir nicht. Wenn es bew\u00f6lkt ist, spielen wir immer.\u201c</p>"},{"location":"KW2/decision_trees/intro/#vorteile-und-nachteile-von-entscheidungsbaumen","title":"Vorteile und Nachteile von Entscheidungsb\u00e4umen","text":""},{"location":"KW2/decision_trees/intro/#vorteile","title":"Vorteile:","text":"<ul> <li>Einfach und intuitiv: Die Struktur eines Entscheidungsbaums ist leicht verst\u00e4ndlich, auch f\u00fcr Personen ohne tiefes technisches Wissen.</li> <li>Transparenz: Entscheidungsprozesse k\u00f6nnen klar nachvollzogen werden, da jede Entscheidung im Baum logisch dargestellt ist.</li> <li>Flexibilit\u00e4t: Entscheidungsb\u00e4ume k\u00f6nnen sowohl f\u00fcr Klassifikations- als auch f\u00fcr Regressionsprobleme verwendet werden.</li> </ul>"},{"location":"KW2/decision_trees/intro/#nachteile","title":"Nachteile:","text":"<ul> <li>\u00dcberanpassung (Overfitting): Entscheidungsb\u00e4ume neigen dazu, sich stark an die Trainingsdaten anzupassen, insbesondere wenn sie sehr tief werden.</li> <li>Instabilit\u00e4t: Kleine \u00c4nderungen in den Daten k\u00f6nnen gro\u00dfe Auswirkungen auf die Baumstruktur haben.</li> <li>Begrenzte Genauigkeit: Einzelne Entscheidungsb\u00e4ume k\u00f6nnen im Vergleich zu anderen Modellen, wie z.\u202fB. Random Forests, weniger genau sein.</li> </ul>"},{"location":"KW2/decision_trees/intro/#anwendungsbeispiel-in-python","title":"Anwendungsbeispiel in Python","text":"<p>Hier ein Beispiel, wie ein Entscheidungsbaum mit dem Iris-Datensatz trainiert und visualisiert werden kann:</p> <pre><code>from sklearn.datasets import load_iris\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree\nimport matplotlib.pyplot as plt\n\n# Datensatz laden\niris = load_iris()\nX, y = iris.data, iris.target\n\n# Entscheidungsbaum-Klassifikator erstellen\nclf = DecisionTreeClassifier(max_depth=3, random_state=42)\nclf.fit(X, y)\n\n# Entscheidungsbaum visualisieren\nplt.figure(figsize=(12, 8))\ntree.plot_tree(clf, feature_names=iris.feature_names, class_names=iris.target_names, filled=True)\nplt.show()\n</code></pre> <p>In diesem Beispiel wird ein Entscheidungsbaum erstellt, der die Blumenarten des Iris-Datensatzes klassifiziert. Die Visualisierung zeigt, welche Merkmale (z.\u202fB. \u201epetal length\u201c) und Schwellenwerte f\u00fcr die Entscheidungen im Baum herangezogen werden.</p>"},{"location":"KW2/decision_trees/intro/#interpretation-der-visualisierung","title":"Interpretation der Visualisierung","text":"<ul> <li>Jeder Knoten zeigt das Merkmal und den Schwellenwert, der f\u00fcr den Split verwendet wurde.</li> <li>Unterhalb der Entscheidung stehen die Anzahl der Datenpunkte in diesem Knoten und die Verteilung auf die Klassen.</li> <li>Die Bl\u00e4tter stellen die Vorhersage dar (z.\u202fB. \u201eSetosa\u201c).</li> </ul>"},{"location":"KW2/decision_trees/intro/#optimierung-eines-entscheidungsbaums","title":"Optimierung eines Entscheidungsbaums","text":"<p>Um die Performance eines Entscheidungsbaums zu verbessern und \u00dcberanpassung zu vermeiden, gibt es mehrere Strategien:</p> <ul> <li>Pruning (Beschneiden): Reduziert die Baumtiefe, indem unwichtige \u00c4ste entfernt werden.</li> <li>Maximale Tiefe begrenzen: Setzt eine Obergrenze f\u00fcr die Anzahl der Schichten im Baum.</li> <li>Minimale Anzahl an Datenpunkten pro Knoten: Verhindert, dass der Baum zu stark spezialisiert wird.</li> </ul> <p>Hier ein Beispiel f\u00fcr das Beschneiden durch Begrenzung der maximalen Tiefe:</p> <pre><code>clf_pruned = DecisionTreeClassifier(max_depth=2, random_state=42)\nclf_pruned.fit(X, y)\n\nplt.figure(figsize=(12, 8))\ntree.plot_tree(clf_pruned, feature_names=iris.feature_names, class_names=iris.target_names, filled=True)\nplt.show()\n</code></pre> <p>In diesem Fall ist der Baum weniger tief, was ihn weniger anf\u00e4llig f\u00fcr \u00dcberanpassung macht.</p>"},{"location":"KW2/decision_trees/random_forest/","title":"Noch Exkurs: Random Forests","text":"<p>Random Forests sind eine Ensemble-Lernmethode im Bereich des maschinellen Lernens. Sie geh\u00f6ren zur Familie der Entscheidungsb\u00e4ume und werden f\u00fcr Klassifikationsund Regressionsaufgaben verwendet. Der Hauptgedanke hinter Random Forests ist, mehrere Entscheidungsb\u00e4ume zu erstellen und ihre Vorhersagen zu kombinieren, um robustere und genaue Modelle zu erhalten. Einen \u00dcberblick zu Entscheidungsb\u00e4umen findest du hier.</p> <p></p>"},{"location":"KW2/decision_trees/random_forest/#erstellung-eines-random-forests","title":"Erstellung eines Random Forests","text":"<p>Ein Random Forest besteht aus einer gro\u00dfen Anzahl von Entscheidungsb\u00e4umen. Jeder dieser B\u00e4ume ist ein Klassifikator, der eine Entschiedung f\u00fcr eine Teilmeng der Ausgangsklassen trifft. Die Vorhersage des Random Forests ist die Klasse, die von den meisten B\u00e4umen vorhergesagt wird. Die Vorhersage eines einzelnen Baumes ist m\u00f6glicherweise nicht sehr genau, aber wenn wir die Vorhersagen vieler B\u00e4ume kombinieren, erhalten wir eine Vorhersage, die genauer ist und weniger anf\u00e4llig f\u00fcr Rauschen ist. Die Erstellung eines Random Forests kann in mehrere Schritte aufgeteilt werden.</p> <ol> <li> <p>Datens\u00e4tze ausw\u00e4hlen (Bootstrap Aggregating - Bagging):    F\u00fcr jeden Baum aus dem Trainingsdatensatz werden zuf\u00e4llige Stichproben (mit Wiederholung) erstellt. Dies bedeutet, dass einige Datenpunkte mehrfach in einer Stichprobe auftreten k\u00f6nnen, w\u00e4hrend andere m\u00f6glicherweise \u00fcberhaupt nicht enthalten sind.</p> </li> <li> <p>Entscheidungsbaum erstellen:    F\u00fcr jede erstellte Stichprobe wird ein Entscheidungsbaum trainiert. Bei jedem Schritt des Baumwachstums wird nur eine zuf\u00e4llige Teilmenge der Features ber\u00fccksichtigt, um die Unabh\u00e4ngigkeit der B\u00e4ume sicherzustellen.</p> </li> <li> <p>Ensemble bilden:    Die erstellten Entscheidungsb\u00e4ume bilden das Ensemble des Random Forests.</p> </li> <li> <p>Vorhersage erstellen:    F\u00fcr eine neue Instanz wird die Vorhersage durch Aggregation der Vorhersagen aller B\u00e4ume erstellt. Bei Klassifikationsaufgaben erfolgt dies oft durch Mehrheitsabstimmung, und bei Regressionsaufgaben wird der Durchschnitt der Vorhersagen genommen.</p> </li> <li> <p>Out-of-Bag (OOB) Bewertung:    Da verschiedene B\u00e4ume verschiedene Teilmengen der Daten sehen, k\u00f6nnen Datenpunkte, die in einem Baum nicht verwendet wurden, f\u00fcr die Bewertung der Modellgenauigkeit verwendet werden. Diese Technik wird als Out-of-Bag (OOB) Bewertung bezeichnet.</p> </li> <li> <p>Feature Importance:    Random Forests bieten eine M\u00f6glichkeit zur Berechnung der Bedeutung einzelner Features. Dies geschieht durch die Analyse, wie sehr sich die Genauigkeit des Modells \u00e4ndert, wenn die Werte eines Features permutiert werden.</p> </li> <li> <p>Anzahl der B\u00e4ume und Hyperparameter:    Die Anzahl der B\u00e4ume (Estimators) und andere Hyperparameter des Random Forests k\u00f6nnen je nach Anwendungsfall angepasst werden, um die Modellleistung zu optimieren.</p> </li> </ol> <p>Der Schl\u00fcsselaspekt eines Random Forests ist die Randomisierung sowohl in Bezug auf die Daten (durch Bagging) als auch in Bezug auf die Features (durch zuf\u00e4llige Auswahl von Features bei jedem Split des Entscheidungsbaums). Diese Randomisierung tr\u00e4gt dazu bei, Overfitting zu reduzieren und die Robustheit des Modells zu verbessern.</p>"},{"location":"KW2/decision_trees/random_forest/#exkurs-bootstrapping","title":"Exkurs: Bootstrapping","text":"<p>Bootstrapping ist eine statistische Methode, die dazu dient, Sch\u00e4tzungen der Verteilung von Sch\u00e4tzern zu erstellen. Der Begriff \"Bootstrapping\" leitet sich von der Vorstellung ab, dass man sich an den eigenen Stiefelriemen zieht, was darauf hinweist, dass man aus den vorhandenen Daten selbst weitere Stichproben erstellt.</p> <p>Der Bootstrapping-Prozess erfolgt in folgenden Schritten:</p> <ol> <li> <p>Stichprobe erstellen:    Aus einem vorhandenen Datensatz wird eine zuf\u00e4llige Stichprobe mit Wiederholung gezogen. Das bedeutet, dass bestimmte Datenpunkte mehrmals in der Stichprobe erscheinen k\u00f6nnen, w\u00e4hrend andere m\u00f6glicherweise \u00fcberhaupt nicht ausgew\u00e4hlt werden.</p> </li> <li> <p>Sch\u00e4tzung berechnen:    Mit der erstellten Stichprobe wird eine Sch\u00e4tzung (z. B. Mittelwert, Median, Standardabweichung) des interessierenden Parameters berechnet.</p> </li> <li> <p>Schritte wiederholen:    Die Schritte 1 und 2 werden viele Male wiederholt (typischerweise Tausende von Malen), um eine Verteilung der Sch\u00e4tzungen zu erstellen.</p> </li> <li> <p>Verteilung analysieren:    Durch Analyse der Verteilung der Sch\u00e4tzungen k\u00f6nnen Konfidenzintervalle und andere statistische Eigenschaften des gesch\u00e4tzten Parameters abgeleitet werden.</p> </li> </ol> <p>Bootstrapping ist besonders n\u00fctzlich, wenn die analytischen L\u00f6sungen kompliziert oder nicht verf\u00fcgbar sind. Es erm\u00f6glicht auch, die Unsicherheit von Sch\u00e4tzern zu quantifizieren, indem man mehrere Stichproben aus denselben Daten erstellt. Bootstrapping wird in verschiedenen Bereichen der Statistik und des maschinellen Lernens verwendet, um bessere Sch\u00e4tzungen von Parametern und deren Unsicherheiten zu erhalten.</p> <p>Beispiel:</p> <pre><code>from sklearn.utils import resample\nfrom sklearn.datasets import load_iris\n\n# Lade den Iris-Datensatz f\u00fcr das Beispiel\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# Beispiel f\u00fcr Bootstrapping\ndef bootstrap_example(data):\n    # Erzeuge eine Bootstrap-Stichprobe\n    bootstrap_sample = resample(data, replace=True, n_samples=len(data))\n    print(\"Originaldaten:\", data[:5])\n    print(\"Bootstrapped Stichprobe:\", bootstrap_sample[:5])\n\n# Bootstrapping-Beispiel\nprint(\"Bootstrapping-Beispiel:\")\nbootstrap_example(X)\n</code></pre>"},{"location":"KW2/decision_trees/random_forest/#exkurs-baggingbootstrap-aggregating","title":"Exkurs: Bagging(Bootstrap Aggregating)","text":"<p>Bagging (Bootstrap Aggregating) ist eine Ensemble-Lernmethode im Bereich des maschinellen Lernens. Es basiert auf dem Prinzip des Bootstrapping und wird verwendet, um die Vorhersagegenauigkeit und Stabilit\u00e4t von Modellen zu verbessern, insbesondere von Entscheidungsb\u00e4umen.</p> <p>Hier sind die Grundprinzipien des Bagging:</p> <ol> <li> <p>Bootstrapping:    Es werden zuf\u00e4llige Stichproben (mit Wiederholung) aus dem Trainingsdatensatz gezogen, um mehrere \"bootstrapped\" Datens\u00e4tze zu erstellen.</p> </li> <li> <p>Modelle erstellen:    Auf jedem der bootstrapped Datens\u00e4tze wird ein separates Modell (typischerweise ein Entscheidungsbaum) trainiert. Da die Datens\u00e4tze unterschiedlich sind, werden auch die erstellten Modelle unterschiedlich sein.</p> </li> <li> <p>Aggregation der Vorhersagen:    Die Vorhersagen der einzelnen Modelle werden kombiniert, um eine Gesamtvorhersage zu erstellen. Bei Klassifikationsaufgaben geschieht dies oft durch Mehrheitsabstimmung, und bei Regressionsaufgaben wird der Durchschnitt der Vorhersagen genommen.</p> </li> </ol> <p>Der Hauptvorteil von Bagging liegt darin, dass es die Varianz des Modells reduziert, indem es die Modelle auf unterschiedlichen Daten trainiert und dann ihre Vorhersagen aggregiert. Dies hilft, Overfitting zu minimieren und die Modellleistung auf neuen Daten zu verbessern.</p> <p>Random Forests, einer der bekanntesten Anwendungsbereiche von Bagging, verwenden diese Methode, indem sie mehrere Entscheidungsb\u00e4ume auf unterschiedlichen Teilmengen des Datensatzes trainieren und deren Vorhersagen kombinieren. Bagging kann jedoch auch mit anderen Basisalgorithmen verwendet werden.</p> <p>Beispiel:</p> <pre><code>from sklearn.ensemble import BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# Lade den Iris-Datensatz f\u00fcr das Beispiel\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# Beispiel f\u00fcr Bagging mit Entscheidungsbaum\ndef bagging_example(X_train, X_test, y_train, y_test):\n    # Erzeuge Entscheidungsbaum-Modell\n    base_model = DecisionTreeClassifier(random_state=42)\n\n    # Erzeuge Bagging-Modell\n    bagging_model = BaggingClassifier(base_model, n_estimators=10, random_state=42)\n\n    # Trainiere das Bagging-Modell\n    bagging_model.fit(X_train, y_train)\n\n    # Vorhersagen\n    predictions = bagging_model.predict(X_test)\n\n    # Auswertung der Genauigkeit\n    accuracy = accuracy_score(y_test, predictions)\n    print(\"Bagging-Modell Genauigkeit:\", accuracy)\n\n# Aufteilung der Daten in Trainings- und Testsets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Bagging-Beispiel mit Entscheidungsbaum\nprint(\"\\nBagging-Beispiel:\")\nbagging_example(X_train, X_test, y_train, y_test)\n</code></pre>"},{"location":"KW2/decision_trees/random_forest/#kombination-von-entscheidungsbaumen","title":"Kombination von Entscheidungsb\u00e4umen","text":"<p>Bei einem Random Forest gibt es verschiedene M\u00f6glichkeiten, die Vorhersagen der einzelnen B\u00e4ume zu kombinieren. Diese Kombinationsstrategien sind wichtig, um eine robuste und genaue Gesamtvorhersage zu erstellen. Hier sind die Hauptmethoden:</p> <ol> <li> <p>Mehrheitsabstimmung (Classification):    In Klassifikationsaufgaben wird die Klasse ausgew\u00e4hlt, die von den meisten B\u00e4umen vorhergesagt wird. Dies wird als Mehrheitsabstimmung oder Mehrheitsregel bezeichnet. Die Klasse mit den meisten Stimmen wird als endg\u00fcltige Vorhersage verwendet.</p> </li> <li> <p>Durchschnitt (Regression):    In Regressionsaufgaben wird der Durchschnitt der Vorhersagen aller B\u00e4ume genommen. Dies f\u00fchrt zu einer stabilen und konsistenten Vorhersage, die weniger anf\u00e4llig f\u00fcr Ausrei\u00dfer ist.</p> </li> <li> <p>Gewichtete Kombination:    Es besteht die M\u00f6glichkeit, den Vorhersagen der B\u00e4ume Gewichte zuzuweisen. B\u00e4ume mit h\u00f6herer Genauigkeit oder Vertrauensw\u00fcrdigkeit k\u00f6nnen h\u00f6here Gewichte erhalten. Dies erm\u00f6glicht eine feinere Steuerung \u00fcber den Einfluss einzelner B\u00e4ume.</p> </li> <li> <p>Soft Voting (Classification):    Anstelle einer harten Mehrheitsabstimmung, bei der einfach die Klasse mit den meisten Stimmen ausgew\u00e4hlt wird, kann auch eine weiche Mehrheitsabstimmung durchgef\u00fchrt werden. Dabei werden die Wahrscheinlichkeiten f\u00fcr jede Klasse aus den B\u00e4umen aggregiert, und die Klasse mit der h\u00f6chsten aggregierten Wahrscheinlichkeit wird ausgew\u00e4hlt.</p> </li> <li> <p>Out-of-Bag (OOB) Sch\u00e4tzung:    Bei der Trainingsphase des Random Forests wird f\u00fcr jeden Baum eine OOB-Sch\u00e4tzung erstellt, indem der Baum auf den nicht f\u00fcr die Stichprobe ausgew\u00e4hlten Daten getestet wird. Die Vorhersagen aller B\u00e4ume werden dann kombiniert, um eine Gesamtsch\u00e4tzung zu erhalten.</p> </li> </ol> <p>Die Wahl der Kombinationsstrategie h\u00e4ngt von der Art des Problems (Klassifikation oder Regression) und dem Anwendungsfall ab. In der Praxis hat sich die Mehrheitsabstimmung als effektive Methode f\u00fcr Klassifikationsprobleme und der Durchschnitt f\u00fcr Regressionsprobleme bew\u00e4hrt.</p>"},{"location":"KW2/decision_trees/random_forest/#random-forest-vs-decision-tree","title":"Random Forest vs Decision Tree","text":"<p>Die Wahl zwischen einem Random Forest und einem einzelnen Entscheidungsbaum h\u00e4ngt von den Anforderungen des Problems, der Gr\u00f6\u00dfe des Datensatzes und den verf\u00fcgbaren Rechenressourcen ab. In vielen F\u00e4llen bieten Random Forests jedoch eine robuste L\u00f6sung mit verbesserter Vorhersagegenauigkeit.</p> <p>Vorteile von Random Forests gegen\u00fcber Entscheidungsb\u00e4umen:</p> <ol> <li> <p>Reduziert Overfitting:    Random Forests sind weniger anf\u00e4llig f\u00fcr Overfitting im Vergleich zu einzelnen Entscheidungsb\u00e4umen. Die Randomisierung bei der Erstellung von Teilb\u00e4umen tr\u00e4gt dazu bei, dass die Gesamtvorhersage des Modells robuster und besser verallgemeinerungsf\u00e4hig ist.</p> </li> <li> <p>H\u00f6here Genauigkeit:    In der Regel liefern Random Forests aufgrund ihrer Ensemble-Natur eine h\u00f6here Genauigkeit bei Vorhersagen. Indem sie mehrere Entscheidungsb\u00e4ume kombinieren, reduzieren sie die Varianz und verbessern die Vorhersageleistung.</p> </li> <li> <p>Bessere Generalisierung:    Durch die Verwendung verschiedener Teilmengen der Daten und Features bei jedem Baum verbessern Random Forests die Generalisierungsf\u00e4higkeit des Modells. Dies ist besonders wichtig, wenn der Trainingsdatensatz begrenzt ist.</p> </li> <li> <p>Automatische Feature-Selektion:    Random Forests f\u00fchren eine automatische Feature-Selektion durch, indem sie die Wichtigkeit der Features bewerten. Dies kann dazu beitragen, irrelevante oder redundante Features zu eliminieren.</p> </li> <li> <p>Out-of-Bag (OOB) Sch\u00e4tzung:    Random Forests erm\u00f6glichen die Verwendung von Out-of-Bag-Sch\u00e4tzungen, bei denen jedes Beispiel durchschnittlich etwa in einem Drittel der B\u00e4ume ausgelassen wird. Dies bietet eine interne Sch\u00e4tzung der Modellgenauigkeit.</p> </li> </ol> <p>Nachteile von Random Forests gegen\u00fcber Entscheidungsb\u00e4umen:</p> <ol> <li> <p>Komplexit\u00e4t und Rechenressourcen:    Random Forests sind aufgrund der Notwendigkeit, mehrere B\u00e4ume zu trainieren und zu kombinieren, rechenintensiver und komplexer als einzelne Entscheidungsb\u00e4ume. Dies kann zu l\u00e4ngeren Trainingszeiten f\u00fchren.</p> </li> <li> <p>Schwer interpretierbar:    Die Kombination mehrerer B\u00e4ume macht Random Forests schwerer interpretierbar im Vergleich zu einzelnen Entscheidungsb\u00e4umen. Es ist schwieriger, den genauen Einfluss jedes Features auf die Vorhersagen zu verstehen.</p> </li> <li> <p>Overhead durch Randomisierung:    In einigen F\u00e4llen kann die Randomisierung zu einem gewissen Overhead f\u00fchren, insbesondere wenn der Datensatz klein ist oder die Features nicht stark differenzierend sind. In solchen F\u00e4llen kann ein einfacher Entscheidungsbaum m\u00f6glicherweise effizienter sein.</p> </li> <li> <p>Unn\u00f6tig f\u00fcr klare Zusammenh\u00e4nge:    Wenn die zugrundeliegenden Zusammenh\u00e4nge im Datensatz bereits einfach und klar sind, kann die Komplexit\u00e4t von Random Forests unn\u00f6tig sein. In solchen F\u00e4llen k\u00f6nnte ein einfacher Entscheidungsbaum ausreichen.</p> </li> </ol>"},{"location":"KW2/decision_trees/random_forest/#referenzen-und-nachschlagewerke","title":"Referenzen und Nachschlagewerke","text":"<ul> <li>Listandata.com</li> <li>IMB</li> <li>Youtube: Visual Guide to Random Forests</li> </ul>"},{"location":"KW2/decision_trees/split-criteria/","title":"Split Kriterium","text":""},{"location":"KW2/decision_trees/split-criteria/#split-kriterien-in-entscheidungsbaumen","title":"Split-Kriterien in Entscheidungsb\u00e4umen","text":"<p>Ein zentraler Aspekt bei der Konstruktion eines Entscheidungsbaums ist die Auswahl des besten Merkmals (Spalte) und des besten Schwellenwerts, um die Daten an jedem Knoten aufzuteilen. Dieses Verfahren wird durch Split-Kriterien gesteuert. Ein gutes Split-Kriterium sorgt daf\u00fcr, dass die resultierenden Untergruppen m\u00f6glichst \u201erein\u201c sind, also nur wenige Klassenmischungen oder Varianzen aufweisen. Im Folgenden werden die g\u00e4ngigsten Split-Kriterien und ihre Bedeutung erkl\u00e4rt:</p>"},{"location":"KW2/decision_trees/split-criteria/#1-informationsgewinn-information-gain-entropieverlust","title":"1. Informationsgewinn (Information Gain)/ Entropieverlust","text":"<p>Der Informationsgewinn misst, wie viel Unsicherheit (Entropie) durch eine Aufteilung der Daten reduziert wird. Die Entropie beschreibt dabei die Unordnung oder Unsicherheit in einer Menge. Ziel ist es, die Entropie nach einem Split m\u00f6glichst gering zu halten, also eine maximale Homogenit\u00e4t in den Untergruppen zu erreichen.</p> <p>Formel f\u00fcr den Informationsgewinn: $$ \\text{Informationsgewinn} = \\text{Entropie}(D) - \\sum_{i=1}^{k} \\frac{|D_i|}{|D|} \\cdot \\text{Entropie}(D_i) $$</p> <ul> <li>\\( D \\): Die Datenmenge vor dem Split.  </li> <li>\\( D_i \\): Die Untergruppen nach dem Split.  </li> <li>\\( k \\): Anzahl der Untergruppen.</li> </ul> <p>Definition der Entropie</p> <p>Die Entropie ist ein Ma\u00df f\u00fcr die Unordnung oder Unsicherheit in einem Datensatz. Sie stammt aus der Informationstheorie und beschreibt, wie viel Information ben\u00f6tigt wird, um die Klasse eines Elements korrekt vorherzusagen. Ein Datensatz mit hoher Entropie ist stark gemischt, w\u00e4hrend ein Datensatz mit niedriger Entropie sehr homogen ist.</p> <p>Die Entropie wird wie folgt berechnet:</p> \\[ H(D) = - \\sum_{i=1}^{n} p_i \\cdot \\log_2(p_i) \\] <ul> <li>\\( H(D) \\): Entropie des Datensatzes \\( D \\) </li> <li>\\( n \\): Anzahl der Klassen  </li> <li>\\( p_i \\): Wahrscheinlichkeit der Klasse \\( i \\) im Datensatz (Anteil der Klasse \\( i \\) an den Gesamtbeispielen)  </li> </ul> <p>Die Entropie erreicht: - 0, wenn alle Beispiele zu einer einzigen Klasse geh\u00f6ren (maximale Homogenit\u00e4t). - 1, wenn alle Klassen gleichm\u00e4\u00dfig verteilt sind (maximale Unordnung).  </p>"},{"location":"KW2/decision_trees/split-criteria/#beispiel","title":"Beispiel","text":"<p>Angenommen, wir haben einen Datensatz mit 10 Eintr\u00e4gen, die auf zwei Klassen verteilt sind: - Klasse A: 6 Eintr\u00e4ge - Klasse B: 4 Eintr\u00e4ge  </p> <p>Die Wahrscheinlichkeiten sind: $$ p_A = \\frac{6}{10} = 0.6, \\quad p_B = \\frac{4}{10} = 0.4 $$</p> <p>Die Entropie berechnet sich dann wie folgt: $$ H(D) = - (0.6 \\cdot \\log_2(0.6) + 0.4 \\cdot \\log_2(0.4)) $$</p> <p>Die Logarithmen k\u00f6nnen n\u00e4herungsweise berechnet werden: $$ \\log_2(0.6) \\approx -0.51, \\quad \\log_2(0.4) \\approx -0.92 $$</p> <p>Eingesetzt ergibt sich: $$ H(D) = - (0.6 \\cdot -0.51 + 0.4 \\cdot -0.92) \\approx 0.67 $$</p> <p>Die Entropie des Datensatzes betr\u00e4gt also 0.67, was darauf hinweist, dass die Daten m\u00e4\u00dfig gemischt sind.</p> <p>Beispiel in Python: Hier wird demonstriert, wie der Informationsgewinn bei einer bin\u00e4ren Aufteilung berechnet werden kann.</p> <pre><code>from math import log2\n\n# Funktion zur Berechnung der Entropie\ndef entropy(classes):\n    total = sum(classes)\n    return -sum((count / total) * log2(count / total) for count in classes if count &gt; 0)\n\n# Beispiel: Vor dem Split\nclasses_before = [30, 20]  # 30 positive, 20 negative Beispiele\nentropy_before = entropy(classes_before)\n\n# Beispiel: Nach dem Split\nsplit_classes = [[20, 10], [10, 10]]  # Zwei Gruppen nach dem Split\nentropy_after = sum((sum(group) / sum(classes_before)) * entropy(group) for group in split_classes)\n\n# Informationsgewinn\ninfo_gain = entropy_before - entropy_after\nprint(\"Informationsgewinn:\", info_gain)\n</code></pre>"},{"location":"KW2/decision_trees/split-criteria/#2-gini-index","title":"2. Gini-Index","text":"<p>Der Gini-Index misst die Wahrscheinlichkeit, dass ein zuf\u00e4llig ausgew\u00e4hltes Element falsch klassifiziert wird, wenn es entsprechend der Verteilung der Datenpunkte im Knoten klassifiziert wird. Ein niedriger Gini-Index deutet auf eine hohe Reinheit hin.</p> <p>Formel f\u00fcr den Gini-Index: $$ \\text{Gini-Index} = 1 - \\sum_{i=1}^{n} (p_i)^2 $$</p> <ul> <li>\\( p_i \\): Anteil der Klasse \\( i \\) an den Daten.  </li> </ul> <p>Beispiel in Python: Berechnung des Gini-Index f\u00fcr eine einzelne Gruppe:</p> <pre><code>def gini(classes):\n    total = sum(classes)\n    return 1 - sum((count / total) ** 2 for count in classes)\n\n# Beispiel: Daten vor und nach einem Split\nclasses_before = [30, 20]\ngini_before = gini(classes_before)\n\nsplit_classes = [[20, 10], [10, 10]]\ngini_after = sum((sum(group) / sum(classes_before)) * gini(group) for group in split_classes)\n\nprint(\"Gini vor dem Split:\", gini_before)\nprint(\"Gini nach dem Split:\", gini_after)\n</code></pre> <p>Der Gini-Index wird in der Praxis h\u00e4ufig verwendet, da er einfacher zu berechnen ist als der Informationsgewinn und in vielen Szenarien \u00e4hnliche Ergebnisse liefert.</p>"},{"location":"KW2/decision_trees/split-criteria/#3-varianzreduktion","title":"3. Varianzreduktion","text":"<p>Die Varianzreduktion ist ein Split-Kriterium, das vor allem bei Regressionsb\u00e4umen verwendet wird. Es misst, wie stark die Streuung der Zielvariablen durch den Split reduziert wird. Ziel ist es, die Varianz der Zielwerte in den Untergruppen so gering wie m\u00f6glich zu halten.</p> <p>Formel f\u00fcr die Varianzreduktion: $$ \\text{Varianzreduktion} = \\text{Varianz}(D) - \\sum_{i=1}^{k} \\frac{|D_i|}{|D|} \\cdot \\text{Varianz}(D_i) $$</p> <ul> <li>\\( \\text{Varianz}(D) \\): Streuung der Zielwerte in den urspr\u00fcnglichen Daten.  </li> </ul> <p>Beispiel in Python: Hier wird demonstriert, wie die Varianzreduktion berechnet werden kann.</p> <pre><code>import numpy as np\n\n# Funktion zur Berechnung der Varianz\ndef variance(data):\n    return np.var(data)\n\n# Zielwerte vor dem Split\nvalues_before = [2, 3, 4, 5, 6, 7]\nvariance_before = variance(values_before)\n\n# Zielwerte nach dem Split\nsplit_values = [[2, 3, 4], [5, 6, 7]]\nweighted_variance = sum((len(group) / len(values_before)) * variance(group) for group in split_values)\n\nvariance_reduction = variance_before - weighted_variance\nprint(\"Varianzreduktion:\", variance_reduction)\n</code></pre>"},{"location":"KW2/decision_trees/split-criteria/#wann-und-warum-diese-kriterien-verwenden","title":"Wann und warum diese Kriterien verwenden?","text":"<p>Die Wahl des Split-Kriteriums h\u00e4ngt stark von der Art des Problems und der Zielsetzung ab: - Informationsgewinn: Besonders n\u00fctzlich bei Klassifikationsproblemen mit einer ungleichen Verteilung der Klassen. Es ber\u00fccksichtigt explizit die Unsicherheit. - Gini-Index: Einfacher zu berechnen als der Informationsgewinn und oft bei gro\u00dfen Datens\u00e4tzen vorteilhaft. - Varianzreduktion: Wird ausschlie\u00dflich bei Regressionsproblemen verwendet, da sie auf numerischen Zielvariablen basiert.</p> <p>In der Praxis verwenden die meisten Algorithmen, wie <code>DecisionTreeClassifier</code> in Scikit-learn, den Gini-Index als Standardkriterium f\u00fcr Klassifikationsaufgaben und die Varianzreduktion f\u00fcr Regressionsb\u00e4ume. </p>"},{"location":"KW2/kmeans/k-means/","title":"Exkurs K-Means","text":""},{"location":"KW2/kmeans/k-means/#k-means","title":"K-Means","text":"<p>K-Means ist Clustering-Algorithmus im Bereich des unbewachten Lernens. Der Algorithmus zielt darauf ab, Datenpunkte in k vordefinierte Gruppen zu gruppieren, wobei k die Anzahl der Cluster ist. Die Grundidee besteht darin, Cluster zu finden, die die Datenpunkte innerhalb des Clusters minimieren und gleichzeitig die Entfernungen zwischen den Clustern maximieren.</p>"},{"location":"KW2/kmeans/k-means/#anwendungsfalle","title":"Anwendungsf\u00e4lle","text":"<p>K-Means findet in verschiedenen Anwendungsbereichen Anwendung, darunter:</p> <ul> <li>Marktforschung: Segmentierung von Kunden auf Grundlage ihrer Einkaufsgewohnheiten.</li> <li>Bildkompression: Reduktion der Farbanzahl in einem Bild durch Gruppierung \u00e4hnlicher Farbt\u00f6ne.</li> <li>Genomik: Gruppierung von Genen mit \u00e4hnlichen Expressionsmustern.</li> </ul>"},{"location":"KW2/kmeans/k-means/#beispiel","title":"Beispiel","text":"<p>Angenommen, wir haben einen Datensatz von Kunden, die online einkaufen. Wir k\u00f6nnten K-Means verwenden, um Kunden mit \u00e4hnlichem Einkaufsverhalten zu gruppieren, z.B. Gelegenheitsk\u00e4ufer, regelm\u00e4\u00dfige K\u00e4ufer und Schn\u00e4ppchenj\u00e4ger.</p>"},{"location":"KW2/kmeans/k-means/#vorteile","title":"Vorteile","text":"<ul> <li>Einfachheit: K-Means ist einfach zu verstehen und zu implementieren.</li> <li>Skalierbarkeit: Der Algorithmus ist auch auf gro\u00dfen Datens\u00e4tzen effizient.</li> </ul>"},{"location":"KW2/kmeans/k-means/#nachteile","title":"Nachteile","text":"<ul> <li>Empfindlich auf Anfangszustand: Die Wahl der Anfangsclusterzentren kann das Ergebnis beeinflussen.</li> <li>Anf\u00e4llig f\u00fcr Ausrei\u00dfer: Ausrei\u00dfer k\u00f6nnen die Qualit\u00e4t der Clusterung beeintr\u00e4chtigen.</li> </ul>"},{"location":"KW2/linear_regression/linear_regression/","title":"Lineare Regression","text":""},{"location":"KW2/linear_regression/linear_regression/#lineare-regression-einfuhrung-und-funktionsweise","title":"Lineare Regression: Einf\u00fchrung und Funktionsweise","text":"<p>Die lineare Regression ist ein grundlegendes statistisches Verfahren, das zur Modellierung der Beziehung zwischen einer abh\u00e4ngigen Variablen (Zielvariable) und einer oder mehreren unabh\u00e4ngigen Variablen (Pr\u00e4diktoren) verwendet wird. Sie ist ein essenzielles Werkzeug in der Data Science und wird h\u00e4ufig f\u00fcr Vorhersagen und Analyseaufgaben eingesetzt.</p>"},{"location":"KW2/linear_regression/linear_regression/#grundidee-der-linearen-regression","title":"Grundidee der linearen Regression","text":"<p>Die lineare Regression versucht, eine Gerade zu finden, die den Zusammenhang zwischen den Variablen m\u00f6glichst gut beschreibt. Die Gerade wird durch die folgende Gleichung definiert:</p> \\[ y = \\beta_0 + \\beta_1 \\cdot x_1 + \\beta_2 \\cdot x_2 + \\dots + \\beta_n \\cdot x_n + \\epsilon \\] <ul> <li>\\( y \\): Abh\u00e4ngige Variable (Zielvariable)  </li> <li>\\( \\beta_0 \\): Achsenabschnitt (Intercept)  </li> <li>\\( \\beta_1, \\beta_2, \\dots, \\beta_n \\): Koeffizienten der unabh\u00e4ngigen Variablen  </li> <li>\\( x_1, x_2, \\dots, x_n \\): Unabh\u00e4ngige Variablen (Pr\u00e4diktoren)  </li> <li>\\( \\epsilon \\): Fehlerterm  </li> </ul> <p>Die Koeffizienten \\( \\beta \\) geben die St\u00e4rke und Richtung des Einflusses jeder unabh\u00e4ngigen Variable auf die Zielvariable an.</p>"},{"location":"KW2/linear_regression/linear_regression/#wann-verwendet-man-die-lineare-regression","title":"Wann verwendet man die lineare Regression?","text":"<p>Die lineare Regression ist geeignet, wenn: - Es einen linearen Zusammenhang zwischen den Variablen gibt. - Die Zielvariable kontinuierlich ist (z. B. Preis, Temperatur, Umsatz). - Das Ziel darin besteht, die Beziehung zwischen Variablen zu quantifizieren oder Vorhersagen zu treffen.</p> <p>Beispielanwendungen: - Vorhersage des Hauspreises basierend auf Fl\u00e4che und Lage. - Analyse der Verkaufszahlen im Zusammenhang mit Werbeausgaben.  </p>"},{"location":"KW2/linear_regression/linear_regression/#beispiel-einfache-lineare-regression-in-python","title":"Beispiel: Einfache lineare Regression in Python","text":"<p>Angenommen, wir haben Daten \u00fcber den Fl\u00e4cheninhalt eines Hauses (\\( x \\)) und dessen Preis (\\( y \\)) und m\u00f6chten eine Vorhersage treffen:</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\n# Beispiel-Daten\nx = np.array([50, 60, 70, 80, 90]).reshape(-1, 1)  # Fl\u00e4che in Quadratmetern\ny = np.array([150000, 180000, 210000, 240000, 270000])  # Preis in Euro\n\n# Lineares Regressionsmodell\nmodel = LinearRegression()\nmodel.fit(x, y)\n\n# Vorhersage\ny_pred = model.predict(x)\n\n# Visualisierung\nplt.scatter(x, y, color=\"blue\", label=\"Datenpunkte\")\nplt.plot(x, y_pred, color=\"red\", label=\"Regressionslinie\")\nplt.xlabel(\"Fl\u00e4che (m\u00b2)\")\nplt.ylabel(\"Preis (\u20ac)\")\nplt.title(\"Lineare Regression: Fl\u00e4che vs. Preis\")\nplt.legend()\nplt.show()\n\n# Ausgeben der Koeffizienten\nprint(\"Steigung (\u03b21):\", model.coef_[0])\nprint(\"Achsenabschnitt (\u03b20):\", model.intercept_)\n</code></pre>"},{"location":"KW2/linear_regression/linear_regression/#erweiterung-multiple-lineare-regression","title":"Erweiterung: Multiple lineare Regression","text":"<p>Wenn es mehrere unabh\u00e4ngige Variablen gibt (z. B. Fl\u00e4che, Anzahl der Zimmer, Baujahr), spricht man von multipler linearer Regression. Die Grundidee bleibt gleich, aber die Berechnung umfasst mehrere Pr\u00e4diktoren:</p> \\[ y = \\beta_0 + \\beta_1 \\cdot x_1 + \\beta_2 \\cdot x_2 + \\dots + \\beta_n \\cdot x_n \\] <p>Beispiel in Python:</p> <pre><code># Beispiel-Daten f\u00fcr multiple Regression\nX = np.array([[50, 2], [60, 3], [70, 3], [80, 4], [90, 4]])  # Fl\u00e4che, Zimmeranzahl\ny = np.array([150000, 180000, 210000, 240000, 270000])       # Preis\n\n# Modell trainieren\nmodel = LinearRegression()\nmodel.fit(X, y)\n\n# Ausgeben der Koeffizienten\nprint(\"Koeffizienten:\", model.coef_)\nprint(\"Achsenabschnitt (\u03b20):\", model.intercept_)\n</code></pre> <p>Ergebnis: Die Koeffizienten geben an, wie stark sich der Preis ver\u00e4ndert, wenn z. B. die Fl\u00e4che um einen Quadratmeter oder die Zimmeranzahl um eins steigt.</p>"},{"location":"KW2/linear_regression/linear_regression/#evaluierung-des-modells","title":"Evaluierung des Modells","text":"<p>Um die G\u00fcte der linearen Regression zu bewerten, wird h\u00e4ufig der R\u00b2-Wert (Bestimmtheitsma\u00df) verwendet. Er gibt an, wie gut die unabh\u00e4ngigen Variablen die Zielvariable erkl\u00e4ren k\u00f6nnen. Ein Wert von 1 bedeutet eine perfekte Anpassung, ein Wert von 0 bedeutet, dass die unabh\u00e4ngigen Variablen keinen Einfluss haben.</p> <p>Beispiel:</p> <pre><code>print(\"R\u00b2-Wert:\", model.score(X, y))\n</code></pre>"},{"location":"KW2/naive_bayes/intro/","title":"Klassifikation und naive bayes","text":"<p>Wir werden dieses Thema noch einmal behandeln, wenn wir mit Klassifikaton und Machine Learning anfangen.</p> <p>In diesem Exkurs werden wir uns mit dem Thema Klassifikation und dem Naive Bayes Klassifikator besch\u00e4ftigen. Klassifikation ist ein wichtiger Bereich des maschinellen Lernens, der sich mit der Zuordnung von Objekten zu vordefinierten Klassen befasst. Der Naive Bayes Klassifikator ist ein probabilistischer Klassifikator, der auf dem Bayes-Theorem basiert und die Annahme macht, dass die Features unabh\u00e4ngig voneinander sind.</p>"},{"location":"KW2/naive_bayes/intro/#klassifikation","title":"Klassifikation","text":"<p>Im Bereich des maschinellen Lernens ist Klassifikation ein \u00fcberwachtes Lernproblem, bei dem ein Algorithmus eine Funktion erstellt, die Eingabedaten in vordefinierte Klassen oder Kategorien einteilt. Die Klassifikation wird h\u00e4ufig zur Vorhersage von Kategorien wie Spam oder Nicht-Spam, Kreditw\u00fcrdigkeit oder Kreditrisiko, Krankheit oder Gesundheit usw. verwendet.</p> <p></p>"},{"location":"KW2/naive_bayes/naive_bayes/","title":"Naive Bayes Klassifikatior","text":"<p>Der Naive Bayes Klassifikator ist ein wahrscheinlichkeits-basierter Klassifikator, der auf dem Bayes Theorem basiert. Der Klassifikator ist \"naive\", weil er die Annahme macht, dass die Features unabh\u00e4ngig voneinander sind. Das bedeutet, dass das Auftreten eines bestimmten Features in einer Klasse nicht von dem Auftreten eines anderen Features in derselben Klasse abh\u00e4ngt. Diese Annahme ist in der Praxis selten erf\u00fcllt, wird aber trotzdem oft explizit oder immplizit getroffen, da sie die Berechnung der Wahrscheinlichkeiten vereinfacht.</p>"},{"location":"KW2/naive_bayes/naive_bayes/#satz-von-bayes","title":"Satz von Bayes","text":"<p>Der Satz von Bayes trifft eine Aussage \u00fcber die bedingte Wahrscheinlichkeit von Ereignissen. Eine bedingte Wahrscheinlichkeit ist die Wahrscheinlichkeit f\u00fcr das Eintreten eines Ereignisses unter der Bedingung, dass ein anderes Ereignis bereits bereits eingetreten ist bzw. mit Sicherheit eintreten wird. Ein Beispiel ist die Wahrscheinlichkeit, dass es regnet, wenn der Himmel bew\u00f6lkt ist. Der Satz von Bayes lautet: </p> \\[ P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)} \\] <p>Dabei ist \\(P(A|B)\\) die Wahrscheinlichkeit, dass Ereignis \\(A\\) eintritt, wenn Ereignis \\(B\\) bereits eingetreten ist. \\(P(B|A)\\) ist die Wahrscheinlichkeit, dass Ereignis \\(B\\) eintritt, wenn Ereignis \\(A\\) bereits eingetreten ist. \\(P(A)\\) und \\(P(B)\\) sind die Wahrscheinlichkeiten, dass Ereignis \\(A\\) bzw. Ereignis \\(B\\) unabh\u00e4ngig voneinander eintreten.</p>"},{"location":"KW2/naive_bayes/naive_bayes/#beispiel","title":"Beispiel","text":"<p>Endstand im Fu\u00dfball</p> <p>Angenommen, wir wollen die Wahrscheinlichkeit berechnen, dass ein Fu\u00dfballspiel gewonnen wird, wenn das Team in der ersten Halbzeit f\u00fchrt. Die Wahrscheinlichkeit, dass das Team \\(A\\) in der ersten Halbzeit f\u00fchrt, betr\u00e4gt \\(P(A) = 0.6\\). Die Wahrscheinlichkeit, dass das Team das Spiel gewinnt, wenn es in der ersten Halbzeit f\u00fchrt, betr\u00e4gt \\(P(B|A) = 0.8\\). Die Wahrscheinlichkeit, dass das Team \\(B\\) das Spiel gewinnt, betr\u00e4gt \\(P(B) = 0.7\\). Die Wahrscheinlichkeit, dass das Team in der ersten Halbzeit f\u00fchrt, wenn es das Spiel gewinnt, betr\u00e4gt \\(P(A|B) = (0.8*0.7)/0.6 = 0.933\\).</p>"},{"location":"KW2/naive_bayes/naive_bayes/#ablauf","title":"Ablauf","text":"<ol> <li>Berechnung der Klassenpriorit\u00e4ten:</li> <li> <p>F\u00fcr jede Klasse wird die Wahrscheinlichkeit berechnet, dass eine Instanz zuf\u00e4llig zu dieser Klasse geh\u00f6rt. Dies wird als Priorit\u00e4ts- oder Apriori-Wahrscheinlichkeit bezeichnet und mit \\(P(C)\\) dargestellt, wobei \\(C\\) die Klasse ist.</p> </li> <li> <p>Berechnung der bedingten Wahrscheinlichkeiten der Merkmale:</p> </li> <li> <p>F\u00fcr jedes Merkmal wird die bedingte Wahrscheinlichkeit berechnet, dass dieses Merkmal in einer gegebenen Klasse auftritt. Das wird als \\(P(F_i | C)\\) dargestellt, wobei \\(F_i\\) ein bestimmtes Merkmal ist und \\(C\\) die Klasse ist.</p> </li> <li> <p>Anwendung des Bayes-Theorems:</p> </li> <li>Wenn eine neue Instanz klassifiziert werden soll, werden die Wahrscheinlichkeiten f\u00fcr jede Klasse unter Verwendung der bedingten Wahrscheinlichkeiten der Merkmale und der Klassenpriorit\u00e4ten berechnet.</li> </ol> <p>$$    P(C | F_1, F_2, ..., F_n) = \\frac{P(F_1 | C) \\cdot P(F_2 | C) \\cdot ... \\cdot P(F_n | C) \\cdot P(C)}{P(F_1) \\cdot P(F_2) \\cdot ... \\cdot P(F_n)}    $$</p> <p>Da der Nenner f\u00fcr alle Klassen gleich ist, kann er oft ignoriert werden, und die Klassifizierung erfolgt durch Auswahl der Klasse mit der h\u00f6chsten Wahrscheinlichkeit.</p> <p>$$    \\text{Klassifizierung} = \\arg\\max_C P(C) \\prod_{i=1}^{n} P(F_i | C)    $$</p>"},{"location":"KW2/naive_bayes/naive_bayes/#spam-detaction-with-naive-bayes-viktor-reichert","title":"Spam Detaction with Naive Bayes (Viktor Reichert)","text":""},{"location":"KW2/naive_bayes/naive_bayes/#mehr","title":"Mehr","text":"<p>\u00dcbersicht Naive Bayes</p> <p>\u00dcbersicht Naive Bayes zum Nachlesen (IBM) Gaussian Naive Bayes</p>"},{"location":"KW2/statistical_and_machine_learning/evaluation/","title":"Exkurs Evaluation","text":""},{"location":"KW2/statistical_and_machine_learning/evaluation/#modell-evaluation-grundlagen-und-methoden","title":"Modell-Evaluation: Grundlagen und Methoden","text":"<p>Die Evaluation eines Modells ist ein entscheidender Schritt im Machine Learning-Prozess. Sie hilft uns zu beurteilen, wie gut ein Modell funktioniert, und sicherzustellen, dass es auf neuen, unbekannten Daten gute Vorhersagen treffen kann. Eine fundierte Evaluation ist wichtig, um \u00dcberanpassung (Overfitting) oder Unteranpassung (Underfitting) zu vermeiden und die richtige Modellwahl zu treffen.</p>"},{"location":"KW2/statistical_and_machine_learning/evaluation/#ziele-der-modell-evaluation","title":"Ziele der Modell-Evaluation","text":"<p>Die Hauptziele der Modell-Evaluation sind: - \u00dcberpr\u00fcfung der Genauigkeit: Wie genau sagt das Modell die Zielvariable vorher? - Bewertung der Generalisierungsf\u00e4higkeit: Wie gut performt das Modell auf neuen Daten, die nicht im Training enthalten waren? - Identifikation von Schw\u00e4chen: Wo liegen die St\u00e4rken und Schw\u00e4chen des Modells, und wie k\u00f6nnen sie verbessert werden?</p>"},{"location":"KW2/statistical_and_machine_learning/evaluation/#wichtige-metriken-zur-modellbewertung","title":"Wichtige Metriken zur Modellbewertung","text":"<p>Die Wahl der Bewertungsmetrik h\u00e4ngt davon ab, ob es sich um ein Regressions- oder Klassifikationsproblem handelt.</p>"},{"location":"KW2/statistical_and_machine_learning/evaluation/#1-klassifikationsmetriken","title":"1. Klassifikationsmetriken","text":""},{"location":"KW2/statistical_and_machine_learning/evaluation/#genauigkeit-accuracy","title":"Genauigkeit (Accuracy)","text":"<p>Die Genauigkeit misst den Anteil der korrekten Vorhersagen an allen Vorhersagen. Sie eignet sich besonders f\u00fcr ausgeglichene Datens\u00e4tze, bei denen die Klassenverteilung gleichm\u00e4\u00dfig ist.</p> <p>Formel: $$ \\text{Accuracy} = \\frac{\\text{Anzahl der korrekten Vorhersagen}}{\\text{Gesamtanzahl der Datenpunkte}} $$</p> <p>Beispiel in Python:</p> <pre><code>from sklearn.metrics import accuracy_score\n\ny_true = [1, 0, 1, 1, 0]  # Wahre Klassen\ny_pred = [1, 0, 1, 0, 0]  # Vorhergesagte Klassen\n\naccuracy = accuracy_score(y_true, y_pred)\nprint(\"Accuracy:\", accuracy)\n</code></pre>"},{"location":"KW2/statistical_and_machine_learning/evaluation/#precision-prazision","title":"Precision (Pr\u00e4zision)","text":"<p>Die Pr\u00e4zision misst den Anteil der korrekt vorhergesagten positiven Klassen an allen als positiv vorhergesagten Klassen. Sie ist wichtig, wenn falsch-positive Vorhersagen (False Positives) minimiert werden sollen.</p> <p>Formel: $$ \\text{Precision} = \\frac{\\text{True Positives (TP)}}{\\text{True Positives (TP)} + \\text{False Positives (FP)}} $$</p> <p>Beispiel in Python:</p> <pre><code>from sklearn.metrics import precision_score\n\nprecision = precision_score(y_true, y_pred)\nprint(\"Precision:\", precision)\n</code></pre>"},{"location":"KW2/statistical_and_machine_learning/evaluation/#recall-empfindlichkeitsensitivitat","title":"Recall (Empfindlichkeit/Sensitivit\u00e4t)","text":"<p>Der Recall gibt an, wie viele der tats\u00e4chlich positiven Klassen korrekt erkannt wurden. Diese Metrik ist relevant, wenn falsch-negative Vorhersagen (False Negatives) vermieden werden sollen.</p> <p>Formel: $$ \\text{Recall} = \\frac{\\text{True Positives (TP)}}{\\text{True Positives (TP)} + \\text{False Negatives (FN)}} $$</p> <p>Beispiel in Python:</p> <pre><code>from sklearn.metrics import recall_score\n\nrecall = recall_score(y_true, y_pred)\nprint(\"Recall:\", recall)\n</code></pre>"},{"location":"KW2/statistical_and_machine_learning/evaluation/#f1-score","title":"F1-Score","text":"<p>Der F1-Score ist das harmonische Mittel von Precision und Recall. Er ist n\u00fctzlich, wenn ein Gleichgewicht zwischen Precision und Recall wichtig ist, z. B. bei unausgeglichenen Datens\u00e4tzen.</p> <p>Formel: $$ F_1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}} $$</p> <p>Beispiel in Python:</p> <pre><code>from sklearn.metrics import f1_score\n\nf1 = f1_score(y_true, y_pred)\nprint(\"F1-Score:\", f1)\n</code></pre>"},{"location":"KW2/statistical_and_machine_learning/evaluation/#roc-auc-wert-receiver-operating-characteristic-area-under-curve","title":"ROC-AUC-Wert (Receiver Operating Characteristic - Area Under Curve)","text":"<p>Der ROC-AUC-Wert misst die Trennf\u00e4higkeit eines Klassifikationsmodells. Ein Wert von 1 zeigt perfekte Trennung, w\u00e4hrend ein Wert von 0,5 auf reines Raten hinweist.</p> <p>Formel f\u00fcr die AUC: Die Berechnung erfolgt basierend auf der Fl\u00e4che unter der ROC-Kurve. Die ROC-Kurve selbst zeigt die True Positive Rate (Recall) gegen die False Positive Rate: $$ \\text{False Positive Rate (FPR)} = \\frac{\\text{False Positives (FP)}}{\\text{False Positives (FP)} + \\text{True Negatives (TN)}} $$</p> <p>Beispiel in Python:</p> <pre><code>from sklearn.metrics import roc_auc_score\n\n# Wahrscheinlichkeiten f\u00fcr die Klasse 1\ny_prob = [0.9, 0.8, 0.3, 0.4, 0.2]\n\nroc_auc = roc_auc_score(y_true, y_prob)\nprint(\"ROC-AUC-Wert:\", roc_auc)\n</code></pre>"},{"location":"KW2/statistical_and_machine_learning/evaluation/#2-regressionsmetriken","title":"2. Regressionsmetriken","text":""},{"location":"KW2/statistical_and_machine_learning/evaluation/#mittlerer-absoluter-fehler-mean-absolute-error-mae","title":"Mittlerer Absoluter Fehler (Mean Absolute Error, MAE)","text":"<p>MAE misst den durchschnittlichen absoluten Unterschied zwischen vorhergesagten und tats\u00e4chlichen Werten. Diese Metrik ist leicht interpretierbar und eignet sich, wenn alle Fehler gleich gewichtet werden sollen.</p> <p>Formel: $$ \\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i| $$</p> <p>Beispiel in Python:</p> <pre><code>from sklearn.metrics import mean_absolute_error\n\ny_true = [3.5, 2.8, 4.0, 5.2]\ny_pred = [3.6, 2.7, 3.9, 5.1]\n\nmae = mean_absolute_error(y_true, y_pred)\nprint(\"MAE:\", mae)\n</code></pre>"},{"location":"KW2/statistical_and_machine_learning/evaluation/#mittlerer-quadratischer-fehler-mean-squared-error-mse","title":"Mittlerer quadratischer Fehler (Mean Squared Error, MSE)","text":"<p>MSE berechnet den durchschnittlichen quadrierten Unterschied zwischen den Vorhersagen und den tats\u00e4chlichen Werten. Gr\u00f6\u00dfere Fehler werden st\u00e4rker gewichtet, was diese Metrik empfindlich gegen\u00fcber Ausrei\u00dfern macht.</p> <p>Formel: $$ \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $$</p> <p>Beispiel in Python:</p> <pre><code>from sklearn.metrics import mean_squared_error\n\nmse = mean_squared_error(y_true, y_pred)\nprint(\"MSE:\", mse)\n</code></pre>"},{"location":"KW2/statistical_and_machine_learning/evaluation/#wurzel-des-mittleren-quadratischen-fehlers-root-mean-squared-error-rmse","title":"Wurzel des Mittleren Quadratischen Fehlers (Root Mean Squared Error, RMSE)","text":"<p>Die RMSE ist die Quadratwurzel des MSE und hat dieselbe Einheit wie die Zielvariable. Sie ist besonders n\u00fctzlich, um die Gr\u00f6\u00dfe der Fehler direkt zu interpretieren.</p> <p>Formel: $$ \\text{RMSE} = \\sqrt{\\text{MSE}} $$</p> <p>Beispiel in Python:</p> <pre><code>import numpy as np\n\nrmse = np.sqrt(mse)\nprint(\"RMSE:\", rmse)\n</code></pre>"},{"location":"KW2/statistical_and_machine_learning/evaluation/#bestimmtheitsma-r2-score","title":"Bestimmtheitsma\u00df (R\u00b2-Score)","text":"<p>Der R\u00b2-Score misst, wie gut das Modell die Varianz der Zielvariable erkl\u00e4rt. Ein Wert von 1 bedeutet perfekte Vorhersagen, ein Wert von 0 deutet darauf hin, dass das Modell nicht besser als der Mittelwert ist.</p> <p>Formel: </p> \\[ R^2 = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2} \\] <p>Beispiel in Python:</p> <pre><code>from sklearn.metrics import r2_score\n\nr2 = r2_score(y_true, y_pred)\nprint(\"R\u00b2-Wert:\", r2)\n</code></pre>"},{"location":"KW2/statistical_and_machine_learning/evaluation/#kreuzvalidierung-cross-validation","title":"Kreuzvalidierung (Cross-Validation)","text":"<p>Die Kreuzvalidierung ist eine Methode, um die Modellleistung stabiler zu bewerten. Dabei wird der Datensatz in k-Folds aufgeteilt, und das Modell wird k-mal trainiert und getestet. Dies reduziert die Abh\u00e4ngigkeit von einer einzelnen Datenaufteilung.</p> <p>Beispiel in Python:</p> <pre><code>from sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LinearRegression\n\nmodel = LinearRegression()\nscores = cross_val_score(model, X, y, cv=5, scoring='r2')\n\nprint(\"R\u00b2-Scores aus der Kreuzvalidierung:\", scores)\nprint(\"Durchschnittlicher R\u00b2-Wert:\", scores.mean())\n</code></pre>"},{"location":"KW2/statistical_and_machine_learning/generalization/","title":"Generalisierung","text":""},{"location":"KW2/statistical_and_machine_learning/generalization/#generalisierung","title":"Generalisierung","text":"<p>Generalisierung beschreibt, wie gut sich ein Modell an die Daten anpassunt und wie gut es auf ungesehenen, neuen Daten zurechtkommt</p>"},{"location":"KW2/statistical_and_machine_learning/generalization/#test-train-split","title":"Test/ Train Split","text":"<p>Wir benutzen ein Splitting zwischen Test- und Trainingsdaten, um unsere Modelle auf ungesehenen Daten zu validieren. Diesen k\u00f6nnen wir beispielsweise durch die Verwendung der funktion <code>train_test_split</code> aus <code>sklearn.model_selection</code> erstellen.</p> <pre><code>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n</code></pre>"},{"location":"KW2/statistical_and_machine_learning/generalization/#overfitting","title":"Overfitting","text":"<p>Overfitting tritt auf, wenn ein Modell zu stark an die Trainingsdaten angepasst ist und dadurch auf neuen, bisher ungesehenen Daten schlecht generalisiert. Overfitting hat daher direkte Auswirkungen auf die Leistungsf\u00e4higkeit und die Vorhersagegenauigkeit von Modellen.</p> <p>Grund f\u00fcr das OVerfitting ist, dass ein Modell die Trainingsdaten zu genau erfasst, einschlie\u00dflich ihrer Rauschen und Ausrei\u00dfer. Das Modell passt sich den Daten so stark an, dass es spezifische Muster und Details lernt, die nicht notwendigerweise auf neuen Daten vorhanden sind. Im Gegensatz dazu steht Underfitting, bei dem das Modell zu einfach ist und selbst die Trainingsdaten nicht gut erkl\u00e4ren kann.</p>"},{"location":"KW2/statistical_and_machine_learning/generalization/#ursachen-von-overfitting","title":"Ursachen von Overfitting:","text":"<ol> <li> <p>Komplexit\u00e4t des Modells: Modelle mit zu vielen Parametern oder zu hoher Komplexit\u00e4t neigen dazu, die Trainingsdaten zu genau zu erfassen und \u00fcberm\u00e4\u00dfig auf Muster zu reagieren, die nicht repr\u00e4sentativ sind.</p> </li> <li> <p>Mangel an Trainingsdaten: Wenn die Menge an Trainingsdaten begrenzt ist, besteht die Gefahr, dass das Modell nicht gen\u00fcgend Vielfalt lernt und stattdessen spezifische Beispiele auswendig lernt.</p> </li> <li> <p>Rauschen in den Daten: Wenn die Trainingsdaten Rauschen oder zuf\u00e4llige Schwankungen enthalten, kann das Modell versuchen, auch dieses Rauschen zu modellieren, was zu Overfitting f\u00fchrt.</p> </li> </ol>"},{"location":"KW2/statistical_and_machine_learning/generalization/#anzeichen-von-overfitting","title":"Anzeichen von Overfitting:","text":"<ol> <li> <p>Hohe Genauigkeit auf Trainingsdaten, niedrige Genauigkeit auf Testdaten: Ein \u00fcberangepasstes Modell wird auf den Trainingsdaten sehr genau sein, aber auf neuen Daten schlecht abschneiden.</p> </li> <li> <p>Instabile Modellparameter: Kleine \u00c4nderungen in den Trainingsdaten k\u00f6nnen zu erheblichen Ver\u00e4nderungen in den Modellparametern f\u00fchren.</p> </li> <li> <p>Overly Complex Decision Boundaries: Visualisierungen von Entscheidungsgrenzen zeigen, dass \u00fcberangepasste Modelle dazu neigen, zu komplexe Formen zu erzeugen, um alle Trainingsdaten zu ber\u00fccksichtigen.</p> </li> </ol>"},{"location":"KW2/statistical_and_machine_learning/generalization/#methoden-zur-vermeidung-von-overfitting","title":"Methoden zur Vermeidung von Overfitting:","text":"<ol> <li> <p>Kreuzvalidierung: Verwenden Sie Kreuzvalidierungstechniken, um die Leistung des Modells auf unabh\u00e4ngigen Testdaten zu bewerten.</p> </li> <li> <p>Regulierungsmethoden: F\u00fcgen Sie Regularisierungsterme zu den Verlustfunktionen hinzu, um die Gr\u00f6\u00dfe der Modellparameter zu begrenzen.</p> </li> <li> <p>Mehr Trainingsdaten: Erh\u00f6hen Sie die Menge an verf\u00fcgbaren Trainingsdaten, um das Modell mit einer breiteren Vielfalt an Beispielen zu versorgen.</p> </li> <li> <p>Feature Selection: Reduzieren Sie die Anzahl der Merkmale oder verwenden Sie Techniken zur Auswahl der wichtigsten Merkmale.</p> </li> </ol> <p>Underfitting</p> <p>Underfitting tritt auf, wenn ein Modell zu einfach ist, um die zugrunde liegenden Strukturen in den Trainingsdaten zu erfassen. Im Gegensatz zum Overfitting, bei dem das Modell zu komplex ist und die Trainingsdaten zu genau erfasst, versagt ein unterangepasstes Modell darin, selbst auf den Trainingsdaten angemessene Vorhersagen zu treffen.</p> <p>Ausl\u00f6ser f\u00fcr Underfitting ist ein Modell, dass nicht in der Lage ist, die Komplexit\u00e4t der zugrunde liegenden Datenstrukturen zu erfassen. Es entsteht, wenn das Modell zu einfach ist, um die Muster in den Trainingsdaten korrekt zu lernen. Ein unterangepasstes Modell generalisiert schlecht auf neue, bisher ungesehene Daten, da es nicht in der Lage ist, die inh\u00e4renten Muster und Zusammenh\u00e4nge zu verstehen.</p>"},{"location":"KW2/statistical_and_machine_learning/generalization/#ursachen-von-underfitting","title":"Ursachen von Underfitting:","text":"<ol> <li> <p>Zu einfaches Modell: Modelle mit zu wenigen Parametern oder zu geringer Komplexit\u00e4t k\u00f6nnen nicht alle relevanten Muster in den Daten erfassen.</p> </li> <li> <p>Mangel an relevanten Merkmalen: Wenn wichtige Merkmale in den Daten fehlen oder nicht ber\u00fccksichtigt werden, kann das Modell nicht angemessen generalisieren.</p> </li> <li> <p>Fehlerhafte Modellwahl: Die Auswahl eines Modells, das nicht zur Struktur der Daten passt, kann zu Underfitting f\u00fchren.</p> </li> </ol>"},{"location":"KW2/statistical_and_machine_learning/generalization/#anzeichen-von-underfitting","title":"Anzeichen von Underfitting:","text":"<ol> <li> <p>Schlechte Leistung auf Trainingsdaten: Das Modell erzielt eine niedrige Genauigkeit oder hohe Fehler auf den Trainingsdaten.</p> </li> <li> <p>Schlechte Generalisierung: Die Vorhersagen des Modells sind auch auf neuen Daten ungenau und wenig zuverl\u00e4ssig.</p> </li> <li> <p>Einfache Entscheidungsgrenzen: Visualisierungen zeigen, dass das Modell zu einfache Entscheidungsgrenzen erzeugt, die nicht den realen Datenverteilungen entsprechen.</p> </li> </ol>"},{"location":"KW2/statistical_and_machine_learning/generalization/#methoden-zur-vermeidung-von-underfitting","title":"Methoden zur Vermeidung von Underfitting:","text":"<ol> <li> <p>Erh\u00f6hen der Modellkomplexit\u00e4t: Verwenden Sie Modelle mit mehr Parametern oder h\u00f6herer Komplexit\u00e4t, um die Daten besser zu erfassen.</p> </li> <li> <p>Hinzuf\u00fcgen relevanter Merkmale: Stellen Sie sicher, dass alle relevanten Merkmale in den Daten ber\u00fccksichtigt werden.</p> </li> <li> <p>Anpassung des Modells: W\u00e4hlen Sie ein Modell, das besser zur Struktur der Daten passt.</p> </li> <li> <p>Optimierung von Hyperparametern: Experimentieren Sie mit verschiedenen Hyperparameterwerten, um die Modellleistung zu verbessern.</p> </li> </ol>"},{"location":"KW2/statistical_and_machine_learning/generalization/#exkurs-regularisierung","title":"Exkurs: Regularisierung","text":"<p>Regularisierung ist ein Konzept im maschinellen Lernen, das dazu dient, Overfitting in Modellen zu verhindern oder zu verringern, indem zus\u00e4tzliche Informationen in den Optimierungsprozess einbezogen werden. Overfitting tritt auf, wenn ein Modell zu gut an die Trainingsdaten angepasst ist und sich dadurch auf neuen, bisher ungesehenen Daten schlecht verallgemeinert.</p> <p>Es gibt zwei Haupttypen der Regularisierung, die in der Praxis h\u00e4ufig verwendet werden:</p> <ol> <li> <p>L1-Regularisierung (Lasso-Regularisierung): Bei der L1-Regularisierung wird der Regularisierungsterm zur Loss-Funktion hinzugef\u00fcgt, der proportional zur absoluten Summe der Modellparameter ist. Dies f\u00fchrt dazu, dass einige der Gewichtungen im Modell auf genau null gesetzt werden k\u00f6nnen, was zu einer automatischen Feature-Auswahl f\u00fchrt. Die L1-Regularisierung tr\u00e4gt somit dazu bei, das Modell zu vereinfachen und irrelevante Features zu eliminieren.</p> </li> <li> <p>L2-Regularisierung (Ridge-Regularisierung): Im Gegensatz dazu f\u00fcgt die L2-Regularisierung der Loss-Funktion einen Term hinzu, der proportional zum Quadrat der L2-Norm der Modellparameter ist. Dies verhindert, dass einzelne Gewichtungen dominieren, indem sie dazu neigen, gro\u00dfe Gewichtungen zu reduzieren. Die L2-Regularisierung tr\u00e4gt dazu bei, das Modell zu stabilisieren und Overfitting zu verhindern, indem sie die Gewichtungen gleichm\u00e4\u00dfig verteilt.</p> </li> </ol> <p>Die Regularisierung wird oft in Verbindung mit linearen Regressionen, logistischen Regressionen und neuronalen Netzwerken verwendet, kann aber auch in anderen Modelltypen angewendet werden.</p>"},{"location":"KW2/statistical_and_machine_learning/generalization/#warum-ist-regularisierung-wichtig","title":"Warum ist Regularisierung wichtig?","text":"<ul> <li> <p>Overfitting-Pr\u00e4vention: Das Hinzuf\u00fcgen von Regularisierungstermen zur Loss-Funktion hilft, Overfitting zu minimieren, indem es das Modell dazu zwingt, nicht zu stark auf die Trainingsdaten zu reagieren.</p> </li> <li> <p>Bessere Verallgemeinerung: Regularisierung verbessert die F\u00e4higkeit eines Modells, auf neuen Daten, die es nicht w\u00e4hrend des Trainings gesehen hat, korrekte Vorhersagen zu treffen.</p> </li> <li> <p>Feature-Auswahl: Insbesondere L1-Regularisierung kann dazu beitragen, irrelevante oder redundante Features zu eliminieren, was die Effizienz des Modells verbessert.</p> </li> </ul> <p>Die St\u00e4rke der Regularisierung wird durch einen sogenannten Hyperparameter gesteuert, der w\u00e4hrend des Trainingsprozesses optimiert wird. Die Wahl des geeigneten Regularisierungsterms h\u00e4ngt von der spezifischen Problemstellung und den Eigenschaften der Daten ab.</p>"},{"location":"KW2/statistical_and_machine_learning/test_train_split/","title":"Test-Train Split","text":"<p>M\u00f6chten wir ein Modell lernen lassenm, so m\u00fcssen wir ein Verfahren nutzen, um diesen Lernprozess \u00fcberpr\u00fcfen. Dieses \u00dcberpr\u00fcfen des Lernprozesses sollte nat\u00fcrlich wieder auf Daten stattfinden. Die einfachste M\u00f6glichkeit f\u00fcr diese \u00dcberpr\u00fcfung w\u00e4re das Verwenden von Daten, welchen wir auch f\u00fcr das Training verwendet haben. Bereits in der Einleitung zu Machine Learning haben wir jedoch gelernt, dass m\u00f6glich ist dass ein Modell seie Daten einfach auswendig lernt. Wir haben dies als Overfitting bezeichnet. W\u00fcrden wir die Performance des Modells auf den Trainingsdaten testen, w\u00fcrden wir dieses Overfitting wahrscheinlich nicht erkennen.</p> <p>Daher ist es ratsam, f\u00fcr das Evaluieren des Modells Daten zu verwenden, welche das Modell noch nicht gesehen hat. Daf\u00fcr teilen wir unseren Datensatz in 3 Teile auf: Trainingsdaten, Validationsdaten und Testdaten. Die Trainingsdaten werden verwendet, um das Modell zu trainieren. Die Validationsdaten werden verwendet, um das Modell zu evaluieren und die Hyperparameter zu optimieren. Die Testdaten werden verwendet, um die Performance des Modells zu evaluieren. Dies Geschieht nach dem Training und der Optimierung der Hyperparameter.</p> <p></p>"},{"location":"KW2/statistical_and_machine_learning/test_train_split/#trainingsdaten-und-validationsdaten","title":"Trainingsdaten und Validationsdaten","text":"<p>Die Aufteilung des Datensatzes in Trainings-, Test- und Validationsdaten sollte in den meisten F\u00e4llen zuf\u00e4llig erfolgen. Ausnahmen k\u00f6nnen zum Beispiel dann gemacht werden, wenn die Daten zeitlich geordnet sind. In diesem Fall k\u00f6nnten die Trainingsdaten die \u00e4lteren Daten enthalten und die Test-/Validationsdaten die neueren. Bei der Aufteilung kann als Ausgangwert eine Aufteilung von 60% Trainingsdaten und jeweils 20% f\u00fcr Validations- und Testdaten verwendet werden. Diese Werte k\u00f6nnen jedoch je nach Anwendungsfall und gr\u00f6\u00dfe des Datensatzes variieren.</p>"},{"location":"KW2/statistical_and_machine_learning/test_train_split/#anzahl-der-hyperparameter","title":"Anzahl der Hyperparameter","text":"<p>Je gr\u00f6\u00dfer die Anzahl an Hyperparametern sit, desto gr\u00f6\u00dfer ist die Wahrscheinlichkeit, dass das Modell die Trainingsdaten auswendig lernt. Daher ist es ratsam, bei einer gro\u00dfen Anzahl an Hyperparametern die Anzahl der Validationsdaten zu erh\u00f6hen.</p>"},{"location":"KW2/statistical_and_machine_learning/test_train_split/#dimenaionalitat-der-daten","title":"Dimenaionalit\u00e4t der Daten","text":"<p>Bei hochdimensionalen Daten ist es ebenfalls ratsam, die Anzahl der Testdaten zu erh\u00f6hen. Dies liegt daran, dass die Wahrscheinlichkeit, dass das Modell die Trainingsdaten auswendig lernt, bei hochdimensionalen Daten gr\u00f6\u00dfer ist. Der Grund daf\u00fcr ist, dass jeder zus\u00e4tzliche Parameter die Anzahl der m\u00f6glichen Kombinationen erh\u00f6ht. Die Anzahl der Freiheitsgrade steigt also.</p>"},{"location":"KW2/statistical_and_machine_learning/test_train_split/#ziel-des-modells","title":"Ziel des Modells","text":"<p>Das Ziel des Modells kann ebenfalls Einfluss auf die Anzahl der Validationsdaten haben. Soll das Modell zum Beispiel eine Krankheit mit hoher Sicherheit erkennen, so ist es ratsam, die Anzahl der Validationsdaten zu erh\u00f6hen. Somit kann die Sicherheit des Modells besser validiert werden, was in diesem Fall eine h\u00f6here Priorit\u00e4t hat, also alle m\u00f6glichen Krankheiten zu erkennen.</p> <p>Generell hat die Wahl des Test/Train Split eine Auswirkung auf die Varianz in den Daten. Je mehr Daten wir f\u00fcr das Training verwenden, desto geringer ist die Varianz auf den Trainingsdaten. Daf\u00fcr wird die Varianz auf den Validations- und Testdaten wahrscheinlich gr\u00f6\u00dfer. Umgekehrt ist es bei einer geringen Anzahl an Trainingsdaten. Die Varianz auf den Trainingsdaten ist dann wahrscheinlich gr\u00f6\u00dfer, w\u00e4hrend die Varianz auf den Validations- und Testdaten geringer ist.</p> <p>Die nachfolgende Grafik zeigt h\u00e4ufig verwendete Test/Train Split Verh\u00e4ltnisse.</p> <p></p>"},{"location":"KW2/statistical_and_machine_learning/train_models/","title":"Unser erstes Modell trainieren","text":"<p>Der Ablauf, um ein Modell zu trainieren, ist in der Regel wie folgt:</p> <ol> <li>Daten laden: Lade die Daten, die du f\u00fcr das Training des Modells verwenden m\u00f6chtest.</li> <li>Daten vorbereiten: Bereite die Daten so auf, dass sie von einem Modell verarbeitet werden k\u00f6nnen.</li> <li>Daten aufteilen: Teile die Daten in Trainings- und Testdaten auf.</li> <li>Modell erstellen: W\u00e4hle ein Modell aus, das du trainieren m\u00f6chtest.</li> <li>Modell trainieren: Trainiere das Modell mit den vorbereiteten Daten.</li> <li>Modell evaluieren: Bewerte das Modell, um zu sehen, wie gut es funktioniert.</li> <li>Modell anwenden: Wende das trainierte Modell auf neue Daten an, um Vorhersagen zu treffen.</li> </ol>"},{"location":"KW2/statistical_and_machine_learning/train_models/#date-aufteilen","title":"Date aufteilen","text":"<p>Damit wir wissen, ob unser Modell gut funktioniert, m\u00fcssen wir es testen. Dazu teilen wir unsere Daten in zwei Teile auf: Trainingsdaten und Testdaten. Die Trainingsdaten verwenden wir, um das Modell zu trainieren, und die Testdaten verwenden wir, um das Modell zu evaluieren. F\u00fcr das Aufteilen der Daten wird oftmal ein Verh\u00e4ltnis von 80% Trainingsdaten und 20% Testdaten verwendet. </p> <p>Je nach gr\u00f6\u00dfe des Datensets, Verteilung Samples und den Anforderungen an das Modell kann dieses Verh\u00e4ltnis jedoch variieren. </p> <p>In Python k\u00f6nnen wir die Funktion <code>train_test_split</code> aus dem Modul <code>sklearn.model_selection</code> verwenden, um die Daten aufzuteilen. </p> <pre><code>from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n</code></pre> <p>Dabei sind: - <code>X</code> die Features (Eigenschaften) der Daten - <code>y</code> die Zielvariable (was wir vorhersagen wollen) - <code>test_size</code> der Anteil der Testdaten (hier 20%) - <code>random_state</code> ein Wert, der festlegt, wie die Daten aufgeteilt werden. Wenn du den gleichen Wert verwendest, erh\u00e4ltst du bei jedem Durchlauf die gleiche Aufteilung. - <code>X_train</code>, <code>X_test</code> die aufgeteilten Features - <code>y_train</code>, <code>y_test</code> die aufgeteilten Zielvariablen</p>"},{"location":"KW2/statistical_and_machine_learning/train_models/#modell-erstellen","title":"Modell erstellen","text":"<p>Nachdem wir die Daten aufgeteilt haben, k\u00f6nnen wir ein Modell erstellen. Dazu w\u00e4hlen wir ein Modell aus, das wir trainieren m\u00f6chten. Es gibt viele verschiedene Modelle, die f\u00fcr verschiedene Aufgaben geeignet sind. In unserem Fall verwenden wir den Gaussian Naive Bayes Klassifikator, der f\u00fcr Klassifikationsprobleme geeignet ist.</p> <pre><code>from sklearn.naive_bayes import GaussianNB\n\nmodel = GaussianNB()\n</code></pre>"},{"location":"KW2/statistical_and_machine_learning/train_models/#modell-trainieren","title":"Modell trainieren","text":"<p>Nachdem wir das Modell erstellt haben, k\u00f6nnen wir es mit den Trainingsdaten trainieren. Dazu verwenden wir die <code>fit</code> Methode des Modells.</p> <pre><code>model.fit(X_train, y_train)\n</code></pre>"},{"location":"KW2/statistical_and_machine_learning/train_models/#modell-evaluieren","title":"Modell evaluieren","text":"<p>Nachdem wir das Modell trainiert haben, k\u00f6nnen wir es mit den Testdaten evaluieren. Dazu verwenden wir den <code>accuracy_score</code> aus dem Modul <code>sklearn.metrics</code>, der die Genauigkeit des Modells berechnet.</p> <pre><code>from sklearn.metrics import accuracy_score\n\ny_pred = model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'Genauigkeit: {accuracy}')\n</code></pre>"},{"location":"KW2/statistical_and_machine_learning/train_models/#accuracy-score","title":"Accuracy Score","text":"<p>Die Genauigkeit (Accuracy) ist eine Metrik, die angibt, wie gut ein Modell funktioniert. Sie berechnet sich als der Anteil der korrekt vorhergesagten Instanzen an der Gesamtzahl der Instanzen. Die Genauigkeit liegt zwischen 0 und 1, wobei 1 f\u00fcr 100% Genauigkeit steht.</p>"},{"location":"KW2/statistical_and_machine_learning/distance_metrics/distance_metrics/","title":"Distanzmetriken","text":"<p>Es gibt viele verschiedene M\u00f6glichkeiten den Abstand zwischen zwei Punkten zu messen. Wie k\u00f6nnte man z.B. im folgenden Bild den Abstand der beiden Steckdosen messen?</p> <p>Es w\u00e4re sowohl m\u00f6glich mit dem Manhattenma\u00df (rot) den Abstand an der Wand zu messen, aber es w\u00e4re auch m\u00f6glich mit derm euclidischen Ma\u00df den Abstand als Luftlinie (blau) zu messen. Welches der beiden Abstandsma\u00dfe das richtige ist, h\u00e4ngt vom Anwendungsfall ab. Beides sind jedoch erlaubte Werkezuge um Abst\u00e4nde zu messen.</p> <p></p> <p>Es gibt eine unendliche Anzahl an M\u00f6glichkeiten den Abstand zwischen Punkten zu messen. Manchmal ist vorher m\u00f6glich ein sinnvolles zu definieren, oft muss dieses aber als ein  Hyperparameter ausprobiert werden.</p>"},{"location":"KW2/statistical_and_machine_learning/distance_metrics/distance_metrics/#was-ist-ein-distanzma","title":"Was ist ein Distanzma\u00df","text":"<p>Im Folgenden sind die Regeln genannt, nach denen eine Funktion als ein Abstandma\u00df gilt:</p> <p></p>"},{"location":"KW2/statistical_and_machine_learning/distance_metrics/distance_metrics/#beispiele-fur-abstandmae","title":"Beispiele f\u00fcr Abstandma\u00dfe","text":""},{"location":"KW2/svm/svm/","title":"Support Vector Machines","text":"<p>Support Vector Machines (SVM) sind maschinelle Lernmodelle, die in verschiedenen Anwendungsgebieten weit verbreitet sind. Ihre Beliebtheit gr\u00fcndet sich auf ihrer F\u00e4higkeit, komplexe Entscheidungsgrenzen zu modellieren und gleichzeitig eine effiziente Verarbeitung gro\u00dfer Datenmengen zu erm\u00f6glichen. Die Motivation hinter Support Vector Machines liegt in ihrer F\u00e4higkeit, Muster in Daten zu erkennen und Klassifikationsprobleme pr\u00e4zise zu l\u00f6sen. SVMs zeichnen sich durch ihre Robustheit und Flexibilit\u00e4t aus, wodurch sie sowohl f\u00fcr bin\u00e4re als auch f\u00fcr mehrklassige Klassifikationsaufgaben geeignet sind. Ihr fundamentales Prinzip besteht darin, eine optimale Trennebene zwischen verschiedenen Klassen zu finden, wobei der Abstand zu den n\u00e4chsten Datenpunkten maximiert wird. Diese Eigenschaft macht SVMs zu einem bevorzugten Werkzeug in Bereichen wie Mustererkennung, Bildverarbeitung, Textanalyse und vielen anderen Anwendungen im Bereich des maschinellen Lernens.</p> <p>Die SVM ist ein Modell aus dem \u00fcberwachten Lernen. Wir schauen uns die SVM als Klassifikationsmodell an. Sie kann allerdings auch f\u00fcr Regressionen verwendet werden.</p> <p></p>"},{"location":"KW2/svm/svm/#wann-ist-eine-svm-geeignet","title":"Wann ist eine SVM geeignet?","text":"<p>Support Vector Machines (SVMs) sind besonders geeignet, wenn bestimmte Merkmale in den Daten hervorgehoben werden sollen oder wenn die Daten hochdimensional sind. Hier sind einige Szenarien, in denen die Verwendung von SVMs angebracht sein k\u00f6nnte:</p> <ol> <li> <p>Klassifikation mit klaren Entscheidungsgrenzen:    SVMs eignen sich gut f\u00fcr Aufgaben, bei denen klare Entscheidungsgrenzen zwischen verschiedenen Klassen erforderlich sind. Ihr Hauptziel ist es, eine Trennebene zu finden, die den maximalen Abstand zu den n\u00e4chstgelegenen Datenpunkten jeder Klasse aufweist.</p> </li> <li> <p>Hochdimensionale Daten:    SVMs sind effektiv, wenn der Datensatz viele Merkmale oder Dimensionen aufweist. Sie k\u00f6nnen gut mit hochdimensionalen Daten umgehen, ohne anf\u00e4llig f\u00fcr den sogenannten \"Fluch der Dimensionalit\u00e4t\" zu sein.</p> </li> <li> <p>Daten mit nicht-linearen Strukturen:    SVMs k\u00f6nnen durch den Einsatz von Kernel-Tricks auch nicht-lineare Entscheidungsgrenzen modellieren. Das macht sie geeignet f\u00fcr Aufgaben, bei denen die Beziehung zwischen Eingangsmerkmalen und Zielvariablen komplex und nicht linear ist.</p> </li> <li> <p>Kleine Datens\u00e4tze mit hoher Dimensionalit\u00e4t:    Wenn die Anzahl der Beobachtungen relativ gering ist, aber die Anzahl der Merkmale hoch ist, k\u00f6nnen SVMs aufgrund ihrer F\u00e4higkeit, mit hochdimensionalen Daten umzugehen, gute Ergebnisse liefern.</p> </li> <li> <p>Anwendungen mit klaren Trennkriterien:    SVMs eignen sich gut f\u00fcr Anwendungen, bei denen eine klare Entscheidung oder Trennung zwischen verschiedenen Klassen erforderlich ist, wie beispielsweise in der Gesichtserkennung oder bei der Erkennung von Anomalien.</p> </li> <li> <p>Text- und Bildklassifikation:    In Anwendungen wie Textklassifikation (z. B. Spam-Erkennung) und Bildklassifikation (z. B. Objekterkennung) haben sich SVMs als leistungsstark erwiesen.</p> </li> <li> <p>Geriner Datenumfang    SVMs sind gut geeignet, wenn der Datensatz relativ klein ist, da sie mit wenigen Daten gut funktionieren k\u00f6nnen.</p> </li> </ol>"},{"location":"KW2/svm/svm/#mathematische-formulierung","title":"Mathematische Formulierung","text":"<p>Siehe hier</p> <ul> <li>Die SVM maximiert den Abstand zwischen der separierenden Hyperebene und den n\u00e4chstgelegenen Klassfikationspunkten (Support Vektoren)</li> <li>Die Hyperebene wird durch die Gleichung \\(w^Tx + b = 0\\) beschrieben</li> <li>Die Klassifikation erfolgt durch die Vorhersage des Vorzeichens von \\(w^Tx + b\\)</li> <li>Um die SVM auch auf nicht linear separierbare Daten anwenden zu k\u00f6nnen, wird die sog. Soft-Margin SVM verwendet. Hierbei wird eine Strafterm eingef\u00fchrt, der die Anzahl der falsch klassifizierten Punkte minimiert. Dieser Strafterm wird durch den Parameter \\(C\\) gesteuert. Je gr\u00f6\u00dfer \\(C\\) ist, desto mehrwerden falsch klassifizierte Punkte bestraft.</li> <li> <p>Sind die in der Ausgangsdimension nicht linear separierbar, kann der Kernel-Trick verwendet werden, um die Daten in eine h\u00f6here Dimension zu transformieren, in der sie linear separierbar sind. Die SVM wird dann in dieser h\u00f6heren Dimension trainiert und die Vorhersage erfolgt in der urspr\u00fcnglichen Dimension. </p> </li> <li> <p>In der Regel wird der RBF Kernel \\(K(x,y)=exp(-\\frac{\\| x - y \\|}{2*\\sigma^2})\\) verwendet.</p> </li> <li>Weitere Kernel sind der  <ul> <li>lineare Kernel \\(K(x,y)=\\langle x^Ty\\rangle\\)</li> <li>Polynomiale Kernel \\(K(x,y)= \\gamma \\langle x^Ty \\rangle + r^d\\)</li> <li>Sigmoid Kernel \\(K(x,y)=tanh(\\gamma \\langle x^Ty \\rangle + r)\\)</li> </ul> </li> </ul> <p></p> <p>Technisch wird das Optimierungsproblem der SVM durch eine Lagrange Multiplikation umgeschrieben. Dadurch entstehen \\(\\alpha_i\\), welche als Ein- bzw. Ausschalter f\u00fcr die Gewichtung einer Vorhersage bei der Optimierung interpretiert werden k\u00f6nnen. Die Gewichtung der Vorhersage erfolgt durch die Summe der Produkte aus \\(\\alpha_i\\) und dem Kernel \\(K(x_i,x)\\), also \\(\\sum_{i=1}^n \\alpha_i K(x_i,x)\\). </p> <p>Bei der Verwendung eines Soft-Margins werden Schlupfvariablen \\(\\xi_i\\) einfed\u00fchrt, welche bei falschen Klassifizierungen den Einflus des Fehlers redizieren k\u00f6nnen. Dadurch werden beim Training Fehler zugelassen. Die Gewichtung der Fehler erfolgt dann durch \\(C \\sum_{i=1}^n \\xi_i\\). \\(C\\) steuert also, wie Stark Fehler gewichtet werden. Je gr\u00f6\u00dfer \\(C\\) ist, desto st\u00e4rker werden Fehler gewichtet.</p> <p>Der Parameter Gamma aus den Kernel-Funktionen gewichtet ebenfalls, wie stark die einzelnen Punkte in der Vorhersage gewichtet werden. Je gr\u00f6\u00dfer Gamma ist, desto st\u00e4rker werden die einzelnen Punkte gewichtet. Dies kann zu einer \u00dcberanpassung f\u00fchren.</p> <p>In der Praxis sollte Gammach nach M\u00f6glichkeit gering gehalten werden, um eine \u00dcberanpassung zu vermeiden. \\(C\\) sollte so gew\u00e4hlt werden, dass die Anzahl der falsch klassifizierten Punkte m\u00f6glichst gering ist. Dies kann durch eine Kreuzvalidierung ermittelt werden. Gute Parameterkombinationen k\u00f6nnen beispielsweise durch eine GridSearch ermittelt werden.</p> <p>Aufgrund der Konstruktion der SVM kann diese zun\u00e4chst nur in 2 Klassen (bin\u00e4r) unterscheiden. Um trotzdem mehr als 2 Klassen behandeln zu k\u00f6nnen, gibt es verschiedene Ans\u00e4tze. Diese vergleichen die einzelenen Klasse untereinander oder fassen sie zu tempor\u00e4r zu Oberklassen zusamme. Eine \u00dcbersicht zu zwei verbreiteten Ans\u00e4tzen findet ihr hier.</p>"},{"location":"KW2/svm/svm/#referenzen-und-nachschlagmaterial","title":"Referenzen und Nachschlagmaterial","text":"<ul> <li>Support Vector Machines in scikit-learn</li> <li>Erkl\u00e4rung SVM und Kernel</li> <li>Youtube Video zu SVM</li> </ul>"},{"location":"KW3/goals/","title":"Ziele","text":"<p>Ich kann...</p> <p>... Dashboards mit streamlit erstellen. ... Dashboards mit mehreren Seiten und einer Benutzerauthentifizierung erstellen. ... ein Dashboard mit einer API verbinden. ... erkl\u00e4ren, was Callbacks in Dash sind ... Dashboards in plotly Dash erstellen. ... Daten in plotly Dash zwischen Callbacks austauschen. ... plotly Dashboards debuggen. ... eine Basic Authentication in dash einbauen. ... die Begriffe State, Input und Output in einem Dash Dashboard erkl\u00e4ren. ... den Begriff Layout in Dash erkl\u00e4ren.</p>"},{"location":"KW3/intro/","title":"Einf\u00fchrung Dashboards","text":""},{"location":"KW3/intro/#einfuhrung-in-dashboards-mit-streamlit-und-dash","title":"Einf\u00fchrung in Dashboards mit Streamlit und Dash","text":"<p>Dashboards sind zentrale Werkzeuge, um Daten zu visualisieren und Einblicke interaktiv und \u00fcbersichtlich zu pr\u00e4sentieren. Sie erleichtern datenbasierte Entscheidungen, indem sie komplexe Informationen in Echtzeit aufbereiten und zug\u00e4nglich machen.</p> <p>Streamlit und Dash sind zwei Frameworks, die speziell f\u00fcr die Entwicklung von Dashboards mit Python genutzt werden. Beide bieten effiziente M\u00f6glichkeiten, Datenanwendungen zu erstellen, unterscheiden sich jedoch in ihrem Fokus:</p> <ul> <li>Streamlit: Konzentriert sich auf schnelle und unkomplizierte Dashboard-Erstellung mit minimalem Aufwand. Ideal f\u00fcr Prototypen und kleinere Projekte.</li> <li>Dash: Eignet sich f\u00fcr umfassendere und anpassbare Anwendungen, die spezifische Anforderungen erf\u00fcllen m\u00fcssen. Perfekt f\u00fcr Produktionsumgebungen.</li> </ul> <p>Dashboards mit diesen Tools erm\u00f6glichen interaktive Datenvisualisierungen, Filter, Eingaben und dynamische Analysen \u2013 sei es f\u00fcr die \u00dcberwachung von Unternehmenskennzahlen oder die Pr\u00e4sentation von Machine-Learning-Ergebnissen.</p> <p>Beide Bibliotheken besitzen eine sehr gute Dokumentation und sind dadruch schnell erlenbar. Wir werden daher auch an vielen Stellen direkt auf die Dokumentation verweisen.</p>"},{"location":"KW3/assignments/dash/","title":"Dash","text":""},{"location":"KW3/assignments/dash/#aufgabe-1","title":"Aufgabe 1","text":"<p>Ersstelle ein Dashboard, welches verschiedene Regressionsgeraden zeichnet.</p> <ol> <li>F\u00fcr die Eingabe sollten ein Auswahlelement f\u00fcr die Regressionsmodelle <code>SVR</code>, <code>DecisionTreeRegressor</code> und <code>LinearRegression</code> vorhanden sein.</li> <li>Au\u00dfedem sollten neue <code>X</code> und <code>Y</code> werte zu dem bisherigen DataFrame f\u00fcr die Berechnung der Regressionsgeraden hinzugef\u00fcgt werden k\u00f6nnen.</li> <li>Das DataFrame und die Regressionsgerade sollten in einem Graphen dargestellt werden.</li> </ol>"},{"location":"KW3/assignments/streamlit/","title":"Aufgaben Streamlit","text":""},{"location":"KW3/assignments/streamlit/#aufgabe-1","title":"Aufgabe 1","text":""},{"location":"KW3/assignments/streamlit/#aufgabe-11","title":"Aufgabe 1.1","text":"<p>Erstelle eine mehrseitiges Streamlit-Web-App, welche als Nachschlagewertr f\u00fcr die Grundlagen des Machine Learnings dient. Die App soll folgende Seiten enthalten:</p> <ol> <li>Startseite: Begr\u00fc\u00dfung und kurze Einleitung</li> <li> <ul> <li>Was ist Machine Learning?</li> </ul> </li> <li> <ul> <li>Welche Arten von Algorithmen gibt es?</li> </ul> </li> <li> <ul> <li>Wie funktioniert ein Machine Learning Modell?</li> </ul> </li> <li> <ul> <li>Was sind Traingins- und Testdaten?</li> </ul> </li> <li> <p>Modelle: Hier sollen die wichtigsten Machine Learning Modelle vorgestellt werden. Jedes der bisher behandelten Modelle erh\u00e4lt eine Unterseite</p> </li> <li> <ul> <li>Lineare Regression</li> </ul> </li> <li> <ul> <li>SVM (Support Vector Machine)</li> </ul> </li> <li> <ul> <li>Decision Trees</li> </ul> </li> </ol> <p>Hinweis: Aufgabe 2 kann in Gruppen bearbeitet werden. Jede Person erh\u00e4lt ein Modell zugewiesen, welches sie vorstellen soll.</p>"},{"location":"KW3/assignments/streamlit/#aufgabe-12","title":"Aufgabe 1.2","text":"<p>F\u00fcge zu jedem Modell ein Beispiel hinzu, welches die Funktionsweise des Modells verdeutlicht. Als Datensa\u00e4tze k\u00f6nnen die Beispiele aus den bisherigen Aufgaben oder Beispieldatens\u00e4tze aus scikit-learn verwendet werden.</p>"},{"location":"KW3/assignments/streamlit/#aufgabe-13","title":"Aufgabe 1.3","text":"<p>F\u00fcge der Streamlit App eine Authentifikation hinzu. Der Nutzer soll sich mit einem Passwort anmelden k\u00f6nnen, um die App zu nutzen. Das Passwort soll in einer Umgebungsvariable gespeichert werden.</p>"},{"location":"KW3/assignments/streamlit/#aufgabe-2","title":"Aufgabe 2","text":"<p>Sieh dir den Datensatz zu t\u00e4glichen Eins\u00e4tzen der Berliner Feuerwehr an https://github.com/Berliner-Feuerwehr/BF-Open-Data/blob/main/Datasets/Daily_Data/BFw_mission_data_daily.csv. </p> <p>Hier findest du eine Beschreibung der Spalten:</p> Spalte Erkl\u00e4rung mission_created_date Datum, an dem die Mission erstellt wurde. mission_count_all Gesamtzahl der Eins\u00e4tze. mission_count_ems Anzahl der medizinischen Notfalleins\u00e4tze (EMS). mission_count_ems_critical Anzahl der kritischen medizinischen Notfalleins\u00e4tze (EMS). mission_count_ems_critical_cpr Anzahl der kritischen EMS-Eins\u00e4tze, bei denen CPR erforderlich war. mission_count_fire Anzahl der Brandeins\u00e4tze. mission_count_technical_rescue Anzahl der technischen Rettungseins\u00e4tze. response_time_ems_critical_mean Durchschnittliche Reaktionszeit f\u00fcr kritische EMS-Eins\u00e4tze. response_time_ems_critical_median Median der Reaktionszeit f\u00fcr kritische EMS-Eins\u00e4tze. response_time_ems_critical_std Standardabweichung der Reaktionszeit f\u00fcr kritische EMS-Eins\u00e4tze. response_time_ems_critical_cpr_mean Durchschnittliche Reaktionszeit f\u00fcr kritische EMS-Eins\u00e4tze mit CPR. response_time_ems_critical_cpr_median Median der Reaktionszeit f\u00fcr kritische EMS-Eins\u00e4tze mit CPR. response_time_ems_critical_cpr_std Standardabweichung der Reaktionszeit f\u00fcr kritische EMS-Eins\u00e4tze mit CPR. response_time_fire_time_to_first_pump_mean Durchschnittliche Zeit bis zum ersten Pumpeneinsatz bei Brandeins\u00e4tzen. response_time_fire_time_to_first_pump_median Median der Zeit bis zum ersten Pumpeneinsatz bei Brandeins\u00e4tzen. response_time_fire_time_to_first_pump_std Standardabweichung der Zeit bis zum ersten Pumpeneinsatz bei Brandeins\u00e4tzen. response_time_fire_time_to_first_ladder_mean Durchschnittliche Zeit bis zur ersten Leiter bei Brandeins\u00e4tzen. response_time_fire_time_to_first_ladder_median Median der Zeit bis zur ersten Leiter bei Brandeins\u00e4tzen. response_time_fire_time_to_first_ladder_std Standardabweichung der Zeit bis zur ersten Leiter bei Brandeins\u00e4tzen. response_time_fire_time_to_full_crew_mean Durchschnittliche Zeit bis zum Einsatz der vollst\u00e4ndigen Crew bei Brandeins\u00e4tzen. response_time_fire_time_to_full_crew_median Median der Zeit bis zum Einsatz der vollst\u00e4ndigen Crew bei Brandeins\u00e4tzen. response_time_fire_time_to_full_crew_std Standardabweichung der Zeit bis zum Einsatz der vollst\u00e4ndigen Crew bei Brandeins\u00e4tzen. response_time_technical_rescue_mean Durchschnittliche Reaktionszeit f\u00fcr technische Rettungseins\u00e4tze. response_time_technical_rescue_median Median der Reaktionszeit f\u00fcr technische Rettungseins\u00e4tze. response_time_technical_rescue_std Standardabweichung der Reaktionszeit f\u00fcr technische Rettungseins\u00e4tze."},{"location":"KW3/assignments/streamlit/#21-erstellen-sie-ein-interaktives-dashboard-zur-visualisierung-der-taglichen-einsatzzahlen","title":"2.1 Erstellen Sie ein interaktives Dashboard zur Visualisierung der t\u00e4glichen Einsatzzahlen","text":"<ol> <li>Filtern Sie nach Datum und Einsatzarten (z. B. <code>mission_count_fire</code>, <code>mission_count_ems</code>), um die Daten nach verschiedenen Kriterien anzuzeigen.</li> <li>Visualisieren Sie die Anzahl der Eins\u00e4tze pro Tag/ Woche/ Monat.</li> <li>Zeigen Sie die durchschnittlichen Reaktionszeiten f\u00fcr alle Einsatzarten (z. B. <code>response_time_fire_time_to_first_pump_mean</code>) in einem Liniendiagramm an.</li> <li>Erstellen Sie interaktive Widgets, mit denen die Benutzer den Zeitraum und die Einsatzarten nach Bedarf anpassen k\u00f6nnen.</li> </ol>"},{"location":"KW3/assignments/streamlit/#22-erstellen-sie-ein-modell-zur-vorhersage-der-reaktionszeit-fur-brandeinsatze","title":"2.2 Erstellen Sie ein Modell zur Vorhersage der Reaktionszeit f\u00fcr Brandeins\u00e4tze","text":"<ol> <li>Verwenden Sie <code>mission_count_fire</code> und <code>mission_count_ems_critical</code> als Eingabefunktionen f\u00fcr Ihr Modell.</li> <li>Bauen Sie ein lineares Regressionsmodell, das die durchschnittliche Reaktionszeit f\u00fcr Brandeins\u00e4tze (<code>response_time_fire_time_to_first_pump_mean</code>) vorhersagt.</li> <li>Bewerten Sie das Modell anhand der mittleren quadratischen Abweichung (MSE), um die Genauigkeit Ihrer Vorhersagen zu pr\u00fcfen.</li> </ol>"},{"location":"KW3/assignments/streamlit/#zusatz-23-alt-kategorisieren-sie-einsatze-als-kritisch-oder-nicht-kritisch","title":"Zusatz: 2.3 ALT Kategorisieren Sie Eins\u00e4tze als \"kritisch\" oder \"nicht-kritisch\"","text":"<ol> <li>Trainieren Sie ein Klassifikationsmodell (z. B. SVM oder Entscheidungsbaum), das auf <code>mission_count_ems_critical</code> und <code>response_time_ems_critical_mean</code> basiert. Gibt es einen zusammenhang?</li> <li>Evaluieren Sie das Modell (mit den Metriken Pr\u00e4zision, Recall und F1-Score), um die Leistung der Klassifikation zu bewerten.</li> </ol>"},{"location":"KW3/assignments/streamlit/#zusatz-23-kategorisieren-sie-einsatze","title":"Zusatz: 2.3 Kategorisieren Sie Eins\u00e4tze","text":"<ol> <li>Trainieren Sie ein Klassifikationsmodell (z. B. SVM oder Entscheidungsbaum), das auf den Datensatz https://github.com/Berliner-Feuerwehr/BF-Open-Data/blob/main/Datasets/Mission_Data/mission_data_set_open_data_2024.csv zur\u00fcckgreift. Das Modell soll die Klasse <code>dispatchcode_criticality</code> vorhersagen</li> <li>Evaluieren Sie das Modell (mit den Metriken Pr\u00e4zision, Recall und F1-Score), um die Leistung der Klassifikation zu bewerten.</li> <li>Erstellen Sie ein Regressionsmodell, um die <code>response_time</code>in diesem </li> </ol>"},{"location":"KW3/assignments/streamlit/#24-erstellen-sie-ein-tool-zur-analyse-der-reaktionszeiten-fur-kritische-und-nicht-kritische-einsatze","title":"2.4 Erstellen Sie ein Tool zur Analyse der Reaktionszeiten f\u00fcr kritische und nicht-kritische Eins\u00e4tze","text":"<ol> <li>Visualisieren Sie die durchschnittliche Reaktionszeit f\u00fcr kritische EMS-Eins\u00e4tze (<code>response_time_ems_critical_mean</code>) und f\u00fcr Brandeins\u00e4tze (<code>response_time_fire_time_to_first_pump_mean</code>).</li> <li>Implementieren Sie interaktive Widgets, mit denen die Benutzer verschiedene Zeitr\u00e4ume ausw\u00e4hlen und die Reaktionszeiten vergleichen k\u00f6nnen.</li> </ol>"},{"location":"KW3/assignments/streamlit/#25-erstellen-sie-ein-kreisdiagramm-zur-darstellung-der-verteilung-der-einsatzarten","title":"2.5 Erstellen Sie ein Kreisdiagramm zur Darstellung der Verteilung der Einsatzarten","text":"<ol> <li>Wie verteilen sich die verschiedenen Arten von Eins\u00e4tzen (Br\u00e4nde, EMS, technische Rettung) \u00fcber den gesamten Zeitraum?</li> <li>Welche Kategorie stellt den gr\u00f6\u00dften Anteil an den Eins\u00e4tzen dar, und wie hoch ist dieser Anteil im Vergleich zu den anderen Kategorien?</li> </ol>"},{"location":"KW3/assignments/streamlit/#26-erstellen-sie-ein-liniendiagramm-zur-darstellung-der-entwicklung-der-einsatzzahlen-im-zeitverlauf","title":"2.6 Erstellen Sie ein Liniendiagramm zur Darstellung der Entwicklung der Einsatzzahlen im Zeitverlauf","text":"<ol> <li>Wie haben sich die Gesamtzahlen der Eins\u00e4tze \u00fcber die Zeit ver\u00e4ndert?</li> <li>Lassen sich saisonale Schwankungen oder langfristige Trends in den Einsatzzahlen erkennen?</li> </ol>"},{"location":"KW3/assignments/streamlit/#27-erstellen-sie-ein-boxplot-zur-darstellung-der-reaktionszeiten-response_time-fur-verschiedene-einsatzarten","title":"2.7 Erstellen Sie ein Boxplot zur Darstellung der Reaktionszeiten (<code>response_time...</code>) f\u00fcr verschiedene Einsatzarten","text":"<ol> <li>Gibt es signifikante Unterschiede in den Reaktionszeiten f\u00fcr die verschiedenen Einsatzarten (EMS, Br\u00e4nde, technische Rettung)?</li> <li>Welche Einsatzarten zeigen die l\u00e4ngsten oder k\u00fcrzesten Reaktionszeiten?</li> </ol>"},{"location":"KW3/assignments/streamlit/#28-erstellen-sie-ein-streudiagramm-um-die-korrelation-zwischen-der-anzahl-der-einsatze-und-den-reaktionszeiten-zu-visualisieren","title":"2.8 Erstellen Sie ein Streudiagramm, um die Korrelation zwischen der Anzahl der Eins\u00e4tze und den Reaktionszeiten zu visualisieren","text":"<ol> <li>Gibt es eine erkennbare Korrelation zwischen der Anzahl der Eins\u00e4tze und den Reaktionszeiten?</li> <li>Wie ver\u00e4ndert sich die Reaktionszeit, wenn die Zahl der Eins\u00e4tze steigt oder sinkt?</li> </ol>"},{"location":"KW3/assignments/streamlit/#29-erstellen-sie-ein-balkendiagramm-zur-darstellung-der-durchschnittlichen-reaktionszeiten-fur-verschiedene-einsatzarten","title":"2.9 Erstellen Sie ein Balkendiagramm zur Darstellung der durchschnittlichen Reaktionszeiten f\u00fcr verschiedene Einsatzarten","text":"<ol> <li>Welche Einsatzart weist die l\u00e4ngste durchschnittliche Reaktionszeit auf?</li> <li>Welche Trends oder Ver\u00e4nderungen in den Reaktionszeiten sind im Zeitverlauf zu beobachten?</li> </ol>"},{"location":"KW3/assignments/streamlit/#210-erstellen-sie-ein-histogramm-zur-darstellung-der-haufigkeit-der-kritischen-einsatze-mission_count__critical","title":"2.10 Erstellen Sie ein Histogramm zur Darstellung der H\u00e4ufigkeit der kritischen Eins\u00e4tze (<code>mission_count_..._critical</code>)","text":"<ol> <li>Wie h\u00e4ufig treten kritische Eins\u00e4tze auf, und wie hoch ist deren Anteil an den Gesamteins\u00e4tzen?</li> <li>Gibt es spezifische Zeitr\u00e4ume oder Tage, an denen besonders viele oder wenige kritische Eins\u00e4tze auftraten?</li> </ol>"},{"location":"KW3/streamlit/caching/","title":"Caching","text":"<p>Caching ist ein zentraler Bestandteil der Streamlit-Architektur, der dabei hilft, die Leistung von Anwendungen zu optimieren. Es erm\u00f6glicht das Zwischenspeichern von aufwendigen Berechnungen, Datenbankabfragen oder anderen ressourcenintensiven Operationen, sodass diese nicht bei jedem Neuladen erneut ausgef\u00fchrt werden m\u00fcssen.  </p> <p>Streamlit bietet zwei Arten von Caching:  </p> <ul> <li>Daten-Caching (<code>@st.cache_data</code>)   Diese Methode speichert Ergebnisse von Funktionen, die Daten laden oder verarbeiten. Sie ist besonders n\u00fctzlich f\u00fcr Operationen wie das Laden von gro\u00dfen CSV-Dateien oder API-Anfragen.  </li> </ul> <p>Beispiel:   ```python   import streamlit as st   import pandas as pd</p> <p>@st.cache_data   def load_data():       return pd.read_csv(\"daten.csv\")</p> <p>data = load_data()   st.write(data)   ```  </p> <ul> <li>Objekt-Caching (<code>@st.cache_resource</code>)   Diese Methode wird f\u00fcr Objekte genutzt, die instanziiert werden m\u00fcssen, wie Datenbankverbindungen oder Modelle.  </li> </ul> <p>Beispiel:   ```python   import streamlit as st   import sqlalchemy</p> <p>@st.cache_resource   def create_connection():       return sqlalchemy.create_engine(\"sqlite:///datenbank.db\")</p> <p>connection = create_connection()   ```  </p> <p>Grundprinzipien des Cachings - Determinismus: Der Output einer gecachten Funktion muss bei gleichen Input-Werten immer gleich sein. - Eingabewerte: Streamlit verwendet die Parameter der Funktion, um zu entscheiden, ob eine gecachte Version wiederverwendet wird. - Unver\u00e4nderbarkeit: Objekte, die an gecachte Funktionen \u00fcbergeben werden, sollten nicht ver\u00e4ndert werden, da dies zu unerwartetem Verhalten f\u00fchren kann.  </p> <p>Wann Cache-Inhalte aktualisiert werden: - \u00c4nderungen am Quellcode. - \u00c4nderungen der Funktionseingaben. - Explizites Leeren des Caches \u00fcber <code>st.cache_data.clear()</code> oder <code>st.cache_resource.clear()</code>.  </p> <p>Caching ist ein m\u00e4chtiges Werkzeug, um die Performance von Streamlit-Apps zu verbessern. Dennoch sollte es bewusst eingesetzt werden, um potenzielle Probleme, wie das Zwischenspeichern veralteter Daten, zu vermeiden.  </p> <p>Weitere Informationen: Streamlit Dokumentation \u2013 Caching</p>"},{"location":"KW3/streamlit/data-handling/","title":"Umgang mit Daten","text":"<p>Daten effizient hochzuladen und anzuzeigen ist ein zentraler Bestandteil jedes Dashboards. Streamlit bietet M\u00f6glichkeiten, Daten direkt aus Dateien oder Datenbanken zu laden und interaktiv darzustellen.</p> <p>Beispiel </p> <pre><code>import streamlit as st\nimport pandas as pd\n\nuploaded_file = st.file_uploader(\"Lade eine CSV-Datei hoch\")\nif uploaded_file is not None:\n    df = pd.read_csv(uploaded_file)\n    st.dataframe(df)\n</code></pre> <p>Dar\u00fcber hinaus k\u00f6nnen wir Daten auch direkt aus Datenbank abfragen und anzeigen:</p> <p>Beispiel </p> <pre><code>import streamlit as st\n\n# Verbindung zur Datenbank\nconn = sqlite3.connect(\"example.db\")\n\n# SQL-Abfrage\nquery = \"SELECT * FROM table\"\ndf = pd.read_sql(query, conn)\n\n# Anzeige der Daten\nst.dataframe(df)\n</code></pre> <p>Dies umgeht in einigen F\u00e4llen die Notwendigkeit, Daten lokal zu speichern und erm\u00f6glicht es, Daten direkt in der Web-App zu verarbeiten.</p> <p>Link zum Nachschlagen Streamlit Dokumentation - File Uploader</p>"},{"location":"KW3/streamlit/interactivity/","title":"Interaktivit\u00e4t","text":"<p>Anders als in Flask wird Interaktivit\u00e4t in Streamlit sehr intuitiv erm\u00f6glicht. Zust\u00e4nde k\u00f6nnen wir so entweder in der Session oder sogar in Variablen direkt speichern und aktualisieren. Zu beachten ist hierbei jedoch, dass im Standardzustand das Streamlit Skript lediglich einmal von oben nach unten durchlaufend interpretiert wird.</p> <p>Beispiel </p> <pre><code>import streamlit as st\n\nif \"counter\" not in st.session_state:\n    st.session_state.counter = 0\n\nif st.button(\"Erh\u00f6he Z\u00e4hler\"):\n    st.session_state.counter += 1\n\nst.write(f\"Z\u00e4hler: {st.session_state.counter}\")\n</code></pre> <p>Link zum Nachschlagen Streamlit Dokumentation - Session State</p>"},{"location":"KW3/streamlit/intro/","title":"Einf\u00fchrung","text":"<p>Streamlit ist eine Python-Bibliothek, die es erm\u00f6glicht, interaktive Web-Apps f\u00fcr Datenanalyse und Visualisierung mit minimalem Code zu erstellen. Es bietet eine schnelle M\u00f6glichkeit, Datenprojekte zu teilen und Ergebnisse interaktiv zu pr\u00e4sentieren. Besonders geeignet ist Streamlit f\u00fcr Entwickler und Analysten, die ohne tiefere Frontend-Kenntnisse visuelle Dashboards erstellen m\u00f6chten.</p> <p>Beispiel </p> <pre><code>import streamlit as st\n\nst.title(\"Willkommen zu Streamlit\")\nst.write(\"Erstelle Dashboards mit wenigen Zeilen Code!\")\n</code></pre> <p>Link zum Nachschlagen Streamlit Dokumentation - Erste Schritte</p>"},{"location":"KW3/streamlit/intro/#einige-funktionen-von-streamlit","title":"Einige Funktionen von Streamlit","text":"<p>Streamlit bietet viele n\u00fctzliche Funktionen, um interaktive und ansprechende Web-Apps zu erstellen:</p> <ul> <li>Widgets: F\u00fcgen Sie interaktive Widgets wie Schieberegler, Dropdown-Men\u00fcs und Textfelder hinzu.</li> <li>Layouts: Organisieren Sie Ihre App mit Spalten, Registerkarten und Seitenleisten.</li> <li>Visualisierungen: Integrieren Sie Plotly, Matplotlib, Altair und andere Bibliotheken f\u00fcr beeindruckende Visualisierungen.</li> <li>Datenanzeige: Zeigen Sie Datenrahmen und Tabellen direkt in Ihrer App an.</li> </ul> <p>Beispiel f\u00fcr ein Widget </p> <pre><code>import streamlit as st\n\noption = st.selectbox(\n    'Wie m\u00f6chten Sie fortfahren?',\n    ('Option 1', 'Option 2', 'Option 3'))\n\nst.write('Sie haben gew\u00e4hlt:', option)\n</code></pre> <p>Link zu weiteren Ressourcen Streamlit Dokumentation - Widgets</p>"},{"location":"KW3/streamlit/layout/","title":"Layout","text":"<p>Oftmals m\u00f6chten wir unsere App in mehrere Bereiche aufteilen. Dies k\u00f6nnen wir beispiesweise druch Spalten und Zeilen umsetzen. In Streamlit erfolgt dies durch die Erstellung eines Zeilen/ Spaltenobjektes. Den einzelnen Spalten/ Zeilen k\u00f6nnen anschlie\u00dfend Komponenten zugewiesen werden.</p> <p>Beispiel </p> <pre><code>import streamlit as st\n\nst.title(\"Dashboard Layout\")\ncol1, col2 = st.columns(2)\ncol1.write(\"Dies ist Spalte 1\")\ncol2.write(\"Dies ist Spalte 2\")\n</code></pre> <p>Neben Spalten und Zeilen k\u00f6nnen wir auch weitere Elemente, wie beispieldweise eine Seitenleiste hinzuf\u00fcgen. Hierf\u00fcr k\u00f6nnen wir die <code>st.sidebar</code>-Funktion verwenden.</p> <p>Beispiel </p> <pre><code>import streamlit as st\n\n# Using object notation\nadd_selectbox = st.sidebar.selectbox(\n    \"How would you like to be contacted?\",\n    (\"Email\", \"Home phone\", \"Mobile phone\")\n)\n\n# Using \"with\" notation\nwith st.sidebar:\n    add_radio = st.radio(\n        \"Choose a shipping method\",\n        (\"Standard (5-15 days)\", \"Express (2-5 days)\")\n    )\n</code></pre> <p>Link zum Nachschlagen Streamlit Dokumentation - Columns Sreamlit Dokumentation - Sidebar</p>"},{"location":"KW3/streamlit/multipage-apps/","title":"Multipage Apps","text":"<p>Mit Multipage-Apps erm\u00f6glicht Streamlit die Erstellung von Anwendungen mit mehreren Ansichtsseiten. Dies verbessert die Benutzerfreundlichkeit und Struktur von komplexen Dashboards und Apps, indem Inhalte logisch auf verschiedene Seiten verteilt werden k\u00f6nnen.  </p>"},{"location":"KW3/streamlit/multipage-apps/#grundkonzept","title":"Grundkonzept","text":"<p>Eine Multipage-App in Streamlit basiert auf dem Konzept eines <code>pages</code>-Verzeichnisses: - Jede Unterseite ist eine eigene Python-Datei im Ordner <code>pages</code>. - Streamlit l\u00e4dt diese Seiten automatisch und f\u00fcgt sie als ausw\u00e4hlbare Tabs in die Anwendung ein. - Jede Datei wird unabh\u00e4ngig ausgef\u00fchrt, sobald sie ausgew\u00e4hlt wird.  </p>"},{"location":"KW3/streamlit/multipage-apps/#einrichten-einer-multipage-app","title":"Einrichten einer Multipage-App","text":"<ol> <li>Hauptskript:    Das Hauptskript bleibt die Startseite deiner App.    ```python    import streamlit as st</li> </ol> <p>st.title(\"Hauptseite\")    st.write(\"Willkommen zu meiner Multipage-App!\")    ```  </p> <ol> <li> <p>Erstellen des <code>pages</code>-Ordners:    Lege ein Verzeichnis namens <code>pages</code> im gleichen Verzeichnis wie dein Hauptskript an.  </p> </li> <li> <p>Erstellen von Seiten:    F\u00fcge Python-Dateien in den <code>pages</code>-Ordner ein, z. B.:  </p> </li> <li><code>pages/Seite_1.py</code> </li> <li> <p><code>pages/Seite_2.py</code>    Jede Datei entspricht einer eigenen Seite.  </p> </li> <li> <p>Seitenstruktur und Inhalte:    Jede Datei verh\u00e4lt sich wie ein normales Streamlit-Skript:    ```python    import streamlit as st</p> </li> </ol> <p>st.title(\"Seite 1\")    st.write(\"Dies ist die erste Unterseite.\")    ```  </p>"},{"location":"KW3/streamlit/multipage-apps/#dateibenennung-und-reihenfolge","title":"Dateibenennung und Reihenfolge","text":"<ul> <li>Standardm\u00e4\u00dfig werden die Seiten alphabetisch im Men\u00fc sortiert.  </li> <li>F\u00fcr eine benutzerdefinierte Reihenfolge kannst du Zahlen voranstellen:  </li> <li><code>pages/1_Seite_1.py</code> </li> <li><code>pages/2_Seite_2.py</code> </li> <li>Diese Zahlen erscheinen nicht im Men\u00fc, sondern dienen nur der Sortierung.  </li> </ul>"},{"location":"KW3/streamlit/multipage-apps/#navigation-zwischen-seiten","title":"Navigation zwischen Seiten","text":"<p>Streamlit f\u00fcgt automatisch ein Men\u00fc in der linken Seitenleiste hinzu, \u00fcber das Benutzer zwischen den Seiten navigieren k\u00f6nnen.  </p> <p>Weitere Informationen: Streamlit Dokumentation \u2013 Multipage Apps </p>"},{"location":"KW3/streamlit/outlook/","title":"Ausblick","text":"<p>Neben den grundlegenden Grafiken und Textelementen sind in Streamlit auch fortgeschrittene Visualisierungen, wie beispielsweise das Anzeigen interaktiver Karten m\u00f6glich.</p> <p>Beispiel </p> <pre><code>import streamlit as st\nimport pandas as pd\n\n# Beispiel-Daten mit Geokoordinaten\ndata = pd.DataFrame({\n    \"lat\": [52.5200, 48.8566, 40.7128],\n    \"lon\": [13.4050, 2.3522, -74.0060]\n})\n\nst.map(data)\n</code></pre> <p>Link zum Nachschlagen Streamlit Dokumentation - Map</p>"},{"location":"KW3/streamlit/secrets-authentication/","title":"Secrets und Authentication","text":"<p>In Streamlit gibt es eine einfache M\u00f6glichkeit, geheime Daten wie API-Schl\u00fcssel oder Passw\u00f6rter sicher zu verwalten: Das Secrets Management. Hierbei wird eine Datei namens <code>.streamlit/secrets.toml</code> verwendet, um diese sensiblen Informationen zu speichern. Diese Datei wird im Projektverzeichnis abgelegt und erm\u00f6glicht es, die Daten \u00fcber den Befehl <code>st.secrets</code> im Code zu verwenden, ohne sie direkt im Quellcode zu hinterlegen.</p> <p>Die Struktur der Datei folgt dem TOML-Format und k\u00f6nnte beispielsweise folgenderma\u00dfen aussehen:</p> <pre><code>[database]\nhost = \"localhost\"\nuser = \"user\"\npassword = \"password\"\n</code></pre> <p>Auf die in der Datei gespeicherten Werte kann im Code wie folgt zugegriffen werden:</p> <pre><code>import streamlit as st\ndb_host = st.secrets[\"database\"][\"host\"]\ndb_user = st.secrets[\"database\"][\"user\"]\ndb_password = st.secrets[\"database\"][\"password\"]\n</code></pre> <p>Dieses Vorgehen sch\u00fctzt sensible Informationen vor unbeabsichtigtem Zugriff, etwa \u00fcber \u00f6ffentliche Repositories. Bei der Bereitstellung der Anwendung auf Streamlit Cloud k\u00f6nnen geheime Daten \u00fcber das Interface im Dashboard hinzugef\u00fcgt werden, sodass diese ebenfalls sicher und ohne manuelle Konfiguration verf\u00fcgbar sind.</p> <p>Vorteile des Secrets Managements: - Sicherheit: Keine sensiblen Daten im Quellcode. - Nachvollziehbarkeit: Eine klare Trennung zwischen Code und geheimen Informationen. - Einfache Handhabung: Sicherer Zugriff auf vertrauliche Daten ohne zus\u00e4tzliche Setup-Schritte. </p> <p>Durch diese Methode wird gew\u00e4hrleistet, dass deine sensiblen Daten jederzeit sicher bleiben, insbesondere bei der Bereitstellung in der Cloud.</p>"},{"location":"KW3/streamlit/session/","title":"Session","text":"<p>In Streamlit-Anwendungen erm\u00f6glicht der Session State, Daten und Zust\u00e4nde \u00fcber mehrere Interaktionen hinweg zu speichern. Er ist essenziell, um dynamische und interaktive Anwendungen zu erstellen, bei denen Werte nicht bei jedem Skript-Neustart zur\u00fcckgesetzt werden.  </p>"},{"location":"KW3/streamlit/session/#funktionen-von-session-state","title":"Funktionen von Session State","text":"<p>Der Session State bietet eine M\u00f6glichkeit, Informationen wie Benutzerinteraktionen, Z\u00e4hler oder Formulareingaben w\u00e4hrend der Laufzeit der App zu verwalten. </p>"},{"location":"KW3/streamlit/session/#verwendung-von-session-state","title":"Verwendung von Session State","text":"<ul> <li>Initialisierung von Zust\u00e4nden:   \u00dcberpr\u00fcfe, ob ein Zustand existiert, und setze einen Standardwert, falls nicht.   ```python   import streamlit as st</li> </ul> <p>if \"counter\" not in st.session_state:       st.session_state.counter = 0   ```  </p> <ul> <li>Manipulation von Zust\u00e4nden:   Aktualisiere den Zustand durch Interaktionen, z. B. bei Button-Klicks.   ```python   if st.button(\"Z\u00e4hler erh\u00f6hen\"):       st.session_state.counter += 1</li> </ul> <p>st.write(f\"Aktueller Z\u00e4hlerwert: {st.session_state.counter}\")   ```  </p> <ul> <li>Callbacks mit Session State:   Widgets k\u00f6nnen durch Callbacks Zust\u00e4nde direkt \u00e4ndern.   ```python   def reset_counter():       st.session_state.counter = 0</li> </ul> <p>st.button(\"Z\u00e4hler zur\u00fccksetzen\", on_click=reset_counter)   ```  </p>"},{"location":"KW3/streamlit/session/#besonderheiten-von-session-state","title":"Besonderheiten von Session State","text":"<ol> <li>Permanenz w\u00e4hrend der Sitzung: Werte bleiben w\u00e4hrend einer Benutzersitzung erhalten, unabh\u00e4ngig davon, wie oft das Skript neu geladen wird.  </li> <li>Key-basierte Struktur: Zust\u00e4nde werden mithilfe eines Dictionary-\u00e4hnlichen Keys gespeichert und abgerufen. <code>python    st.session_state[\"username\"] = \"Benutzer123\"    st.write(st.session_state[\"username\"])</code> </li> </ol>"},{"location":"KW3/streamlit/session/#anwendungsfalle-fur-den-session-state","title":"Anwendungsf\u00e4lle f\u00fcr den Session State","text":"<ul> <li>Interaktive Formulare: Bewahre Zwischenergebnisse und Eingabewerte w\u00e4hrend einer Sitzung.  </li> <li>Fortschrittsverfolgung: Verwalte schrittweise Prozesse wie Multi-Step-Formulare oder Workflow-Apps.  </li> <li>Zustandsabh\u00e4ngige UI: \u00c4ndere die Darstellung von Elementen basierend auf gespeicherten Werten.  </li> </ul>"},{"location":"KW3/streamlit/session/#best-practices","title":"Best Practices","text":"<ul> <li>Initialisiere alle Schl\u00fcssel zu Beginn der App, um Probleme mit nicht existierenden Werten zu vermeiden.  </li> <li>Nutze Callbacks, um Code sauber und modular zu halten.  </li> <li>L\u00f6sche den Zustand gezielt, falls veraltete Werte das Verhalten der App beeintr\u00e4chtigen k\u00f6nnten.  </li> </ul> <p>Weitere Informationen: Streamlit Dokumentation \u2013 Session State</p>"},{"location":"KW3/streamlit/structure/","title":"Struktur","text":"<p>In Streamlit gibt es Layout-Optionen, um Inhalte effizient zu organisieren. Mit diesen kannst du die Inhalte deiner App durch Layout-Optionen wie Seitenleisten oder Mehrspaltenansichten \u00fcbersichtlich gestalten.</p> <p>Beispiel </p> <pre><code>import streamlit as st\n\n# Sidebar und Hauptbereich\nst.sidebar.title(\"Navigation\")\nst.sidebar.radio(\"Seiten\", [\"Home\", \"Kontakt\"])\n\nst.title(\"Hauptbereich\")\nst.write(\"Hier steht der Hauptinhalt.\")\n</code></pre> <p>Die Struktur ist grunds\u00e4tzlich so aufgebaut, dass Elemente untereinander in einer Spalte organisiert werden. Die einzelnen Elemente (Widgets) werden \u00fcber <code>st.__element_name__</code>aufgerufen.</p> <p>Um eine App zu starten, wird der Befehl <code>streamlit run your_script.py</code>verwendet. Das funktioniert \u00fcbrigens sogar mit Github URLs. </p>"},{"location":"KW3/streamlit/structure/#funktionsweise-von-streamlit-apps","title":"Funktionsweise von Streamlit Apps","text":"<p>Streamlit-Apps basieren auf einer Architektur, die das Schreiben wie bei gew\u00f6hnlichen Python-Skripten erm\u00f6glicht. Die gesamte App wird bei \u00c4nderungen neu ausgef\u00fchrt, und zwar in folgenden F\u00e4llen:</p> <ul> <li>Wenn der Quellcode der App ge\u00e4ndert wird.  </li> <li>Wenn Nutzer mit Widgets interagieren (z. B. Schieberegler bewegen, Text eingeben, Buttons klicken).  </li> <li>Bei Nutzung von Callbacks (z. B. \u00fcber <code>on_change</code> oder <code>on_click</code>), die immer vor dem Rest des Skripts ausgef\u00fchrt werden.  </li> </ul> <p>Um die Performance zu optimieren, nutzt Streamlit den <code>@st.cache_data</code>-Decorator, der teure Berechnungen beim erneuten Ausf\u00fchren der App \u00fcberspringt.</p> <p>Link zum Nachschlagen Streamlit Dokumentation - Hauptkonzepte</p>"},{"location":"KW3/streamlit/visualization/","title":"Visualisierungen","text":"<p>Einer der Hauptvorteile von Streamlit ist, dass es daf\u00fcr ausgelegt wurde, mit Dataframes und Visualisierungen zu arbeiten. Dadurch k\u00f6nnen grafiken sehr schnell und einfach erstellt werden. Bibliotheken wie Matplotlib, Seaborn oder Plotly sind direkt integriert. Diese Flexibilit\u00e4t erm\u00f6glicht es, visuelle Darstellungen individuell an die Bed\u00fcrfnisse deines Projekts anzupassen, sei es f\u00fcr explorative Analysen oder die Kommunikation von Ergebnissen.</p> <p>Beispiel </p> <pre><code>import streamlit as st\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Daten und Plot\ndata = pd.DataFrame({\"Kategorie\": [\"A\", \"B\", \"C\"], \"Werte\": [10, 20, 15]})\nfig, ax = plt.subplots()\ndata.plot(kind=\"bar\", x=\"Kategorie\", y=\"Werte\", ax=ax)\n\n# Ausgabe\nst.pyplot(fig)\n</code></pre> <p>Streamlit bietet mit sogenannten <code>magic commands</code> sogar eine Abk\u00fcrzung, um Texte und Dataframes auszugeben. Diese k\u00f6nnen direkt in den Code integriert werden und ersparen dir das Schreiben von <code>st.write()</code> oder <code>st.dataframe()</code>.</p> <p>Link zum Nachschlagen Streamlit Dokumentation - Charts Streamlit Magic Commands</p>"},{"location":"KW4/goals/","title":"Ziele","text":"<p>Ich kann...</p> <p>... Dashboards mit Streamlit erstellen ... Dashboards mit Plotly Dash erstellen. ... den Unterschied zwischen Streamlit und Dash erkl\u00e4ren. ... den Unterschied zwischen Dash und flask erkl\u00e4ren. ... Dashboards mit mehreren Unterseiten erstellen. ... mein Dashboard in einer Cloud bereitstellen. ... Over- und Undersampling-Methoden erkl\u00e4ren. ... SMOTE erkl\u00e4ren. ... die Idee hinter PCA erkl\u00e4ren. ... eine PCA in Python anwenden.</p>"},{"location":"KW4/knn/","title":"K-Nearest Neighbors (KNN) - Grundlagen und Implementierung","text":""},{"location":"KW4/knn/#1-grundprinzip","title":"1. Grundprinzip","text":"<p>KNN ist ein instanzbasierter Lernalgorithmus, der sowohl f\u00fcr Klassifikation als auch Regression verwendet werden kann. Er basiert auf dem einfachen Prinzip: \"Sage mir, wer deine Nachbarn sind, und ich sage dir, wer du bist.\"</p>"},{"location":"KW4/knn/#2-funktionsweise","title":"2. Funktionsweise","text":""},{"location":"KW4/knn/#21-fur-klassifikation","title":"2.1 F\u00fcr Klassifikation:","text":"<ul> <li>Bestimme die k n\u00e4chsten Nachbarn eines neuen Datenpunkts</li> <li>F\u00fchre eine Mehrheitsabstimmung durch</li> <li>Weise dem neuen Punkt die h\u00e4ufigste Klasse unter den Nachbarn zu</li> </ul>"},{"location":"KW4/knn/#22-fur-regression","title":"2.2 F\u00fcr Regression:","text":"<ul> <li>Bestimme die k n\u00e4chsten Nachbarn eines neuen Datenpunkts</li> <li>Berechne den Durchschnitt (oder gewichteten Durchschnitt) der Zielwerte</li> <li>Weise dem neuen Punkt diesen Durchschnittswert zu</li> </ul>"},{"location":"KW4/knn/#3-der-algorithmus-im-detail","title":"3. Der Algorithmus im Detail","text":""},{"location":"KW4/knn/#31-pseudocode-fur-knn-klassifikation","title":"3.1 Pseudocode f\u00fcr KNN-Klassifikation:","text":"<pre><code>INPUT:\n    D: Trainingsdata (x_i, y_i)\n    k: Anzahl der Nachbarn\n    x_new: Neuer Datenpunkt f\u00fcr Klassifikation\n\nALGORITHMUS:\n1. F\u00dcR ALLE Punkte x_i in D:\n    - Berechne Distanz d(x_new, x_i)\n    - Speichere (d(x_new, x_i), y_i) in Liste distances\n\n2. Sortiere distances nach aufsteigender Distanz\n\n3. W\u00e4hle die ersten k Eintr\u00e4ge aus distances\n   Speichere deren Labels in neighbors\n\n4. Z\u00e4hle H\u00e4ufigkeit jeder Klasse in neighbors\n   y_pred = Klasse mit h\u00f6chster H\u00e4ufigkeit\n\nRETURN: y_pred\n</code></pre>"},{"location":"KW4/knn/#32-distanzmetriken","title":"3.2 Distanzmetriken","text":"<p>Die h\u00e4ufigsten Distanzmetriken sind:</p> <ol> <li> <p>Euklidische Distanz:    d(x,y) = \u221a(\u03a3(x\u1d62 - y\u1d62)\u00b2)</p> </li> <li> <p>Manhattan-Distanz:    d(x,y) = \u03a3|x\u1d62 - y\u1d62|</p> </li> <li> <p>Minkowski-Distanz:    d(x,y) = (\u03a3|x\u1d62 - y\u1d62|\u1d56)^(1/p)</p> </li> </ol>"},{"location":"KW4/knn/#4-wichtige-uberlegungen-und-parameter","title":"4. Wichtige \u00dcberlegungen und Parameter","text":""},{"location":"KW4/knn/#41-wahl-von-k","title":"4.1 Wahl von k","text":"<ul> <li>Kleines k: Hohe Varianz, niedriger Bias</li> <li>Gro\u00dfes k: Niedrige Varianz, hoher Bias</li> <li>k sollte ungerade sein (bei Bin\u00e4rklassifikation)</li> <li>Typische Werte: 3, 5, 7, 11</li> </ul>"},{"location":"KW4/knn/#42-distanzgewichtung","title":"4.2 Distanzgewichtung","text":"<p>Die Stimmen der Nachbarn k\u00f6nnen gewichtet werden: - Uniform: Alle Nachbarn haben gleiches Gewicht - Nach Distanz: w = 1/d oder w = 1/d\u00b2</p>"},{"location":"KW4/knn/#43-skalierung","title":"4.3 Skalierung","text":"<ul> <li>Feature-Skalierung ist KRITISCH</li> <li>Standardisierung (z-score) oder </li> <li>Min-Max-Skalierung sind \u00fcblich</li> </ul>"},{"location":"KW4/knn/#5-implementierungsbeispiel-in-python","title":"5. Implementierungsbeispiel in Python","text":"<pre><code>import numpy as np\nfrom collections import Counter\n\nclass SimpleKNN:\n    def __init__(self, k=3):\n        self.k = k\n\n    def fit(self, X, y):\n        self.X_train = X\n        self.y_train = y\n\n    def euclidean_distance(self, x1, x2):\n        return np.sqrt(np.sum((x1 - x2) ** 2))\n\n    def predict(self, X):\n        predictions = []\n\n        for x in X:\n            # Distanzen berechnen\n            distances = []\n            for idx, x_train in enumerate(self.X_train):\n                dist = self.euclidean_distance(x, x_train)\n                distances.append((dist, self.y_train[idx]))\n\n            # Sortieren nach Distanz\n            distances.sort(key=lambda x: x[0])\n\n            # k n\u00e4chste Nachbarn finden\n            k_nearest = distances[:self.k]\n\n            # Labels der k n\u00e4chsten Nachbarn\n            k_nearest_labels = [label for _, label in k_nearest]\n\n            # Mehrheitsentscheidung\n            most_common = Counter(k_nearest_labels).most_common(1)\n            predictions.append(most_common[0][0])\n\n        return np.array(predictions)\n</code></pre>"},{"location":"KW4/knn/#6-vor-und-nachteile","title":"6. Vor- und Nachteile","text":""},{"location":"KW4/knn/#61-vorteile","title":"6.1 Vorteile","text":"<ul> <li>Einfach zu verstehen und zu implementieren</li> <li>Keine Trainingsphase notwendig</li> <li>Nicht-parametrisch (keine Annahmen \u00fcber Datenverteilung)</li> <li>Gut f\u00fcr multimodale Klassen</li> <li>Nat\u00fcrliche Behandlung von Multi-Class-Problemen</li> </ul>"},{"location":"KW4/knn/#62-nachteile","title":"6.2 Nachteile","text":"<ul> <li>Berechnungsaufwand bei der Vorhersage</li> <li>Speicherintensiv (alle Trainingsdaten m\u00fcssen gespeichert werden)</li> <li>Sensibel gegen\u00fcber irrelevanten Features</li> <li>\"Fluch der Dimensionalit\u00e4t\"</li> <li>Skalierung der Features ist kritisch</li> </ul>"},{"location":"KW4/metrics/","title":"Evaluierungsmetriken","text":""},{"location":"KW4/metrics/#uberblick-der-evaluierungsmetriken","title":"\u00dcberblick der Evaluierungsmetriken","text":"Metrik Formel Anwendungsbereich Wertebereich Prim\u00e4rer Einsatzzweck Accuracy (TP + TN) / (TP + TN + FP + FN) Klassifikation [0,1] Allgemeine Klassifikationsg\u00fcte Precision TP / (TP + FP) Klassifikation [0,1] FP-Minimierung Recall TP / (TP + FN) Klassifikation [0,1] FN-Minimierung F1-Score 2 * (P * R) / (P + R) Klassifikation [0,1] Kombinierte Bewertung ROC-AUC Integral ROC-Kurve Bin\u00e4re Klassifikation [0,1] Schwellwert-unabh\u00e4ngig MSE \u03a3(y_true - y_pred)\u00b2 / n Regression [0,\u221e) Quadratische Fehlerbewertung RMSE \u221a(MSE) Regression [0,\u221e) Skalenkonforme Bewertung MAE \u03a3 y_true - y_pred / n Regression R\u00b2 1 - (MSE / Var(y)) Regression (-\u221e,1] Varianzbasierte Bewertung"},{"location":"KW4/metrics/#klassifikationsmetriken","title":"Klassifikationsmetriken","text":""},{"location":"KW4/metrics/#accuracy","title":"Accuracy","text":"<p>Die Accuracy ist die am einfachsten zu verstehende Metrik, da sie direkt den Anteil der korrekten Vorhersagen misst. Sie berechnet sich aus der Anzahl der richtigen Vorhersagen geteilt durch die Gesamtzahl aller Vorhersagen. W\u00e4hrend diese Metrik bei ausgeglichenen Datens\u00e4tzen sehr aussagekr\u00e4ftig ist, verliert sie bei unbalancierten Daten stark an Bedeutung. </p> <p>In einem Datensatz zur Krankheitserkennung mit 98% gesunden und 2% kranken Patienten w\u00fcrde ein Modell, das einfach immer \"gesund\" vorhersagt, eine Accuracy von 0.98 erreichen - trotz seiner offensichtlichen Nutzlosigkeit. Dies f\u00fchrt uns zu differenzierteren Metriken.</p> <pre><code>from sklearn.metrics import accuracy_score\nimport numpy as np\n\n# Simulieren eines Klassifikationsproblems: Spam-Erkennung\n# 0 = normale E-Mail, 1 = Spam\ny_true = np.array([0, 0, 1, 1, 0, 0, 1, 0, 0, 1])  # Wahre Labels\ny_pred = np.array([0, 0, 1, 0, 0, 0, 1, 1, 0, 1])  # Vorhersagen des Modells\n\naccuracy = accuracy_score(y_true, y_pred)\nprint(f\"Accuracy: {accuracy:.2f}\")  # 0.80 = 80% korrekte Vorhersagen\n\n# Zeigt das Problem bei unbalancierten Daten\ny_true_unbalanced = np.array([0]*90 + [1]*10)  # 90% normale Mails, 10% Spam\ny_pred_naive = np.array([0]*100)  # Modell sagt immer \"normale Mail\"\n\naccuracy_unbalanced = accuracy_score(y_true_unbalanced, y_pred_naive)\nprint(f\"Accuracy (unbalanciert): {accuracy_unbalanced:.2f}\")  # 0.90, aber nutzlos!\n</code></pre>"},{"location":"KW4/metrics/#precision","title":"Precision","text":"<p>Precision fokussiert sich auf die Qualit\u00e4t der positiven Vorhersagen. Sie beantwortet die Frage: \"Wenn unser Modell etwas als positiv klassifiziert, wie oft stimmt das?\" Diese Metrik ist besonders wichtig, wenn falsche positive Vorhersagen kostspielig oder problematisch sind.</p> <p>Ein Beispiel: Bei einem Spam-Filter ist eine hohe Precision wichtig, da f\u00e4lschlicherweise als Spam klassifizierte wichtige E-Mails (falsch positive) sehr st\u00f6rend f\u00fcr den Nutzer sind. Die Precision gibt uns hier den Anteil der tats\u00e4chlichen Spam-Mails unter allen als Spam klassifizierten E-Mails.</p> <pre><code>from sklearn.metrics import precision_score, recall_score, precision_recall_curve\nimport matplotlib.pyplot as plt\n\n# Medizinische Diagnose: Krankheitserkennung\n# Simuliere Testdaten mit Wahrscheinlichkeiten\nnp.random.seed(42)\nn_samples = 1000\n# Generiere Wahrscheinlichkeiten f\u00fcr positive Klasse\ny_scores = np.random.rand(n_samples)\n# Wahre Labels (5% Krankheitsf\u00e4lle)\ny_true = np.random.choice([0, 1], size=n_samples, p=[0.95, 0.05])\n\n# Verschiedene Schwellenwerte testen\nthresholds = [0.3, 0.5, 0.7]\nfor threshold in thresholds:\n    # Konvertiere Wahrscheinlichkeiten zu bin\u00e4ren Vorhersagen\n    y_pred = (y_scores &gt;= threshold).astype(int)\n\n    precision = precision_score(y_true, y_pred)\n    recall = recall_score(y_true, y_pred)\n\n    print(f\"\\nSchwellenwert: {threshold}\")\n    print(f\"Precision: {precision:.2f}\")\n    print(f\"Recall: {recall:.2f}\")\n\n# Precision-Recall Kurve\nprecisions, recalls, _ = precision_recall_curve(y_true, y_scores)\n\nplt.figure(figsize=(10, 6))\nplt.plot(recalls, precisions)\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall Kurve')\nplt.grid(True)\n</code></pre>"},{"location":"KW4/metrics/#recall","title":"Recall","text":"<p>Der Recall hingegen konzentriert sich auf die Vollst\u00e4ndigkeit der positiven Klassifikationen. Er beantwortet die Frage: \"Von allen tats\u00e4chlich positiven F\u00e4llen, wie viele haben wir gefunden?\" Diese Metrik ist entscheidend, wenn das \u00dcbersehen positiver F\u00e4lle schwerwiegende Folgen hat.</p> <p>In der medizinischen Diagnostik ist ein hoher Recall oft wichtiger als eine hohe Precision. Wenn es um die Erkennung einer schweren Krankheit geht, ist es besser, einige falsche Alarme zu haben (niedrige Precision), als kranke Patienten zu \u00fcbersehen (niedriger Recall).</p>"},{"location":"KW4/metrics/#f1-score","title":"F1-Score","text":"<p>Der F1-Score vereint Precision und Recall in einer einzigen Metrik durch Bildung des harmonischen Mittels. Dies ist besonders n\u00fctzlich, wenn wir einen ausgewogenen Kompromiss zwischen Precision und Recall ben\u00f6tigen. Das harmonische Mittel reagiert besonders empfindlich auf niedrige Werte, wodurch ein Modell nur dann einen hohen F1-Score erreichen kann, wenn sowohl Precision als auch Recall gut sind.</p> <p>Der F1-Score eignet sich besonders f\u00fcr Anwendungsf\u00e4lle, in denen sowohl falsch positive als auch falsch negative Vorhersagen wichtig sind, wie etwa bei der Qualit\u00e4tskontrolle in der Produktion.</p> <pre><code>from sklearn.metrics import f1_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\n# Qualit\u00e4tskontrolle in der Produktion\n# Simuliere Produktionsdaten mit Features und Defekt-Labels\nnp.random.seed(42)\nn_samples = 1000\nn_features = 5\n\n# Generiere Features (z.B. Temperatur, Druck, Geschwindigkeit etc.)\nX = np.random.randn(n_samples, n_features)\n# Generiere Labels (10% defekte Produkte)\ny = np.random.choice([0, 1], size=n_samples, p=[0.9, 0.1])\n\n# Daten aufteilen und Modell trainieren\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n# Feature-Skalierung\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Modell trainieren\nmodel = LogisticRegression()\nmodel.fit(X_train_scaled, y_train)\n\n# Vorhersagen\ny_pred = model.predict(X_test_scaled)\n\n# Metriken berechnen\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nprint(\"\\nQualit\u00e4tskontrolle Metriken:\")\nprint(f\"Precision: {precision:.2f}\")\nprint(f\"Recall: {recall:.2f}\")\nprint(f\"F1-Score: {f1:.2f}\")\n</code></pre>"},{"location":"KW4/metrics/#roc-auc-mit-vollstandigem-beispiel","title":"ROC-AUC mit vollst\u00e4ndigem Beispiel","text":"<pre><code>from sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\n\n# Kreditkarten-Betrug Erkennung\n# Simuliere Transaktionsdaten\nnp.random.seed(42)\nn_samples = 10000\n\n# Generiere Wahrscheinlichkeiten f\u00fcr betr\u00fcgerische Transaktionen\nfraud_probs = np.random.beta(2, 10, n_samples)  # Beta-Verteilung f\u00fcr realistische Wahrscheinlichkeiten\n# Echte Labels (1% Betrug)\nis_fraud = np.random.choice([0, 1], size=n_samples, p=[0.99, 0.01])\n\n# ROC-Kurve berechnen\nfpr, tpr, thresholds = roc_curve(is_fraud, fraud_probs)\nroc_auc = auc(fpr, tpr)\n\n# Visualisierung\nplt.figure(figsize=(10, 6))\nplt.plot(fpr, tpr, color='darkorange', lw=2, \n         label=f'ROC curve (AUC = {roc_auc:.2f})')\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve - Betrugserkennungsmodell')\nplt.legend(loc=\"lower right\")\nplt.grid(True)\n</code></pre>"},{"location":"KW4/metrics/#roc-auc","title":"ROC-AUC","text":"<p>Die ROC-AUC (Area Under the Receiver Operating Characteristic Curve) ist eine besonders aussagekr\u00e4ftige Metrik, da sie die Leistung des Modells \u00fcber verschiedene Schwellenwerte hinweg bewertet. Sie zeigt, wie gut das Modell positive von negativen F\u00e4llen trennen kann, unabh\u00e4ngig von einem spezifischen Schwellenwert.</p> <p>Diese Metrik ist besonders wertvoll beim Vergleich verschiedener Modelle, da sie von der konkreten Wahl des Schwellenwerts unabh\u00e4ngig ist. Ein perfektes Modell h\u00e4tte eine AUC von 1.0, w\u00e4hrend ein zuf\u00e4lliges Modell eine AUC von 0.5 erreicht.</p>"},{"location":"KW4/metrics/#regressionsmetriken","title":"Regressionsmetriken","text":""},{"location":"KW4/metrics/#mean-squared-error-mse","title":"Mean Squared Error (MSE)","text":"<p>Der MSE ist die grundlegendste Regressionsmetrik. Er bestraft gro\u00dfe Fehler \u00fcberproportional stark durch die Quadrierung der Differenzen. Dies macht ihn besonders empfindlich gegen\u00fcber Ausrei\u00dfern. </p> <p>Diese Eigenschaft ist in manchen F\u00e4llen erw\u00fcnscht - etwa wenn einzelne gro\u00dfe Abweichungen besonders problematisch sind, wie bei der Steuerung von Industrieanlagen. In anderen F\u00e4llen kann diese \u00dcbergewichtung gro\u00dfer Fehler aber auch zu verzerrten Optimierungen f\u00fchren.</p>"},{"location":"KW4/metrics/#root-mean-squared-error-rmse","title":"Root Mean Squared Error (RMSE)","text":"<p>Der RMSE ist die Quadratwurzel des MSE und hat den gro\u00dfen Vorteil, dass er in der gleichen Einheit wie die Zieldaten gemessen wird. Dies macht ihn deutlich intuitiver interpretierbar als den MSE. </p> <p>Bei einer Verkaufspreisprognose in Euro gibt der RMSE direkt die durchschnittliche Abweichung in Euro an, w\u00e4hrend der MSE in quadrierten Euro gemessen w\u00fcrde. Dennoch beh\u00e4lt der RMSE die charakteristische \u00fcberproportionale Bestrafung gro\u00dfer Fehler bei.</p> <pre><code>from sklearn.metrics import mean_squared_error\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Hauspreisvorhersage\n# Simuliere Immobiliendaten\nnp.random.seed(42)\nn_samples = 1000\n\n# Features: Gr\u00f6\u00dfe (m\u00b2), Alter, Anzahl Zimmer, Entfernung zum Zentrum\nX = np.random.rand(n_samples, 4)\nX[:, 0] = X[:, 0] * 150 + 50  # Gr\u00f6\u00dfe: 50-200m\u00b2\nX[:, 1] = X[:, 1] * 50        # Alter: 0-50 Jahre\nX[:, 2] = np.round(X[:, 2] * 4 + 2)  # Zimmer: 2-6\nX[:, 3] = X[:, 3] * 30        # Entfernung: 0-30km\n\n# Preise generieren (in Tausend Euro)\nbase_price = 200 + X[:, 0] * 2 - X[:, 1] * 1.5 + X[:, 2] * 20 - X[:, 3] * 2\nnoise = np.random.normal(0, 20, n_samples)\ny = base_price + noise\n\n# Modell trainieren\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\nmodel = RandomForestRegressor(n_estimators=100)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\n# Metriken berechnen\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\n\nprint(\"\\nHauspreisvorhersage Metriken:\")\nprint(f\"MSE: {mse:.2f} (Tausend Euro\u00b2)\")\nprint(f\"RMSE: {rmse:.2f} (Tausend Euro)\")\n</code></pre>"},{"location":"KW4/metrics/#mean-absolute-error-mae","title":"Mean Absolute Error (MAE)","text":"<p>Der MAE unterscheidet sich fundamental von MSE und RMSE, da er Fehler linear und nicht quadratisch bestraft. Er gibt den durchschnittlichen absoluten Fehler an und ist damit robuster gegen\u00fcber Ausrei\u00dfern.</p> <p>Diese Metrik eignet sich besonders, wenn einzelne gro\u00dfe Abweichungen nicht \u00fcberproportional ins Gewicht fallen sollen. Ein typisches Beispiel sind Nachfrageprognosen im Einzelhandel, wo einzelne gr\u00f6\u00dfere Abweichungen nicht dramatisch sind, solange der durchschnittliche Fehler gering bleibt.</p> <pre><code>from sklearn.metrics import mean_absolute_error\n\n# Energieverbrauchsvorhersage\n# Simuliere st\u00fcndlichen Energieverbrauch\nhours = np.arange(24)\nbase_consumption = 100 + np.sin(hours * np.pi/12) * 50  # Tagesrhythmus\nnoise = np.random.normal(0, 10, 24)\nactual_consumption = base_consumption + noise\n\n# Modellvorhersagen simulieren (mit systematischer Abweichung)\npredicted_consumption = base_consumption + np.random.normal(5, 15, 24)\n\nmae = mean_absolute_error(actual_consumption, predicted_consumption)\nmse = mean_squared_error(actual_consumption, predicted_consumption)\nrmse = np.sqrt(mse)\n\nprint(\"\\nEnergieverbrauch Vorhersage Metriken:\")\nprint(f\"MAE: {mae:.2f} kWh\")\nprint(f\"RMSE: {rmse:.2f} kWh\")\n\n# Visualisierung\nplt.figure(figsize=(12, 6))\nplt.plot(hours, actual_consumption, label='Tats\u00e4chlicher Verbrauch')\nplt.plot(hours, predicted_consumption, label='Vorhergesagter Verbrauch')\nplt.xlabel('Stunde des Tages')\nplt.ylabel('Energieverbrauch (kWh)')\nplt.title('Tats\u00e4chlicher vs. Vorhergesagter Energieverbrauch')\nplt.legend()\nplt.grid(True)\n</code></pre>"},{"location":"KW4/metrics/#r2-mit-ausfuhrlichem-beispiel","title":"R\u00b2 mit ausf\u00fchrlichem Beispiel","text":"<pre><code>from sklearn.metrics import r2_score\nfrom sklearn.linear_model import LinearRegression\n\n# Verkaufsprognose\n# Simuliere monatliche Verkaufsdaten mit saisonalem Effekt\nmonths = 24\nseasonal_pattern = np.sin(np.arange(months) * 2 * np.pi / 12)\n\n# Features: Werbeausgaben, Saisonalit\u00e4t, Preis\nX = np.random.rand(months, 3)\nX[:, 1] = seasonal_pattern  # Saisonaler Effekt\nX[:, 2] = np.random.rand(months) * 0.2 + 0.9  # Preisfaktor\n\n# Verkaufszahlen generieren\nsales = 1000 + X[:, 0] * 500 + X[:, 1] * 200 - X[:, 2] * 300\nnoise = np.random.normal(0, 50, months)\ny = sales + noise\n\n# Modell trainieren\nmodel = LinearRegression()\nmodel.fit(X, y)\ny_pred = model.predict(X)\n\n# R\u00b2 berechnen\nr2 = r2_score(y, y_pred)\n\nprint(\"\\nVerkaufsprognose Metrik:\")\nprint(f\"R\u00b2 Score: {r2:.3f}\")\n\n# Visualisierung der Anpassungsg\u00fcte\nplt.figure(figsize=(10, 6))\nplt.scatter(y, y_pred, alpha=0.5)\nplt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--', lw=2)\nplt.xlabel('Tats\u00e4chliche Verk\u00e4ufe')\nplt.ylabel('Vorhergesagte Verk\u00e4ufe')\nplt.title('Tats\u00e4chliche vs. Vorhergesagte Verk\u00e4ufe')\nplt.grid(True)\n</code></pre>"},{"location":"KW4/metrics/#r2-bestimmtheitsma","title":"R\u00b2 (Bestimmtheitsma\u00df)","text":"<p>R\u00b2 ist eine besonders intuitive Metrik, da sie angibt, welchen Anteil der Varianz in den Daten das Modell erkl\u00e4ren kann. Ein R\u00b2 von 0.7 bedeutet, dass 70% der Varianz in den Daten durch das Modell erkl\u00e4rt werden.</p> <p>Diese Metrik ist besonders n\u00fctzlich f\u00fcr die Kommunikation mit nicht-technischen Stakeholdern, da sie leicht verst\u00e4ndlich ist. Allerdings sollte beachtet werden, dass R\u00b2 bei nicht-linearen Zusammenh\u00e4ngen oder bei Modellen mit vielen Features irref\u00fchrend sein kann.</p>"},{"location":"KW4/metrics/#vergleichende-entscheidungshilfe","title":"Vergleichende Entscheidungshilfe","text":"<p>Die Wahl der richtigen Metrik h\u00e4ngt von mehreren Faktoren ab:</p> <ol> <li>Problemtyp</li> <li>Bei unbalancierten Klassifikationsproblemen: Precision, Recall, F1-Score</li> <li>Bei balancierten Klassifikationsproblemen: Accuracy oder ROC-AUC</li> <li>Bei Regressionen mit Ausrei\u00dfern: MAE</li> <li> <p>Bei Regressionen mit kritischen gro\u00dfen Fehlern: RMSE</p> </li> <li> <p>Anwendungskontext</p> </li> <li>Kritische Sicherheitsanwendungen: Fokus auf Recall</li> <li>Qualit\u00e4tssicherung: Ausgewogener F1-Score</li> <li> <p>Kostenoptimierung: MSE bei quadratischem Kostenwachstum</p> </li> <li> <p>Kommunikationsbedarf</p> </li> <li>Technisches Publikum: Detaillierte Metriken wie ROC-AUC</li> <li>Nicht-technisches Publikum: Intuitive Metriken wie Accuracy oder R\u00b2</li> <li>Management: Gesch\u00e4ftsrelevante Metriken wie kostengewichtete Fehlerraten</li> </ol>"},{"location":"KW4/pca/","title":"Principal Component Analysis (PCA)","text":""},{"location":"KW4/pca/#die-grundidee-verstehen","title":"Die Grundidee verstehen","text":"<p>Die Principal Component Analysis (PCA) ist eine der wichtigsten und am h\u00e4ufigsten verwendeten Techniken in der Datenanalyse und im maschinellen Lernen. Im Kern geht es darum, die \"wichtigsten\" Richtungen in den Daten zu finden - also jene Richtungen, entlang derer die Daten die gr\u00f6\u00dfte Variation zeigen.</p> <p>Stellen Sie sich vor, Sie fotografieren einen Bleistift aus verschiedenen Winkeln. Von der Seite erscheint er lang und schmal, von vorne sehen Sie nur den runden Querschnitt. Die Seitenansicht enth\u00e4lt die meiste Information \u00fcber die Form des Bleistifts - sie ist gewisserma\u00dfen die \"erste Hauptkomponente\". Die PCA macht genau das: Sie findet die aussagekr\u00e4ftigsten \"Ansichten\" Ihrer Daten.</p>"},{"location":"KW4/pca/#der-mathematische-kern","title":"Der mathematische Kern","text":"<p>Die mathematische Magie der PCA basiert auf der Eigenwertzerlegung der Kovarianzmatrix. Das klingt zun\u00e4chst komplex, l\u00e4sst sich aber Schritt f\u00fcr Schritt verstehen:</p> <ol> <li> <p>Die Kovarianzmatrix ist wie ein Fingerabdruck unserer Daten. Sie zeigt uns, wie stark die verschiedenen Merkmale miteinander zusammenh\u00e4ngen. Wenn zwei Merkmale stark korreliert sind, haben wir gewisserma\u00dfen redundante Information.</p> </li> <li> <p>Eigenvektoren dieser Matrix sind besondere Richtungen im Datenraum. Wenn wir unsere Daten entlang eines Eigenvektors \"anschauen\", sehen wir sie aus einer besonders aufschlussreichen Perspektive. Man kann sich Eigenvektoren wie die \"nat\u00fcrlichen Achsen\" unserer Daten vorstellen.</p> </li> <li> <p>Eigenwerte sagen uns, wie \"wichtig\" jeder Eigenvektor ist. Ein gro\u00dfer Eigenwert bedeutet, dass die Daten in Richtung des zugeh\u00f6rigen Eigenvektors stark variieren - dort passiert also \"viel Interessantes\".</p> </li> </ol>"},{"location":"KW4/pca/#der-pca-prozess-im-detail","title":"Der PCA-Prozess im Detail","text":"<p>Lassen Sie uns den gesamten Prozess der PCA Schritt f\u00fcr Schritt durchgehen:</p>"},{"location":"KW4/pca/#1-datenvorbereitung","title":"1. Datenvorbereitung","text":"<p>Zun\u00e4chst m\u00fcssen wir unsere Daten zentrieren - das bedeutet, wir ziehen von jedem Feature seinen Mittelwert ab. Dies ist wichtig, weil wir uns f\u00fcr die Variation um den Mittelwert interessieren. H\u00e4ufig werden die Daten auch skaliert, damit alle Features gleich gewichtet werden.</p> <pre><code># Daten zentrieren und skalieren\nX_centered = X - np.mean(X, axis=0)\nX_standardized = X_centered / np.std(X_centered, axis=0)\n</code></pre>"},{"location":"KW4/pca/#2-kovarianzmatrix-berechnen","title":"2. Kovarianzmatrix berechnen","text":"<p>Die Kovarianzmatrix ist der Schl\u00fcssel zur PCA. Sie zeigt uns die Beziehungen zwischen allen Paaren von Features:</p> <pre><code># Kovarianzmatrix berechnen\ncovariance_matrix = np.dot(X_standardized.T, X_standardized) / (n_samples - 1)\n</code></pre>"},{"location":"KW4/pca/#3-eigenwerte-und-eigenvektoren","title":"3. Eigenwerte und Eigenvektoren","text":"<p>Jetzt kommt der mathematische Kernschritt: Wir berechnen die Eigenwerte und Eigenvektoren der Kovarianzmatrix. Die Eigenvektoren mit den gr\u00f6\u00dften Eigenwerten sind unsere Hauptkomponenten.</p> <pre><code>eigenvalues, eigenvectors = np.linalg.eigh(covariance_matrix)\n# Sortieren nach Gr\u00f6\u00dfe der Eigenwerte\nidx = eigenvalues.argsort()[::-1]\neigenvalues = eigenvalues[idx]\neigenvectors = eigenvectors[idx, :]\n</code></pre>"},{"location":"KW4/pca/#die-bedeutung-fur-die-datenanalyse","title":"Die Bedeutung f\u00fcr die Datenanalyse","text":"<p>Die PCA ist aus mehreren Gr\u00fcnden so wertvoll:</p>"},{"location":"KW4/pca/#dimensionsreduktion","title":"Dimensionsreduktion","text":"<p>In realen Datens\u00e4tzen haben wir oft Hunderte oder Tausende von Features. Viele davon sind redundant oder unwichtig. Die PCA hilft uns, die wenigen wichtigen Dimensionen zu finden, die den Gro\u00dfteil der Information enthalten.</p> <p>Ein Beispiel: Bei Bilderkennung k\u00f6nnte ein 1000x1000 Pixel Bild theoretisch 1 Million Dimensionen haben. Aber die meisten realen Bilder lassen sich mit viel weniger Dimensionen gut beschreiben, weil benachbarte Pixel oft \u00e4hnliche Werte haben.</p>"},{"location":"KW4/pca/#rauschunterdruckung","title":"Rauschunterdr\u00fcckung","text":"<p>Indem wir uns auf die Hauptkomponenten mit den gr\u00f6\u00dften Eigenwerten konzentrieren, filtern wir automatisch Rauschen heraus. Dies liegt daran, dass Rauschen typischerweise in allen Richtungen gleich stark ist, w\u00e4hrend die \"echten\" Muster in den Daten bestimmte Vorzugsrichtungen haben.</p>"},{"location":"KW4/pca/#visualisierung","title":"Visualisierung","text":"<p>Eine der h\u00e4ufigsten Anwendungen der PCA ist die Visualisierung hochdimensionaler Daten. Indem wir auf die zwei oder drei wichtigsten Hauptkomponenten projizieren, k\u00f6nnen wir die wesentlichen Strukturen in den Daten sichtbar machen.</p>"},{"location":"KW4/pca/#die-wahl-der-komponentenzahl","title":"Die Wahl der Komponentenzahl","text":"<p>Eine zentrale Frage bei der PCA ist: Wie viele Hauptkomponenten sollen wir behalten? Hier gibt es verschiedene Ans\u00e4tze:</p> <ol> <li>Erkl\u00e4rte Varianz: Wir w\u00e4hlen so viele Komponenten, dass ein bestimmter Anteil (z.B. 95%) der Gesamtvarianz erhalten bleibt:</li> </ol> <pre><code>explained_variance_ratio = eigenvalues / np.sum(eigenvalues)\ncumulative_variance_ratio = np.cumsum(explained_variance_ratio)\n\n# Finde Anzahl der Komponenten f\u00fcr 95% erkl\u00e4rte Varianz\nn_components = np.argmax(cumulative_variance_ratio &gt;= 0.95) + 1\n</code></pre> <ol> <li> <p>Elbow-Methode: Wir plotten die erkl\u00e4rte Varianz gegen die Anzahl der Komponenten und suchen den \"Ellbogen\" in der Kurve.</p> </li> <li> <p>Kaiser-Kriterium: Wir behalten nur Komponenten mit Eigenwerten gr\u00f6\u00dfer als 1 (bei standardisierten Daten).</p> </li> </ol>"},{"location":"KW4/pca/#praktische-uberlegungen","title":"Praktische \u00dcberlegungen","text":"<p>Bei der Anwendung der PCA gibt es einige wichtige Punkte zu beachten:</p>"},{"location":"KW4/pca/#skalierung","title":"Skalierung","text":"<p>Die PCA ist nicht skaleninvariant. Wenn Features in sehr unterschiedlichen Gr\u00f6\u00dfenordnungen gemessen werden, sollten die Daten vorher standardisiert werden. Sonst dominieren die Features mit gro\u00dfer numerischer Spannweite das Ergebnis.</p>"},{"location":"KW4/pca/#interpretierbarkeit","title":"Interpretierbarkeit","text":"<p>Ein Nachteil der PCA ist, dass die Hauptkomponenten oft schwer zu interpretieren sind. Sie sind Linearkombinationen der urspr\u00fcnglichen Features, und diese Kombinationen haben nicht immer eine klare semantische Bedeutung.</p>"},{"location":"KW4/pca/#nichtlinearitat","title":"Nichtlinearit\u00e4t","text":"<p>Die PCA ist eine lineare Methode. Wenn die Datenstruktur nichtlinear ist, k\u00f6nnen andere Techniken wie t-SNE oder UMAP besser geeignet sein.</p>"},{"location":"KW4/pca_smote_cheat_sheet/","title":"Cheat-Sheet","text":""},{"location":"KW4/pca_smote_cheat_sheet/#schritte-zum-anwenden-von-smote","title":"Schritte zum Anwenden von SMOTE","text":"<ol> <li>Klassenverteilungen ansehen.</li> <li>Minderheitsklassen (weniger als 1 % der meistvertretenden Klasse) entfernen oder gesondert behandeln. Erstellen von Duplikaten wollen wir meistens vermeiden.</li> <li>In numerische Werte \u00fcberf\u00fchren. F\u00fcr SMOTE selbt ist es egal, wie wir das machen. F\u00fcr die nachfolgednen Werte m\u00f6glicherweise nicht. Es k\u00f6nnen im Standard \u00fcberall auch fraktionale Werte entstehen.</li> <li>Teilen in Trainings und Testdaten. <code>stratify=df[\"target\"]</code> kann bei kleinen Datasets und wenn eine Klasse selten vorhandne ist, sinnvoll sein.</li> <li>smote initialisieren und anwenden, aber nur auf die Trainingsdaten</li> </ol>"},{"location":"KW4/pca_smote_cheat_sheet/#schritte-zum-anwenden-vom-pca","title":"Schritte zum Anwenden vom PCA","text":"<ol> <li>Skalieren/ Normalisieren</li> <li>PCA initialisieren und erkl\u00e4rende Varianz betrachten </li> <li>Anzahl der Komponenten anhand der kummilierten Varianz oder Ellenbogen-Kriterium bestimmen</li> <li>Transformation der Trainingsdaten anhand der PCA durchf\u00fchren</li> <li>Gleiches k\u00f6nnen wir danach \u00fcber <code>model.transform()</code> auch f\u00fcr die Testdaten machen</li> </ol>"},{"location":"KW4/scaling/","title":"Feature Scaling: Methoden und ihre Bedeutung","text":""},{"location":"KW4/scaling/#1-warum-ist-skalierung-wichtig","title":"1. Warum ist Skalierung wichtig?","text":""},{"location":"KW4/scaling/#11-grundlegende-probleme-ohne-skalierung","title":"1.1 Grundlegende Probleme ohne Skalierung","text":"<ul> <li>Features in unterschiedlichen Gr\u00f6\u00dfenordnungen (z.B. Alter [0-100] vs. Einkommen [0-1.000.000])</li> <li>Algorithmen, die auf Distanzen basieren (KNN, k-Means, SVM), werden von gro\u00dfen Werten dominiert</li> <li>Gradient Descent konvergiert langsamer</li> <li>L1/L2 Regularisierung funktioniert nicht optimal</li> </ul>"},{"location":"KW4/scaling/#12-beispiel-ohne-skalierung","title":"1.2 Beispiel ohne Skalierung:","text":"<pre><code>import numpy as np\nfrom sklearn.datasets import make_blobs\n\n# Beispieldaten\nX, y = make_blobs(n_samples=1000, centers=2, random_state=42)\n# Zweite Feature k\u00fcnstlich vergr\u00f6\u00dfern\nX[:, 1] = X[:, 1] * 1000\n\n# Ohne Skalierung: Distanzberechnung wird von Feature 2 dominiert\npoint1 = X[0]\npoint2 = X[1]\ndistance = np.sqrt(np.sum((point1 - point2) ** 2))\nprint(f\"Dominierte Distanz: {distance}\")  # Wird von gro\u00dfen Werten dominiert\n</code></pre>"},{"location":"KW4/scaling/#2-wichtigste-skalierungsmethoden","title":"2. Wichtigste Skalierungsmethoden","text":""},{"location":"KW4/scaling/#21-standardscaler-standardisierung","title":"2.1 StandardScaler (Standardisierung)","text":"<p>Transformiert Features auf Mittelwert=0 und Standardabweichung=1.</p> <pre><code>from sklearn.preprocessing import StandardScaler\nimport pandas as pd\n\n# Beispieldaten\ndata = pd.DataFrame({\n    'Alter': [25, 35, 45, 55],\n    'Einkommen': [30000, 45000, 70000, 90000]\n})\n\nscaler = StandardScaler()\nscaled_data = scaler.fit_transform(data)\n\nprint(\"Original:\\n\", data)\nprint(\"\\nSkaliert:\\n\", pd.DataFrame(scaled_data, columns=data.columns))\n</code></pre> <p>Formel: z = (x - \u03bc) / \u03c3 - Vorteile:   - Robust gegen Ausrei\u00dfer   - Erh\u00e4lt die Form der Verteilung   - Ideal f\u00fcr Normalverteilte Daten - Nachteile:   - Keine garantierten Min/Max Grenzen   - Schwerer interpretierbar</p>"},{"location":"KW4/scaling/#22-minmaxscaler","title":"2.2 MinMaxScaler","text":"<p>Skaliert Features in einen festen Bereich [0,1] oder beliebigen anderen.</p> <pre><code>from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\nscaled_data = scaler.fit_transform(data)\n\nprint(\"MinMax Skaliert:\\n\", pd.DataFrame(scaled_data, columns=data.columns))\n</code></pre> <p>Formel: x_scaled = (x - x_min) / (x_max - x_min) - Vorteile:   - Fester Wertebereich   - Erh\u00e4lt Null-Eintr\u00e4ge   - Gut interpretierbar - Nachteile:   - Empfindlich gegen Ausrei\u00dfer   - Ver\u00e4ndert die Verteilungsform</p>"},{"location":"KW4/scaling/#23-robustscaler","title":"2.3 RobustScaler","text":"<p>Verwendet Statistiken, die robust gegen Ausrei\u00dfer sind (Median und IQR).</p> <pre><code>from sklearn.preprocessing import RobustScaler\n\nscaler = RobustScaler()\nscaled_data = scaler.fit_transform(data)\n\nprint(\"Robust Skaliert:\\n\", pd.DataFrame(scaled_data, columns=data.columns))\n</code></pre> <p>Formel: x_scaled = (x - median) / IQR - Vorteile:   - Sehr robust gegen Ausrei\u00dfer   - Gut f\u00fcr schiefe Verteilungen - Nachteile:   - Keine garantierten Grenzen   - Weniger effizient bei normalverteilten Daten</p>"},{"location":"KW4/scaling/#24-vergleich-der-methoden","title":"2.4 Vergleich der Methoden","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n\n# Daten mit Ausrei\u00dfer\ndata = np.array([1, 2, 3, 4, 100]).reshape(-1, 1)\n\nscalers = {\n    'Standard': StandardScaler(),\n    'MinMax': MinMaxScaler(),\n    'Robust': RobustScaler()\n}\n\n# Skalierung vergleichen\nfor name, scaler in scalers.items():\n    scaled = scaler.fit_transform(data)\n    print(f\"\\n{name} Scaler:\")\n    print(f\"Skalierte Werte: {scaled.flatten()}\")\n</code></pre>"},{"location":"KW4/scaling/#3-anwendungsgebiete","title":"3. Anwendungsgebiete","text":""},{"location":"KW4/scaling/#31-wann-welche-skalierung","title":"3.1 Wann welche Skalierung?","text":"Szenario Empfohlene Skalierung Grund Neural Networks StandardScaler Schnellere Konvergenz KNN, K-Means MinMaxScaler Gleiche Gewichtung aller Features Daten mit Ausrei\u00dfern RobustScaler Minimaler Einfluss von Ausrei\u00dfern SVM StandardScaler Optimale Hyperebenen-Berechnung"},{"location":"KW4/scaling/#32-praktisches-beispiel-vergleich-der-auswirkungen","title":"3.2 Praktisches Beispiel: Vergleich der Auswirkungen","text":"<pre><code>from sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Daten laden\niris = load_iris()\nX_train, X_test, y_train, y_test = train_test_split(\n    iris.data, iris.target, random_state=42)\n\n# Verschiedene Skalierungen testen\nresults = {}\nfor name, scaler in scalers.items():\n    # Daten skalieren\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_test_scaled = scaler.transform(X_test)\n\n    # Model trainieren\n    knn = KNeighborsClassifier(n_neighbors=3)\n    knn.fit(X_train_scaled, y_train)\n\n    # Vorhersagen\n    y_pred = knn.predict(X_test_scaled)\n    accuracy = accuracy_score(y_test, y_pred)\n    results[name] = accuracy\n\nprint(\"\\nKNN Accuracy mit verschiedenen Skalierungen:\")\nfor name, acc in results.items():\n    print(f\"{name}: {acc:.3f}\")\n</code></pre>"},{"location":"KW4/scaling/#4-best-practices","title":"4. Best Practices","text":""},{"location":"KW4/scaling/#41-wichtige-regeln","title":"4.1 Wichtige Regeln:","text":"<ol> <li>Skalierung immer nur auf Trainingsdaten fitten</li> <li>Gleiche Skalierung auf Test-/Validierungsdaten anwenden</li> <li>Skalierung vor Feature Selection/Engineering</li> <li>Bei Updates: Alte Skalierungsparameter verwenden</li> </ol>"},{"location":"KW4/scaling/#42-pipeline-integration","title":"4.2 Pipeline-Integration:","text":"<pre><code>from sklearn.pipeline import Pipeline\n\n# Korrekte Integration in Pipeline\npipeline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('classifier', KNeighborsClassifier())\n])\n\n# Falsch w\u00e4re:\nX_scaled = scaler.fit_transform(X)  # Data Leakage!\n</code></pre>"},{"location":"KW4/smote/","title":"SMOTE: Theoretischer Hintergrund und mathematische Grundlagen","text":""},{"location":"KW4/smote/#motivation","title":"Motivation","text":"<p>SMOTE ist aus mehreren wichtigen Gr\u00fcnden f\u00fcr das maschinelle Lernen bedeutsam:</p> <ul> <li> <p>Problem der Modellverzerrung    Ein klassisches Beispiel: Stell dir vor, du hast einen Datensatz mit 1000 gesunden und nur 10 kranken Patienten. Ein naives Modell k\u00f6nnte einfach IMMER \"gesund\" vorhersagen und h\u00e4tte damit 99% Genauigkeit - w\u00e4re aber v\u00f6llig nutzlos f\u00fcr den eigentlichen Zweck.</p> </li> <li> <p>Reale Anwendungsf\u00e4lle sind oft unbalanciert</p> <ul> <li>Betrugserkennung: Nur etwa 0.1% aller Transaktionen sind betr\u00fcgerisch</li> <li>Medizinische Diagnosen: Seltene Krankheiten betreffen oft weniger als 1% der Patienten</li> <li>Maschinenwartung: Kritische Ausf\u00e4lle sind selten, aber sehr wichtig zu erkennen</li> <li>Spam-Erkennung: Normalerweise sind deutlich weniger als 50% der E-Mails Spam</li> </ul> </li> <li> <p>Auswirkungen auf die Modellleistung ohne SMOTE:</p> </li> </ul> <pre><code># Beispiel mit stark unbalancierten Daten\nfrom sklearn.metrics import classification_report\n\n# Angenommen: 990 negative, 10 positive F\u00e4lle\ny_true = [0]*990 + [1]*10\ny_pred = [0]*1000  # Modell sagt immer 0 vorher\n\nprint(classification_report(y_true, y_pred))\n# Ergebnis:\n# Klasse 0: Precision=0.99, Recall=1.00\n# Klasse 1: Precision=0.00, Recall=0.00\n</code></pre> <ul> <li> <p>Verbesserungen durch SMOTE:</p> <ul> <li>Balanced Accuracy steigt (oft um 20-30%)</li> <li>Recall f\u00fcr die Minderheitsklasse verbessert sich deutlich </li> <li>Das Modell lernt tats\u00e4chlich die relevanten Merkmale der Minderheitsklasse</li> </ul> </li> <li> <p>Praktische Bedeutung:</p> <ul> <li>Ein Beispiel aus der Kreditkartenbetrugsbek\u00e4mpfung:<ul> <li>Ohne SMOTE: Modell erkennt 30% der Betrugsf\u00e4lle</li> <li>Mit SMOTE: Modell erkennt 80% der Betrugsf\u00e4lle</li> <li>Resultat: Millionen \u20ac weniger Schaden durch Betrug</li> </ul> </li> </ul> </li> <li> <p>Warum nicht einfach Oversampling?</p> <ul> <li> <p>Einfaches Oversampling (Kopieren der Minderheitsklasse) f\u00fchrt zu:</p> <ul> <li>Overfitting auf die wenigen echten Beispiele</li> <li>Keine neuen Informationen f\u00fcr das Modell</li> <li>Schlechter Generalisierung</li> </ul> </li> <li> <p>SMOTE ist besonders wichtig in kritischen Anwendungen:</p> <ul> <li>Medizinische Diagnostik (seltene Krankheiten)</li> <li>Sicherheitssysteme (Eindringlingsversuche)</li> <li>Qualit\u00e4tskontrolle (seltene Produktfehler)</li> <li>Finanzwesen (Betrugsversuche)</li> </ul> </li> <li> <p>Ein konkretes Beispiel aus der Praxis bei der Erkennung von Brustkrebs in Mammographien:</p> <ul> <li>Urspr\u00fcngliche Daten: 98% negativ, 2% positiv</li> <li>Modell ohne SMOTE: 99% Accuracy, aber nur 20% der Krebsf\u00e4lle erkannt</li> <li>Modell mit SMOTE: 95% Accuracy, aber 90% der Krebsf\u00e4lle erkannt \u2192 Der leichte R\u00fcckgang in der Gesamtgenauigkeit ist hier ein kleiner Preis f\u00fcr die deutlich verbesserte Erkennung der wichtigen positiven F\u00e4lle.</li> </ul> </li> </ul> </li> </ul>"},{"location":"KW4/smote/#2-grundlegendes-problem-imbalanced-learning","title":"2. Grundlegendes Problem: Imbalanced Learning","text":"<p>Das fundamentale Problem, das SMOTE (Synthetic Minority Over-sampling Technique) adressiert, liegt in der Natur unausgewogener Datens\u00e4tze. In vielen realen Anwendungsf\u00e4llen ist die Klassenverteilung stark ungleich verteilt, was zu verschiedenen Herausforderungen f\u00fchrt:</p>"},{"location":"KW4/smote/#21-mathematische-probleme-bei-imbalanced-data","title":"2.1 Mathematische Probleme bei Imbalanced Data","text":"<ul> <li>Die Likelihood-Funktion wird von der Mehrheitsklasse dominiert</li> <li>Die Prior-Wahrscheinlichkeit der Minderheitsklasse ist sehr klein</li> <li>Klassische Verlustfunktionen wie Cross-Entropy optimieren haupts\u00e4chlich auf die Mehrheitsklasse</li> </ul>"},{"location":"KW4/smote/#22-auswirkungen-auf-das-machine-learning","title":"2.2 Auswirkungen auf das Machine Learning","text":"<ul> <li>Klassifikatoren tendieren zur Mehrheitsklasse</li> <li>Die Decision Boundary wird zugunsten der Mehrheitsklasse verschoben</li> <li>Feature-Importance-Metriken werden verzerrt</li> </ul>"},{"location":"KW4/smote/#3-mathematische-grundlagen-von-smote","title":"3. Mathematische Grundlagen von SMOTE","text":""},{"location":"KW4/smote/#31-grundprinzip","title":"3.1 Grundprinzip","text":"<p>SMOTE basiert auf dem Konzept der Interpolation im Feature-Raum. F\u00fcr jeden Punkt x\u2081 der Minderheitsklasse wird ein synthetischer Punkt x_new wie folgt erzeugt:</p> <p>x_new = x\u2081 + \u03bb(x\u2082 - x\u2081)</p> <p>wobei:    - x\u2081 ist der Ausgangspunkt in der Minderheitsklasse    - x\u2082 ist einer der k n\u00e4chsten Nachbarn von x\u2081    - \u03bb ist ein zuf\u00e4lliger Wert zwischen 0 und 1</p>"},{"location":"KW4/smote/#32-k-nearest-neighbors-im-smote-kontext","title":"3.2 K-Nearest Neighbors im SMOTE-Kontext","text":"<p>Die Auswahl der k n\u00e4chsten Nachbarn erfolgt typischerweise im euklidischen Raum:</p> <p>d(x\u2081, x\u2082) = \u221a(\u03a3(x\u2081\u1d62 - x\u2082\u1d62)\u00b2)</p> <p>Die Wahl von k beeinflusst dabei:    - Die Varianz der synthetischen Beispiele    - Die Dichte der generierten Punkte    - Die Gefahr von Overfitting oder Underfitting</p>"},{"location":"KW4/smote/#4-theoretische-eigenschaften-und-garantien","title":"4. Theoretische Eigenschaften und Garantien","text":""},{"location":"KW4/smote/#41-topologische-eigenschaften","title":"4.1 Topologische Eigenschaften","text":"<p>SMOTE erh\u00e4lt wichtige topologische Eigenschaften des urspr\u00fcnglichen Datenraums:</p> <ul> <li>Die konvexe H\u00fclle der Minderheitsklasse bleibt erhalten</li> <li>Die Dichte-Verteilung wird lokal approximiert</li> <li>Cluster-Strukturen bleiben weitgehend erhalten</li> </ul>"},{"location":"KW4/smote/#42-statistische-eigenschaften","title":"4.2 Statistische Eigenschaften","text":"<p>Die generierten Datenpunkte haben folgende statistische Eigenschaften:</p> <ul> <li>Der Erwartungswert liegt auf der Verbindungslinie zwischen zwei realen Punkten</li> <li>Die Varianz wird durch die lokale Nachbarschaft bestimmt</li> <li>Die Kovarianzstruktur der originalen Daten bleibt approximativ erhalten</li> </ul>"},{"location":"KW4/smote/#5-theoretische-limitationen","title":"5. Theoretische Limitationen","text":""},{"location":"KW4/smote/#51-grundlegende-einschrankungen","title":"5.1 Grundlegende Einschr\u00e4nkungen","text":"<ul> <li>Die Methode ist anf\u00e4llig f\u00fcr Rauschen in den Originaldaten</li> <li>Bei hochdimensionalen Daten kann der Fluch der Dimensionalit\u00e4t problematisch werden</li> </ul>"},{"location":"KW4/smote/#52-statistische-limitationen","title":"5.2 Statistische Limitationen","text":"<ul> <li>Die wahre Verteilung der Minderheitsklasse wird nur approximiert</li> <li>Extreme Werte (Outlier) k\u00f6nnen zu verzerrten synthetischen Beispielen f\u00fchren</li> <li>Die Unabh\u00e4ngigkeitsannahme zwischen den Features wird implizit vorausgesetzt</li> </ul>"},{"location":"KW4/smote/#6-theoretische-verbindungen-zu-anderen-konzepten","title":"6. Theoretische Verbindungen zu anderen Konzepten","text":""},{"location":"KW4/smote/#61-verbindung-zur-manifold-hypothese","title":"6.1 Verbindung zur Manifold-Hypothese","text":"<p>SMOTE basiert implizit auf der Annahme, dass: - Die Daten auf einem niedrigdimensionalen Manifold liegen - Lineare Interpolation auf diesem Manifold sinnvoll ist - Lokale Strukturen global relevant sind</p>"},{"location":"KW4/smote/#62-beziehung-zu-anderen-sampling-methoden","title":"6.2 Beziehung zu anderen Sampling-Methoden","text":"<ul> <li>Random Oversampling: Spezialfall mit \u03bb = 0</li> <li>Tomek Links: Komplement\u00e4re Methode zur Grenzbereinigung</li> <li>Condensed Nearest Neighbor: Alternative f\u00fcr Unterabtastung</li> </ul>"},{"location":"KW4/assignments/analytics-dashbaord/","title":"Aufgabe","text":"<p>Erstellt ein Dashboard, auf wir verschiedene Algorithmen, Preprocessoren und Metriken f\u00fcr Klassifikationsprobleme vergleichen k\u00f6nnen.</p> <p>F\u00fcr die Erstellung sind sowohl Dash, als auch Stramlit zul\u00e4ssig.</p> <p>Folgende Kernfunktion sollte das Dashboard beinhalten:</p> <ul> <li>Datensatz ausw\u00e4hlen</li> <li>Visualisierung ausw\u00e4hlen</li> <li>Modelle Ausw\u00e4hlen</li> <li>Vorverabeitungsmodell ausw\u00e4hlen</li> </ul>"},{"location":"KW4/assignments/analytics-dashbaord/#wie-kann-man-starten","title":"Wie kann man starten","text":"<ol> <li>Erstelle den Dropdown, welcher verschiedene Datens\u00e4tze ausw\u00e4hlen l\u00e4sst. Wir beschr\u00e4nken uns hier mal auf Klassifikation.<ul> <li>M\u00f6gliche Datens\u00e4tzen findest du beispielsweise hier: https://github.com/mwaskom/seaborn-data</li> <li>Erstelle einen Multiselect, um die Spalten des Datensatzes auszuw\u00e4hlen.</li> <li>Erstelle ein Barplot mit der verteilung der Klassen anzeigt</li> <li>Erm\u00f6gliche \u00fcber einen Dropdown das einsetzen von SMOTE </li> <li>Zeige, wie sich dadurch Verteilung und Gr\u00f6\u00dfe der Klassen ver\u00e4ndert</li> <li>Erstelle ein Scatterplot, welches die ersten beiden Dimensionen des Datensatzes anzeigt. Die Punkte sollen nach Klassen gef\u00e4rbt sein.</li> <li>Erstelle einen Dropdown, um eine Klassifikationsmodell auszuw\u00e4hlen. M\u00f6gliche Wahlen sind:<ul> <li>KNN</li> <li>Decision Tree</li> <li>SVM</li> </ul> </li> <li>Je nach Modell kannst du weitere Parameter anpassbar machen</li> <li>Gib evaluationsmetriken aus</li> </ul> </li> </ol>"},{"location":"KW4/assignments/car-features/","title":"Car Features and MSRP - 2","text":""},{"location":"KW4/assignments/car-features/#aufgabe-1","title":"Aufgabe 1","text":"<p>In der KW 2 haben wir bereits ein Klassifikationsmodell f\u00fcr die Vorhersage der Marke eines Autos erstellt. </p>"},{"location":"KW4/assignments/car-features/#aufgabe-11","title":"Aufgabe 1.1","text":"<p>Teste an dieser Analyse den Einsatz von SMOTE f\u00fcr das Balancieren des Datensatzes. </p>"},{"location":"KW4/assignments/car-features/#aufgabe-12","title":"Aufgabe 1.2","text":"<p>Vergleiche die Ergebnisse mit dem unbalancierten Datensatz.</p>"},{"location":"KW4/assignments/car-features/#aufgabe-13","title":"Aufgabe 1.3","text":"<p>F\u00fchre das Upsampling einmal vor und einmal nach dem Split durch. Was sind die Unterschiede? Was sind die Vor- / Nachteile der beiden Methoden?</p>"},{"location":"KW4/assignments/car-features/#aufgabe-14","title":"Aufgabe 1.4","text":"<p>Verwende in der Vorverarbeitung der Daten eine PCA. Wie ver\u00e4ndert sich die Genauigkeit des Modells?</p>"},{"location":"KW4/assignments/car-features/#aufgabe-15","title":"Aufgabe 1.5","text":"<p>Vergleiche den Einsatz der PCA mit deine Spaltenauswahl auf basis der Korrelationen.</p>"},{"location":"KW49-50/data_related_roles/","title":"Rollen im Zusammenhang mit Datenbasierten Projekten","text":"<p>Datenbasierte Projekte sind oft komplex und erfordern die Zusammenarbeit von Fachkr\u00e4ften mit unterschiedlichen Kompetenzen. Eine klare Definition der Rollen und Verantwortlichkeiten hilft, die Zusammenarbeit effizient zu gestalten und das Projektergebnis zu optimieren. In diesem Kapitel schauen wir uns die eine der Rollen in datenbasierten Projekten an und welche Aufgaben sie typischerweise \u00fcbernehmen.</p>"},{"location":"KW49-50/data_related_roles/#1-data-scientist","title":"1. Data Scientist","text":"<p>Aufgaben:</p> <ul> <li>Entwicklung und Anwendung von Algorithmen zur Datenanalyse und Modellierung.</li> <li>Experimentieren mit Machine-Learning-Modellen und Auswahl der besten Ans\u00e4tze.</li> <li>Kommunikation der Ergebnisse in Form von Berichten oder Visualisierungen.</li> </ul> <p>Ben\u00f6tigte Kompetenzen:</p> <ul> <li>Starke mathematische und statistische Kenntnisse.</li> <li>Erfahrung mit Programmiersprachen wie Python oder R.</li> <li>F\u00e4higkeit, komplexe Zusammenh\u00e4nge einfach zu erkl\u00e4ren.</li> </ul>"},{"location":"KW49-50/data_related_roles/#2-data-engineer","title":"2. Data Engineer","text":"<p>Aufgaben:</p> <ul> <li>Aufbau und Wartung der Dateninfrastruktur (z. B. Datenbanken, Pipelines).</li> <li>Sicherstellung, dass Daten f\u00fcr Analysen verf\u00fcgbar und qualitativ hochwertig sind.</li> <li>Optimierung von Datenspeichersystemen hinsichtlich Leistung und Skalierbarkeit.</li> </ul> <p>Ben\u00f6tigte Kompetenzen:</p> <ul> <li>Erfahrung mit ETL-Prozessen (Extract, Transform, Load).</li> <li>Kenntnisse in Datenbanktechnologien wie SQL, NoSQL oder Big Data Tools.</li> <li>Verst\u00e4ndnis von Cloud-Diensten wie AWS, Azure oder Google Cloud.</li> </ul>"},{"location":"KW49-50/data_related_roles/#3-business-analyst","title":"3. Business Analyst","text":"<p>Aufgaben:</p> <ul> <li>Verstehen der gesch\u00e4ftlichen Anforderungen und \u00dcbersetzung in datenbezogene Fragestellungen.</li> <li>Interpretation der Analyseergebnisse und Ableitung gesch\u00e4ftlicher Entscheidungen.</li> <li>Identifikation von Potenzialen zur Verbesserung von Prozessen oder Produkten.</li> </ul> <p>Ben\u00f6tigte Kompetenzen:</p> <ul> <li>Kenntnisse \u00fcber die Branche und den spezifischen Gesch\u00e4ftskontext.</li> <li>F\u00e4higkeit, mit Stakeholdern zu kommunizieren und deren Anforderungen zu priorisieren.</li> <li>Grundlagen in Datenvisualisierung und Reporting-Tools (z. B. Tableau, Power BI).</li> </ul>"},{"location":"KW49-50/data_related_roles/#4-data-analyst","title":"4. Data Analyst","text":"<p>Aufgaben:</p> <ul> <li>Analyse von Daten, um Muster, Trends und Anomalien zu erkennen.</li> <li>Erstellung von Dashboards und Berichten, die wichtige Erkenntnisse darstellen.</li> <li>Zusammenarbeit mit anderen Teams, um datengest\u00fctzte Entscheidungen zu treffen.</li> </ul> <p>Ben\u00f6tigte Kompetenzen:</p> <ul> <li>Starke Kenntnisse in Statistik und Datenanalyse.</li> <li>Erfahrung mit Tools wie Excel, SQL oder spezialisierten Analyseplattformen.</li> <li>F\u00e4higkeit, Daten effektiv zu visualisieren.</li> </ul>"},{"location":"KW49-50/data_related_roles/#5-project-manager","title":"5. Project Manager","text":"<p>Aufgaben:</p> <ul> <li>Planung und Steuerung des Projekts, um sicherzustellen, dass es p\u00fcnktlich und im Budget bleibt.</li> <li>Koordination zwischen verschiedenen Teams und Stakeholdern.</li> <li>Identifikation und Management von Risiken.</li> </ul> <p>Ben\u00f6tigte Kompetenzen:</p> <ul> <li>Erfahrung in agilen Methoden wie Scrum oder Kanban.</li> <li>Kommunikations- und Probleml\u00f6sungsf\u00e4higkeiten.</li> <li>Verst\u00e4ndnis der grundlegenden Aspekte datenbasierter Projekte..</li> </ul>"},{"location":"KW49-50/data_related_roles/#7-machine-learning-engineer","title":"7. Machine Learning Engineer","text":"<p>Aufgaben:</p> <ul> <li>Entwicklung, Optimierung und Deployment von Machine-Learning-Modellen.</li> <li>Integration von Modellen in produktive Systeme.</li> <li>Sicherstellung der Skalierbarkeit und Leistung von Modellen im Echtbetrieb.</li> </ul> <p>Ben\u00f6tigte Kompetenzen:</p> <ul> <li>Kenntnisse in Softwareentwicklung und Machine Learning.</li> <li>Erfahrung mit Frameworks wie TensorFlow, PyTorch oder Scikit-Learn.</li> <li>Verst\u00e4ndnis von MLOps-Tools und -Prozessen.</li> </ul>"},{"location":"KW49-50/data_science_buzzwords/","title":"Begriffe aus dem Data-Science-Kosmos","text":"<p>In der Welt der Datenwissenschaft begegnen wir h\u00e4ufig Begriffen, die eng miteinander verwandt sind, aber unterschiedliche Bedeutungen haben. Dieses Kapitel bietet dir eine strukturierte \u00dcbersicht, um Klarheit in die wichtigsten Begriffe zu bringen und sie einzuordnen.</p>"},{"location":"KW49-50/data_science_buzzwords/#1-grundlagenbegriffe","title":"1. Grundlagenbegriffe","text":""},{"location":"KW49-50/data_science_buzzwords/#kunstliche-intelligenz-ki","title":"K\u00fcnstliche Intelligenz (KI)","text":"<ul> <li>Definition: KI umfasst Systeme, die menschen\u00e4hnliches Denken und Verhalten simulieren, z. B. durch Probleml\u00f6sung, Mustererkennung oder Entscheidungsfindung.</li> <li>Einsatzgebiete: Sprach\u00fcbersetzung, Bilderkennung, autonome Fahrzeuge.</li> <li>Beziehung zu anderen Begriffen: Oberbegriff f\u00fcr Machine Learning und Deep Learning.</li> </ul>"},{"location":"KW49-50/data_science_buzzwords/#machine-learning-ml","title":"Machine Learning (ML)","text":"<ul> <li>Definition: Teilbereich der KI, der es Systemen erlaubt, aus Daten zu lernen und Vorhersagen oder Entscheidungen zu treffen, ohne explizit programmiert zu sein.</li> <li>Typen von ML:</li> <li>Supervised Learning (z. B. Regression, Klassifikation)</li> <li>Unsupervised Learning (z. B. Clustering, Dimensionsreduktion)</li> <li>Reinforcement Learning (z. B. Steuerung von Robotern)</li> <li>Beispiel: Vorhersage von Hauspreisen basierend auf historischen Daten.</li> </ul>"},{"location":"KW49-50/data_science_buzzwords/#deep-learning-dl","title":"Deep Learning (DL)","text":"<ul> <li>Definition: Spezialisierter Bereich des Machine Learning, der auf neuronalen Netzwerken mit vielen Schichten basiert.</li> <li>Anwendung: Bilderkennung, Spracherkennung, Generative Modelle (z. B. Text-to-Image-Generierung).</li> <li>Beziehung zu ML und KI: Deep Learning ist ein Teilbereich von ML und somit auch der KI zuzuordnen.</li> </ul>"},{"location":"KW49-50/data_science_buzzwords/#statistik","title":"Statistik","text":"<ul> <li>Definition: Wissenschaft der Datenanalyse zur Gewinnung von Informationen und zur Entscheidungsfindung.</li> <li>Rolle im Data Science: Grundlage vieler Algorithmen im ML (z. B. lineare Regression, Hypothesentests).</li> <li>Abgrenzung zu ML: Statistik betont Erkl\u00e4rbarkeit und Hypothesentestung, w\u00e4hrend ML prim\u00e4r auf Vorhersage fokussiert ist.</li> </ul>"},{"location":"KW49-50/data_science_buzzwords/#2-datenverarbeitung-und-strukturen","title":"2. Datenverarbeitung und -strukturen","text":""},{"location":"KW49-50/data_science_buzzwords/#datenpipeline","title":"Datenpipeline","text":"<ul> <li>Definition: Prozess, der Daten von der Sammlung \u00fcber die Verarbeitung bis zur Analyse automatisiert.</li> <li>Bestandteile: Extraktion, Transformation, Laden (ETL-Prozesse).</li> <li>Beispiel: Daten von IoT-Ger\u00e4ten werden gesammelt, bereinigt und an ein Analysemodell \u00fcbergeben.</li> </ul>"},{"location":"KW49-50/data_science_buzzwords/#big-data","title":"Big Data","text":"<ul> <li>Definition: Verarbeitung und Analyse extrem gro\u00dfer Datenmengen, die mit traditionellen Methoden nicht effizient bearbeitet werden k\u00f6nnen.</li> <li>Merkmale (die 4 Vs): Volume, Velocity, Variety, Veracity.</li> <li>Technologien: Hadoop, Spark.</li> </ul>"},{"location":"KW49-50/data_science_buzzwords/#cloud-computing","title":"Cloud Computing","text":"<ul> <li>Definition: Bereitstellung von Rechenleistung, Speicher und Anwendungen \u00fcber das Internet.</li> <li>Bezug zu Data Science: Erm\u00f6glicht skalierbare Speicherung und Verarbeitung gro\u00dfer Datenmengen.</li> <li>Beispiele: Amazon Web Services (AWS), Google Cloud Platform (GCP).</li> </ul>"},{"location":"KW49-50/data_science_buzzwords/#3-analysemethoden-und-modellierung","title":"3. Analysemethoden und Modellierung","text":""},{"location":"KW49-50/data_science_buzzwords/#explorative-datenanalyse-eda","title":"Explorative Datenanalyse (EDA)","text":"<ul> <li>Definition: Erste Analyse von Daten zur Entdeckung von Mustern, Anomalien und Zusammenh\u00e4ngen.</li> <li>Werkzeuge: Pandas, Seaborn, Matplotlib.</li> <li>Beziehung zu ML: Bereitet Daten f\u00fcr ML-Modelle vor.</li> </ul>"},{"location":"KW49-50/data_science_buzzwords/#feature-engineering","title":"Feature Engineering","text":"<ul> <li>Definition: Erstellung neuer Variablen (Features), um die Leistung eines Modells zu verbessern.</li> <li>Beispiele: Normalisierung, One-Hot-Encoding, Feature-Auswahl.</li> <li>Rolle in ML: Entscheidender Faktor f\u00fcr Modellgenauigkeit.</li> </ul>"},{"location":"KW49-50/data_science_buzzwords/#preprocessing","title":"Preprocessing","text":"<ul> <li>Definition: Vorbereitung der Rohdaten f\u00fcr die Modellierung.</li> <li>Beispiele: Skalierung, fehlende Werte behandeln, Kategorisierung.</li> <li>Unterschied zu Feature Engineering: Beim Preprocessing geht es um generelle Datenvorbereitung, w\u00e4hrend Feature Engineering neue Informationen aus bestehenden Daten erzeugt.</li> </ul>"},{"location":"KW49-50/data_science_buzzwords/#data-cleaning","title":"Data Cleaning","text":"<ul> <li>Definition: Identifikation und Korrektur fehlerhafter, unvollst\u00e4ndiger oder ungenauer Daten.</li> <li>Beispiele: Entfernen von Duplikaten, Bereinigung von Textdaten, Umgang mit Ausrei\u00dfern.</li> <li>Rolle im Workflow: Grundlegender Schritt, der sicherstellt, dass Daten verl\u00e4sslich sind.</li> </ul>"},{"location":"KW49-50/data_science_buzzwords/#4-technologische-begriffe","title":"4. Technologische Begriffe","text":""},{"location":"KW49-50/data_science_buzzwords/#apis-application-programming-interfaces","title":"APIs (Application Programming Interfaces)","text":"<ul> <li>Definition: Schnittstellen, die den Zugriff auf Funktionen oder Daten eines Systems erlauben.</li> <li>Rolle in Projekten: Verbindung von Modellen mit Anwendungen oder Datenquellen.</li> <li>Beispiel: REST-APIs, FastAPI.</li> </ul>"},{"location":"KW49-50/data_science_buzzwords/#datenbanken","title":"Datenbanken","text":"<ul> <li>Relational: Daten in Tabellenform (z. B. MySQL, PostgreSQL).</li> <li>NoSQL: Flexible Speicherung von unstrukturierten Daten (z. B. MongoDB, Cassandra).</li> <li>Relevanz: Speicherung, Abfrage und Analyse von Daten.</li> </ul>"},{"location":"KW49-50/data_science_buzzwords/#containerization","title":"Containerization","text":"<ul> <li>Definition: Verpackung einer Anwendung mit ihrer gesamten Abh\u00e4ngigkeit in einem Container.</li> <li>Technologie: Docker.</li> <li>Nutzen: Erleichtert die Bereitstellung und Skalierung.</li> </ul>"},{"location":"KW49-50/data_science_buzzwords/#5-abgrenzung-ahnlicher-begriffe","title":"5. Abgrenzung \u00e4hnlicher Begriffe","text":""},{"location":"KW49-50/data_science_buzzwords/#ki-vs-ml-vs-dl-vs-statistik","title":"KI vs. ML vs. DL vs. Statistik","text":"<ul> <li>KI: Der umfassende Rahmen.</li> <li>ML: L\u00e4sst Maschinen aus Daten lernen.</li> <li>DL: Subdisziplin, spezialisiert auf neuronale Netzwerke.</li> <li>Statistik: Fokus auf Dateninterpretation und Erkl\u00e4rbarkeit.</li> </ul>"},{"location":"KW49-50/data_science_buzzwords/#big-data-vs-datenanalyse","title":"Big Data vs. Datenanalyse","text":"<ul> <li>Big Data: Umgang mit der Masse an Daten.</li> <li>Datenanalyse: Fokus auf das Verstehen und die Interpretation der Daten.</li> </ul>"},{"location":"KW49-50/data_science_buzzwords/#feature-engineering-vs-preprocessing-vs-data-cleaning","title":"Feature Engineering vs. Preprocessing vs. Data Cleaning","text":"<ul> <li>Feature Engineering: Erstellung neuer Features, die die Modellleistung verbessern (z. B. Kombination von Variablen, Erstellung von Zeitmerkmalen).</li> <li>Preprocessing: Allgemeine Datenvorbereitung wie Normalisierung, Skalierung oder Encoding.</li> <li>Data Cleaning: Beseitigung von Problemen in den Rohdaten, wie fehlerhaften Eintr\u00e4gen oder fehlenden Werten.</li> </ul>"},{"location":"KW49-50/goals/","title":"Ziele","text":"<p>Ich kann...</p> <p>... die Schritte einer EDA beschreiben. ... missing values in einem Datensatz identifizieren. ... die Verteilung von Daten beschreiben. ... die Verteilung von Daten visualisieren. ... missing values in einem Datensatz bearbeiten. ... Ausrei\u00dfer in einem Datensatz identifizieren. ... Ausrei\u00dfer in einem Datensatz bearbeiten. ... Duplikate in einem Datensatz identifizieren. ... Duplikate in einem Datensatz bearbeiten. ... zwischen verschiedenen Berufsgruppen im Zusammenhangn mit Data Science unterscheiden. ... zwischen verschiedenen Begiffen im Zusammenhang mit Data Science unterscheiden.</p>"},{"location":"KW49-50/overview/","title":"Grundlagen von Data Science","text":"<p>Data Science ist ein interdisziplin\u00e4res Feld, das datengetriebene Erkenntnisse gewinnt, um Entscheidungsprozesse zu verbessern und neue Muster sowie Zusammenh\u00e4nge aufzudecken. Dieses Kapitel bietet dir eine Einf\u00fchrung in die grundlegenden Konzepte und Prinzipien, die im Zentrum dieses spannenden Bereichs stehen.</p>"},{"location":"KW49-50/overview/#was-ist-data-science","title":"Was ist Data Science?","text":"<p>Data Science kombiniert verschiedene Disziplinen, darunter:</p> <ul> <li>Statistik: Die Wissenschaft der Datenanalyse und des Schlussfolgerns.</li> <li>Informatik: Methoden zur Verarbeitung, Analyse und Visualisierung gro\u00dfer Datenmengen.</li> <li>Dom\u00e4nenwissen: Spezifisches Wissen \u00fcber den Anwendungsbereich der Datenanalyse.</li> </ul> <p>Das Ziel von Data Science ist es, aus strukturierten und unstrukturierten Daten Wissen zu generieren.</p> <p>Wir haben in der Einf\u00fchrung zur Data Analysis bereits 4 Grundlegende Arten der Datenanalyse (Descriptive Analysis, Diagnostic Analysis, Predictive Analysis, Prescriptive Analysis) kennengelernt. Die einzelnen Schritte dieser Analysen sind ein wesentlicher Bestandteil der Data Science. Dar\u00fcber hinaus gibt es weitere wichtige Konzepte und Methoden, die in der Datenwissenschaft eine Rolle spielen. In einem Kompletten Data Science Prozess werden unter anderem die folgenden Schritte durchlaufen:</p>"},{"location":"KW49-50/overview/#1-problemdefinition","title":"1. Problemdefinition","text":"<ul> <li>Ziel: Verstehen, welches Problem gel\u00f6st werden soll.</li> <li>Fragen: Welche Entscheidungen sollen unterst\u00fctzt werden? Welche Daten sind verf\u00fcgbar?</li> </ul>"},{"location":"KW49-50/overview/#2-datenbeschaffung","title":"2. Datenbeschaffung","text":"<ul> <li>Quellen: Daten k\u00f6nnen aus internen Systemen, externen APIs, Sensoren oder Web-Scraping stammen.</li> <li>Wichtig: Qualit\u00e4t und Verf\u00fcgbarkeit der Daten beurteilen.</li> </ul>"},{"location":"KW49-50/overview/#3-datenaufbereitung","title":"3. Datenaufbereitung","text":"<ul> <li>Data Cleaning: Umgang mit fehlenden Werten, Duplikaten oder Ausrei\u00dfern.</li> <li>Feature Engineering: Erstellung relevanter Variablen, die Modelle verbessern.</li> <li>Preprocessing: Skalieren, Normalisieren und Kodieren von Daten.</li> <li>ETL-Prozesse: (Extract, Transform, Load) Umwandlung und Integration von Daten aus verschiedenen Quellen.</li> </ul>"},{"location":"KW49-50/overview/#4-modellierung","title":"4. Modellierung","text":"<ul> <li>Ziel: Aufbau von pr\u00e4diktiven oder deskriptiven Modellen.</li> <li>Methoden: Klassifikation, Regression, Clustering, etc.</li> </ul>"},{"location":"KW49-50/overview/#5-validierung","title":"5. Validierung","text":"<ul> <li>Metriken: Genauigkeit, F1-Score, RMSE.</li> <li>Cross-Validation: Pr\u00fcfung der Generalisierungsf\u00e4higkeit eines Modells.</li> </ul>"},{"location":"KW49-50/overview/#6-deployment","title":"(6. Deployment)","text":"<ul> <li>Anwendung: Bereitstellung von Modellen in produktiven Systemen.</li> <li>Wartung: Monitoring der Modellleistung und Aktualisierung.</li> </ul>"},{"location":"KW49-50/overview/#mathematische-optimierung-in-data-science","title":"Mathematische Optimierung in Data Science","text":"<p>Data Science setzt sich zwar prim\u00e4r aus den Statistik, Informatik und Dom\u00e4nenwissen zusammen, aber es gibt auch viele andere Disziplinen, die in diesem Bereich eine Rolle spielen. Zu diesen geh\u00f6rt insbesondere die Mathematische Optimierung. Bei vielen statistischen Methoden und Machine-Learning-Modellen handelt es sich um Optimierungsprobleme, bei denen das Ziel darin besteht, die besten Parameter f\u00fcr ein Modell zu finden. Wir gehen im Nachfolgenden auf einige Begrifflichkeiten und Konzepte der Mathematischen Optimierung ein, welche uns als Data Scientisten regelm\u00e4\u00dfig begegnen werden.</p>"},{"location":"KW49-50/overview/#1-definition","title":"1. Definition","text":"<ul> <li>Ziel: Finden der besten L\u00f6sung f\u00fcr ein Problem unter gegebenen Einschr\u00e4nkungen.</li> <li>Beispiele: Minimierung des Fehlers eines Modells, Maximierung der Genauigkeit oder Optimierung von Ressourcen.</li> </ul>"},{"location":"KW49-50/overview/#2-gradient-descent","title":"2. Gradient Descent","text":"<ul> <li>Beschreibung: Iterativer Algorithmus, um die optimalen Parameter eines Modells zu finden.</li> <li>Funktionsweise: Bewegung in Richtung des steilsten Gradienten der Fehlerfunktion, um das Minimum zu erreichen.</li> <li>Einsatzgebiete: Training neuronaler Netzwerke, lineare Regression.</li> </ul>"},{"location":"KW49-50/overview/#3-regularisierung","title":"3. Regularisierung","text":"<ul> <li>Ziel: Verhindern von Overfitting durch Hinzuf\u00fcgen von Straftermen in die Optimierungsfunktion.</li> <li>Beispiele: L1-Regularisierung (Lasso), L2-Regularisierung (Ridge).</li> </ul>"},{"location":"KW49-50/overview/#4-optimierungsprobleme-im-kontext-von-data-science","title":"4. Optimierungsprobleme im Kontext von Data Science","text":"<ul> <li>Lineare Programmierung: Optimierung einer linearen Zielfunktion unter linearen Nebenbedingungen.</li> <li>Nichtlineare Optimierung: Behandlung komplexerer Probleme, bei denen Zielfunktion oder Nebenbedingungen nicht linear sind.</li> <li>Kombinatorische Optimierung: L\u00f6sung diskreter Probleme, wie z. B. die Auswahl der besten Feature-Kombination.</li> </ul> <p>Vor einigen Jahren wurde unter dem Begriff \"Data Science\" nahezu alles zusammengefasst, was mit Daten zu tun hatte. Heute hat sich das Feld weiterentwickelt und spezialisiert, wobei verschiedene Rollen und Aufgabenbereiche entstanden sind. Im Folgenden werden einige wichtige Rollen in Data Science vorgestellt, die in Unternehmen und Organisationen eine zentrale Rolle spielen.</p>"},{"location":"KW49-50/overview/#wichtige-rollen-in-data-science","title":"Wichtige Rollen in Data Science","text":""},{"location":"KW49-50/overview/#data-scientist","title":"Data Scientist","text":"<ul> <li>Verantwortlich f\u00fcr die Analyse und Modellierung der Daten.</li> <li>Kombiniert Statistik, Programmierung und Dom\u00e4nenwissen.</li> </ul>"},{"location":"KW49-50/overview/#data-engineer","title":"Data Engineer","text":"<ul> <li>Entwickelt und pflegt Datenpipelines und Infrastruktur.</li> <li>Sicherstellung, dass Daten effizient und skalierbar verarbeitet werden.</li> </ul>"},{"location":"KW49-50/overview/#data-analyst","title":"Data Analyst","text":"<ul> <li>Fokus auf Datenvisualisierung und Berichterstattung.</li> <li>Ziel: Gesch\u00e4ftsrelevante Erkenntnisse kommunizieren.</li> </ul>"},{"location":"KW49-50/overview/#machine-learning-engineer","title":"Machine Learning Engineer","text":"<ul> <li>Spezialisiert auf die Entwicklung und Implementierung von ML-Modellen.</li> <li>Arbeitet eng mit Data Scientists zusammen, um Modelle in Produktionsumgebungen zu \u00fcberf\u00fchren.</li> </ul>"},{"location":"KW49-50/overview/#business-analyst","title":"Business Analyst","text":"<ul> <li>\u00dcbersetzt Gesch\u00e4ftsanforderungen in datenbasierte L\u00f6sungen.</li> <li>Fokussiert auf die Anwendung von Daten zur Optimierung von Gesch\u00e4ftsprozessen.</li> </ul>"},{"location":"KW49-50/overview/#data-architect","title":"Data Architect","text":"<ul> <li>Entwirft und implementiert Datenbanken und -architekturen.</li> <li>Sicherstellung, dass Daten effizient gespeichert und abgerufen werden k\u00f6nnen.</li> </ul> <p>In der Umgangssprache werden datengbasierte Projekte und teilweise sogar Software Projekte einfach als KI Projekte bezeichnet. In der Realit\u00e4t ist KI jedoch nur ein Teilbereich der Data Science. In diesem Abschnitt werden die Unterschiede und Beziehungen zwischen KI, Maschinellem Lernen, Deep Learning und Statistik erl\u00e4utert.</p>"},{"location":"KW49-50/overview/#begriffe-aus-dem-data-science-kosmos","title":"Begriffe aus dem Data Science Kosmos","text":"<p>In der Welt von Data Science gibt es viele Begriffe, die oft synonym oder missverst\u00e4ndlich verwendet werden. Wir f\u00fchren hier einige der wichtigsten Begriffe auf und erkl\u00e4ren ihre Bedeutung und Beziehung zueinander. Die Berufsfelder aus dem obigen Abschnitt werden dabei nicht wiederholend aufgef\u00fchrt.</p> <ul> <li>K\u00fcnstliche Intelligenz (KI): Der \u00dcberbegriff f\u00fcr Technologien, die menschen\u00e4hnliches Denken und Handeln nachahmen.</li> <li>Maschinelles Lernen (ML): Ein Teilbereich der KI, der sich auf Algorithmen konzentriert, die aus Daten lernen k\u00f6nnen.</li> <li>Deep Learning: Ein Teilbereich des ML, der neuronale Netze mit vielen Schichten verwendet.</li> <li>Statistik: Fundamentale Methoden zur Analyse und Interpretation von Daten.</li> <li>Data Mining: Extraktion von Mustern und Informationen aus gro\u00dfen Datenmengen.</li> <li>Big Data: Verarbeitung und Analyse gro\u00dfer Datenmengen, die mit traditionellen Methoden nicht m\u00f6glich w\u00e4ren.</li> <li>Business Intelligence (BI): Technologien und Anwendungen zur Analyse von Gesch\u00e4ftsdaten.</li> </ul> <p>Wir werden uns in den n\u00e4chsten Abschnitten mit der Datenverarbeitung besch\u00e4ftigen. Genauer werden wir auf Feature Enigneering eingehen. Feature Engineering ist ein wichtiger Schritt im Data-Science-Prozess, bei dem neue, aussagekr\u00e4ftige Variablen aus vorhandenen Daten erstellt werden. Dieser Prozess erm\u00f6glicht es, relevante Informationen zu extrahieren und in einer Form bereitzustellen, die von Algorithmen optimal genutzt werden kann.</p>"},{"location":"KW49-50/data_processing/duplicates/","title":"Umgang mit Duplikaten in Daten","text":"<p>In der Datenbereinigung stellen Duplikate ein h\u00e4ufiges Problem dar. Duplikate sind Eintr\u00e4ge in einem Datensatz, die identisch oder nahezu identisch sind und die Analyse verf\u00e4lschen k\u00f6nnen. Ein bewusster und systematischer Umgang mit Duplikaten ist entscheidend, um valide Ergebnisse zu erzielen.</p>"},{"location":"KW49-50/data_processing/duplicates/#warum-sind-duplikate-problematisch","title":"Warum sind Duplikate problematisch?","text":"<p>Duplikate k\u00f6nnen aus verschiedenen Gr\u00fcnden auftreten:</p> <ul> <li>Fehlerhafte Datenintegration: Beim Zusammenf\u00fchren von Daten aus unterschiedlichen Quellen entstehen oft redundante Eintr\u00e4ge.</li> <li>Mehrfache Datenerfassung: Beispielsweise k\u00f6nnen Kunden ein Formular mehrfach ausgef\u00fcllt haben.</li> <li>Automatisierte Prozesse: Systeme, die ohne ausreichende Kontrollmechanismen Daten generieren, k\u00f6nnen unbeabsichtigt doppelte Werte erzeugen.</li> </ul> <p>Konsequenzen von Duplikaten sind unter anderem:</p> <ul> <li>Verzerrte Analysen durch \u00fcberrepr\u00e4sentierte Werte.</li> <li>Ineffiziente Speicher- und Rechenressourcen.</li> <li>Probleme bei Modellen, die davon ausgehen, dass jeder Datensatz einzigartig ist.</li> </ul>"},{"location":"KW49-50/data_processing/duplicates/#arten-von-duplikaten","title":"Arten von Duplikaten","text":"<p>Es gibt verschiedene Arten von Duplikaten, die je nach Kontext unterschiedlich behandelt werden m\u00fcssen:</p> <p>Exakte Duplikate    - Diese Eintr\u00e4ge stimmen in allen Feldern \u00fcberein.    - Beispiel:      | ID  | Name       | Alter | Ort    |      | --- | ---------- | ----- | ------ |      | 001 | Anna Meier | 29    | Berlin |      | 001 | Anna Meier | 29    | Berlin |</p> <p>Nahezu Duplikate    - Diese Eintr\u00e4ge unterscheiden sich geringf\u00fcgig, z. B. durch Tippfehler oder unterschiedliche Schreibweisen.    - Beispiel:      | ID  | Name          | Alter | Ort     |      | --- | ------------- | ----- | ------- |      | 002 | Thomas Schmid | 35    | Hamburg |      | 003 | T. Schmid     | 35    | Hamburg |</p>"},{"location":"KW49-50/data_processing/duplicates/#vorgehen-zur-identifikation-und-behandlung-von-duplikaten","title":"Vorgehen zur Identifikation und Behandlung von Duplikaten","text":""},{"location":"KW49-50/data_processing/duplicates/#identifikation-von-duplikaten","title":"Identifikation von Duplikaten","text":"<p>Die Identifikation ist der erste Schritt und kann je nach Art der Duplikate unterschiedlich erfolgen:</p> <ul> <li>Exakte Duplikate finden:</li> <li> <p>In Python k\u00f6nnen exakte Duplikate mit Pandas identifiziert werden:     ```python     import pandas as pd</p> </li> <li> <p>Nahezu Duplikate finden:</p> </li> <li> <p>Hier helfen Ans\u00e4tze wie Fuzzy Matching, beispielsweise mit der Bibliothek <code>fuzzywuzzy</code> oder \u00e4hnlichen Tools:     ```python     from fuzzywuzzy import fuzz     from fuzzywuzzy import process</p> </li> </ul>"},{"location":"KW49-50/data_processing/duplicates/#beispiel-datensatz","title":"Beispiel-Datensatz","text":"<p>data = {     'ID': [1, 2, 3, 1],     'Name': ['Anna Meier', 'Thomas Schmid', 'Maria Kurz', 'Anna Meier'],     'Alter': [29, 35, 40, 29] } df = pd.DataFrame(data)</p>"},{"location":"KW49-50/data_processing/duplicates/#duplikate-finden","title":"Duplikate finden","text":"<p>duplicates = df[df.duplicated()] print(duplicates) ```</p>"},{"location":"KW49-50/data_processing/duplicates/#beispiel-vergleich-zweier-namen","title":"Beispiel: Vergleich zweier Namen","text":"<p>score = fuzz.ratio(\"Thomas Schmid\", \"T. Schmid\") print(score)  # Ausgabe: \u00dcber 80%, also wahrscheinlich Duplikat ```</p>"},{"location":"KW49-50/data_processing/duplicates/#behandlung-von-duplikaten","title":"Behandlung von Duplikaten","text":""},{"location":"KW49-50/data_processing/duplicates/#exakte-duplikate-entfernen","title":"Exakte Duplikate entfernen","text":"<ul> <li>Einfaches Entfernen: <code>python   # Entfernen von exakten Duplikaten   df_cleaned = df.drop_duplicates()</code></li> <li>Erhalt bestimmter Eintr\u00e4ge: Wenn du die erste oder letzte Instanz eines Duplikats behalten m\u00f6chtest, kannst du dies steuern:   <code>python   df_cleaned = df.drop_duplicates(keep='first')  # Behalte den ersten Eintrag</code></li> </ul>"},{"location":"KW49-50/data_processing/duplicates/#nahezu-duplikate-bereinigen","title":"Nahezu Duplikate bereinigen","text":"<ul> <li>Manuelle Pr\u00fcfung: Besonders bei kleineren Datens\u00e4tzen lohnt sich die manuelle Sichtung.</li> <li>Automatisierte Verfahren:</li> <li>Berechnung von \u00c4hnlichkeitsscores.</li> <li>Erstellung von Regeln zur Vereinheitlichung (z. B. Namenskonventionen).</li> </ul>"},{"location":"KW49-50/data_processing/duplicates/#zusammenfuhren-von-duplikaten","title":"Zusammenf\u00fchren von Duplikaten","text":"<ul> <li>Wenn Informationen in den Duplikaten variieren, kannst du diese zusammenf\u00fchren:   <code>python   # Gruppieren und Aggregieren   df_grouped = df.groupby('ID').agg({       'Name': 'first',       'Alter': 'mean'   })</code></li> </ul>"},{"location":"KW49-50/data_processing/duplicates/#beispiele-aus-der-praxis","title":"Beispiele aus der Praxis","text":"<ul> <li> <p>Kundenlisten bereinigen:   Ein Unternehmen hat mehrere Listen von Kunden aus verschiedenen Regionen zusammengef\u00fchrt. Dabei entstehen oft exakte und nahe Duplikate, die manuell oder mit Matching-Algorithmen behandelt werden m\u00fcssen.</p> </li> <li> <p>Web-Scraping-Daten:   Beim Scraping von Websites entstehen oft doppelte Eintr\u00e4ge, wenn dieselbe Seite mehrfach abgefragt wird. Hier hilft die Verwendung einer eindeutigen ID oder eines Hashes, um Duplikate zu identifizieren.</p> </li> </ul>"},{"location":"KW49-50/data_processing/feature_enigneerin_ideas/","title":"Feature enigneerin ideas","text":"<ol> <li>Feature-Auswahl:</li> <li>Korrelationsanalyse: Untersuche, welche Features stark miteinander korrelieren und ob es sinnvoll ist, einige Features zu entfernen, um Multikollinearit\u00e4t zu vermeiden.</li> <li> <p>Spearman/Kendall/Korrelation: Wenn du mit nicht-linearen Beziehungen arbeitest, k\u00f6nnen diese Methoden ebenfalls n\u00fctzlich sein.</p> </li> <li> <p>Feature-Transformation:</p> </li> <li>Skalierung: Normalisierung (Min-Max) oder Standardisierung (Z-Score) von Features, um sie auf denselben Ma\u00dfstab zu bringen, besonders f\u00fcr Modelle, die auf Distanzmessungen basieren (z.B. SVM oder k-NN).</li> <li> <p>Log-Transformation: Wenn Features starke Schiefe aufweisen, kann eine Log-Transformation helfen, die Verteilung zu stabilisieren.</p> </li> <li> <p>Feature-Creation:</p> </li> <li>Kombination von Features: Erstelle neue Features durch Kombinationen bestehender (z.B. Multiplikation, Addition oder Differenz von numerischen Variablen).</li> <li>Zeitmerkmale: Wenn du mit Zeitstempeln arbeitest, extrahiere Merkmale wie Wochentag, Monat, Jahr, Stunden, Feiertage etc.</li> <li> <p>Kategorische Merkmale: Wandeln von kategorischen Variablen in numerische Werte (z.B. mit One-Hot-Encoding, Label-Encoding oder Target-Encoding).</p> </li> <li> <p>Handling von fehlenden Werten:</p> </li> <li>Imputation: Fehlende Werte mit dem Mittelwert, Median oder einem Vorhersagemodell auff\u00fcllen.</li> <li> <p>L\u00f6schen: Entferne Zeilen oder Spalten mit zu vielen fehlenden Werten, wenn die Imputation nicht sinnvoll erscheint.</p> </li> <li> <p>Feature-Engineering f\u00fcr Textdaten:</p> </li> <li>Bag of Words oder TF-IDF: Um Textdaten in eine numerische Form zu bringen.</li> <li> <p>Word Embeddings: Verwendung vortrainierter Vektoren wie Word2Vec oder GloVe, um semantische Beziehungen zu erfassen.</p> </li> <li> <p>Feature-Engineering f\u00fcr kategorische Daten:</p> </li> <li>One-Hot-Encoding: F\u00fcr nominale Kategorien, um diese in eine bin\u00e4re Form zu bringen.</li> <li> <p>Ordinal-Encoding: Wenn die Kategorien eine nat\u00fcrliche Reihenfolge haben (z.B. klein, mittel, gro\u00df).</p> </li> <li> <p>Dimensionalit\u00e4tsreduktion:</p> </li> <li>PCA (Principal Component Analysis): Reduziere die Anzahl der Features bei gleichzeitiger Beibehaltung der gr\u00f6\u00dften Varianz im Datensatz.</li> <li>t-SNE oder UMAP: Zur Visualisierung und ggf. als Vorprozessierung bei komplexen Datens\u00e4tzen.</li> </ol>"},{"location":"KW49-50/data_processing/intro/","title":"Datenvorverareitung","text":"<p>Einer der wichtigsten Schritte in der Data Science ist das bearbeiten und aufbereiten von Daten. Unsere Modelle sind nur so gut wie die Daten, die wir ihnen geben. Daher wird die Qualit\u00e4t unser Modelle ma\u00dfgeblich von der Qualit\u00e4t unser Datenverarbeitung beeinflusst.</p> <p>Die Datenverarbeitung wird in verschiedene Untergebiete aufgeteilt. Zu diesen z\u00e4hlen unter anderem das Data Cleaning, Preprocessing und Feature Engineering.</p>"},{"location":"KW49-50/data_processing/intro/#data-cleaning","title":"Data Cleaning","text":"<p>Data Cleaning ist der Prozess der Bereinigung von Daten, um deren Qualit\u00e4t und Konsistenz zu verbessern. Dies ist ein entscheidender Schritt, da unvollst\u00e4ndige oder fehlerhafte Daten die Leistung von Modellen erheblich beeintr\u00e4chtigen k\u00f6nnen. </p> <ul> <li>Fokus: Bereinigung der Daten.</li> <li>Beispiele:<ul> <li>Umgang mit fehlenden Werten: Fehlende Daten k\u00f6nnen durch Imputation (z. B. Mittelwert, Median) oder durch Entfernen der betroffenen Eintr\u00e4ge behandelt werden.</li> <li>Entfernung von Duplikaten: Doppelte Eintr\u00e4ge in den Daten k\u00f6nnen zu Verzerrungen f\u00fchren und sollten identifiziert und entfernt werden.</li> <li>Korrektur von Inkonsistenzen: Unterschiedliche Formate oder Schreibweisen (z. B. Datumsformate) sollten vereinheitlicht werden.</li> </ul> </li> </ul>"},{"location":"KW49-50/data_processing/intro/#preprocessing","title":"Preprocessing","text":"<p>Preprocessing umfasst alle Schritte, die notwendig sind, um Rohdaten in ein Format zu bringen, das f\u00fcr die Modellierung geeignet ist. Dies beinhaltet die Transformation und Normalisierung der Daten, um sicherzustellen, dass sie von Algorithmen effizient verarbeitet werden k\u00f6nnen.</p> <ul> <li>Fokus: Vorbereitung der Daten f\u00fcr Modelle.</li> <li>Beispiele:<ul> <li>Skalieren von Werten: Daten k\u00f6nnen auf einen bestimmten Bereich (z. B. 0 bis 1) skaliert werden, um die Leistung von Algorithmen zu verbessern.</li> <li>Normalisierung: Anpassung der Datenverteilung, z. B. durch Z-Score-Normalisierung, um die Vergleichbarkeit der Variablen zu gew\u00e4hrleisten.</li> </ul> </li> </ul>"},{"location":"KW49-50/data_processing/intro/#feature-engineering","title":"Feature Engineering","text":"<p>Feature Engineering ist der  Prozess der Erstellung neuer, relevanter Features aus den vorhandenen Daten. Ziel ist es, die Leistungsf\u00e4higkeit der Modelle zu steigern, indem zus\u00e4tzliche Informationen extrahiert und bereitgestellt werden. Das Feature Engineering baut auf den vorherigen Schritten auf und erm\u00f6glicht es, die Daten optimal f\u00fcr die Modellierung vorzubereiten.</p> <ul> <li>Fokus: Erstellen neuer, relevanter Features.</li> <li>Beispiele:<ul> <li>Berechnung einer neuen Variable: Erstellen neuer Features durch Kombination oder Transformation bestehender Daten, z. B. das Verh\u00e4ltnis von zwei Variablen.</li> <li>Zeit- und Ortsbezug: Extrahieren von Informationen wie Wochentagen, Uhrzeiten oder geografischen Regionen aus Datums- oder Standortdaten.</li> <li>Transformation bestehender Daten: Anwenden von mathematischen Transformationen wie Logarithmieren oder Skalieren, um die Datenverteilung zu verbessern.</li> <li>Kodierung kategorialer Daten: Umwandlung von Kategorien in numerische Werte, um sie f\u00fcr Algorithmen nutzbar zu machen.</li> </ul> </li> </ul> <p>Feature Engineering erfordert sowohl technisches Wissen als auch ein tiefes Verst\u00e4ndnis der Daten und des Anwendungsbereichs, um die relevantesten und aussagekr\u00e4ftigsten Features zu identifizieren und zu erstellen.</p>"},{"location":"KW49-50/data_processing/intro/#7-einfuhrung-in-feature-engineering","title":"7. Einf\u00fchrung in Feature Engineering","text":"<p>Feature Engineering ist ein essenzieller Schritt im Data-Science-Prozess, der den \u00dcbergang von Rohdaten zu aussagekr\u00e4ftigen Variablen erm\u00f6glicht. Dabei geht es darum, die relevanten Informationen aus den Daten zu extrahieren und in einer Form bereitzustellen, die Algorithmen optimal nutzen k\u00f6nnen.</p>"},{"location":"KW49-50/data_processing/intro/#einordnung-von-feature-engineering","title":"Einordnung von Feature Engineering","text":"<p>Feature Engineering geh\u00f6rt in den Bereich der Datenvorbereitung, grenzt sich jedoch durch den Fokus auf die Erstellung neuer Variablen ab. W\u00e4hrend Data Cleaning und Preprocessing die Qualit\u00e4t und Struktur der Daten verbessern, zielt Feature Engineering darauf ab, Informationen f\u00fcr Modelle nutzbar zu machen.</p>"},{"location":"KW49-50/data_processing/intro/#warum-ist-feature-engineering-wichtig","title":"Warum ist Feature Engineering wichtig?","text":"<ul> <li>Verbesserte Modellleistung: Gut gestaltete Features erh\u00f6hen die Genauigkeit und Robustheit von Modellen.</li> <li>Dom\u00e4nenwissen einbinden: Durch spezifisches Wissen \u00fcber den Anwendungsbereich k\u00f6nnen wichtige Zusammenh\u00e4nge besser dargestellt werden.</li> <li>Reduktion von Komplexit\u00e4t: Feature Engineering kann helfen, irrelevante Informationen zu entfernen und die Analyse zu fokussieren.</li> </ul>"},{"location":"KW49-50/data_processing/intro/#methoden-des-feature-engineering","title":"Methoden des Feature Engineering","text":"<ol> <li>Transformation bestehender Daten: Logarithmieren, Skalieren oder Normalisieren von Variablen.</li> <li>Kombination von Variablen: Erstellen neuer Features durch Kombination bestehender Variablen (z. B. Verh\u00e4ltnisse oder Differenzen).</li> <li>Ableitung von Zeit- oder Ortsbezug: Extrahieren von Wochentagen, Uhrzeiten oder geografischen Regionen aus Datums- oder Standortinformationen.</li> <li>Kodierung kategorialer Daten: Umwandlung von Kategorien in numerische Werte (z. B. One-Hot-Encoding).</li> </ol> <p>Im Nachfolgenden Abschnitt werden wir uns zun\u00e4chst mit Methoden des Data Cleaning und Preprocessing besch\u00e4ftigen, bevor wir tiefer in die Methoden des Feature Engineering eintauchen.</p>"},{"location":"KW49-50/data_processing/missing_values/","title":"Missing Values","text":"<p>Unsere Datens\u00e4tze sind in den wengisten F\u00e4llen perfekt vollst\u00e4ndig. Beispielsweise kann es bei der Erfassung von Personaldaten vorkommen, dass nicht alle Felder ausgef\u00fcllt wurden. Diese fehlenden Werte k\u00f6nnen die Analyse und Modellierung beeintr\u00e4chtigen. Daher ist es wichtig, diese L\u00fccken zu identifizeren und zu behandeln.</p>"},{"location":"KW49-50/data_processing/missing_values/#umgang-mit-fehlenden-werten","title":"Umgang mit fehlenden Werten","text":"<p>Es gibt verschiedene Strategien, um fehlende Werte zu behandeln. Die Wahl der Methode h\u00e4ngt von der Art der Daten und dem Kontext ab. Hier sind einige g\u00e4ngige Ans\u00e4tze:</p>"},{"location":"KW49-50/data_processing/missing_values/#1-entfernen-von-fehlenden-werten","title":"1. Entfernen von fehlenden Werten","text":"<p>Eine einfache Methode besteht darin, alle Zeilen oder Spalten zu entfernen, die fehlende Werte enthalten. Dies kann jedoch zu einem Informationsverlust f\u00fchren, insbesondere wenn viele Daten fehlen. Generell sollten wir beim Entfernen der Daten uns vorher ansehen, ob die Daten zuf\u00e4llig fehlen oder ob es ein Muster gibt. Machmal kann es auch vorkommen, dass unser Datensatz durch das Entfernen von Daten verzerrt oder zu klein wird.</p>"},{"location":"KW49-50/data_processing/missing_values/#2-imputation-von-fehlenden-werten","title":"2. Imputation von fehlenden Werten","text":"<p>Eine h\u00e4ufigere Methode ist die Imputation, bei der fehlende Werte durch Sch\u00e4tzungen basierend auf den verbleibenden Daten ersetzt werden. Dies kann durch den Durchschnitt, den Median oder durch Vorhersagemodelle erfolgen. Die Wahl der Methode h\u00e4ngt von der Art der Daten und dem Kontext ab. Vor der Wahl einer der Methoden f\u00fcr die Imputation m\u00fcssen wir auch hier ein Grundverst\u00e4ndnis von unseren Daten haben. </p> <p>Beobachte wir beispielsweise Wetterdaten und es Fehlt die Temperatur an einem Tag, dann k\u00f6nnten wir die Temperatur des Vortages als Sch\u00e4tzung verwenden. Auch die Verwendung eines Mittelweertes der anliegenden Tage k\u00f6nnte eine M\u00f6glichkeit sein. Haben wir jedoch ungeordnete Daten, wie beispielsweise Namen, dann macht es keinen Sinn, den Durchschnitt f\u00fcr die Spalte Alter als Ersatz f\u00fcr Missing Values zu berechnen.</p> <p>Je nach Art der Daten und des Problems ist bei der Imputation Kreativit\u00e4t und Fachwissen gefragt.</p>"},{"location":"KW49-50/data_processing/missing_values/#3-vorhersagemodelle-fur-die-imputation","title":"3. Vorhersagemodelle f\u00fcr die Imputation","text":"<p>In einigen F\u00e4llen kann es sinnvoll sein, Vorhersagemodelle zu verwenden, um fehlende Werte zu sch\u00e4tzen. Dies kann insbesondere dann n\u00fctzlich sein, wenn die fehlenden Werte nicht zuf\u00e4llig sind und ein Muster aufweisen. Beispielsweise k\u00f6nnten wir ein Regressionsmodell trainieren, um fehlende Werte zu sch\u00e4tzen, basierend auf anderen verf\u00fcgbaren Variablen.</p>"},{"location":"KW49-50/data_processing/missing_values/#4-kategorisierung-von-fehlenden-werten","title":"4. Kategorisierung von fehlenden Werten","text":"<p>Manchmal kann es sinnvoll sein, fehlende Werte als separate Kategorie zu behandeln, anstatt sie zu sch\u00e4tzen oder zu entfernen. Dies kann insbesondere dann n\u00fctzlich sein, wenn das Fehlen von Werten eine Bedeutung hat und Informationen enth\u00e4lt. Beispielsweise kann das fehlen eines Wertes bei der Anwesenheit eines Patienten in einem Krankenhaus darauf hinweisen, dass der Patient nicht anwesend war.</p>"},{"location":"KW49-50/data_processing/outliers/","title":"Umgang mit Ausrei\u00dfern","text":"<p>Ausrei\u00dfer sind Datenpunkte, die erheblich von anderen Beobachtungen in einem Datensatz abweichen. Sie k\u00f6nnen wertvolle Informationen enthalten oder die Analyse st\u00f6ren, weshalb es wichtig ist, sie zu erkennen und sinnvoll zu behandeln.</p>"},{"location":"KW49-50/data_processing/outliers/#warum-sind-ausreier-wichtig","title":"Warum sind Ausrei\u00dfer wichtig?","text":"<p>Ausrei\u00dfer k\u00f6nnen sowohl negative als auch positive Auswirkungen auf die Datenanalyse haben. Negative Effekte umfassen die Verzerrung statistischer Kennzahlen wie Mittelwert und Standardabweichung, was zu fehlerhaften Schlussfolgerungen f\u00fchren kann. Auf der anderen Seite k\u00f6nnen Ausrei\u00dfer auch positive Effekte haben, indem sie auf seltene, aber wichtige Ereignisse hinweisen, wie zum Beispiel Betrug, Fehler oder extreme Ph\u00e4nomene.</p> <p>Ein Beispiel f\u00fcr einen negativen Effekt w\u00e4re ein Monatsgehalt von 1.000.000 \u20ac in einem Datensatz von Durchschnittsgeh\u00e4ltern, das den Mittelwert stark nach oben verzerren w\u00fcrde. Ein Beispiel f\u00fcr einen positiven Effekt w\u00e4re eine Temperaturmessung von 80 \u00b0C in einer Klimadatenreihe, die auf ein au\u00dfergew\u00f6hnliches Wetterereignis hinweisen k\u00f6nnte.</p>"},{"location":"KW49-50/data_processing/outliers/#erkennung-von-ausreiern","title":"Erkennung von Ausrei\u00dfern","text":""},{"location":"KW49-50/data_processing/outliers/#visuelle-methoden","title":"Visuelle Methoden","text":"<p>Visuelle Methoden zur Erkennung von Ausrei\u00dfern umfassen Boxplots und Scatterplots. Boxplots zeigen potenzielle Ausrei\u00dfer oberhalb oder unterhalb der \"Whisker\". Um einen Boxplot f\u00fcr die Variable \"Preis\" in einem Datensatz zu erstellen, kann folgender Python-Code verwendet werden:</p> <pre><code>import matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.boxplot(x=df['Preis'])\nplt.show()\n</code></pre> <p>Scatterplots hingegen zeigen Ausrei\u00dfer in zweidimensionalen Daten. Um \"Gr\u00f6\u00dfe\" gegen \"Preis\" zu plotten und auff\u00e4llige Punkte zu identifizieren, kann folgender Python-Code verwendet werden:</p> <pre><code>plt.scatter(df['Gr\u00f6\u00dfe'], df['Preis'])\nplt.xlabel('Gr\u00f6\u00dfe')\nplt.ylabel('Preis')\nplt.show()\n</code></pre>"},{"location":"KW49-50/data_processing/outliers/#statistische-methoden","title":"Statistische Methoden","text":"<p>Die IQR-Methode (Interquartilsabstand) ist eine g\u00e4ngige Methode zur Identifizierung von Ausrei\u00dfern. Werte, die au\u00dferhalb des Bereichs [Q1 - 1,5 * IQR, Q3 + 1,5 * IQR] liegen, gelten als Ausrei\u00dfer. Um die IQR-Methode zu implementieren und die Ausrei\u00dfer aufzulisten, kann folgender Python-Code verwendet werden:</p> <pre><code>Q1 = df['Preis'].quantile(0.25)\nQ3 = df['Preis'].quantile(0.75)\nIQR = Q3 - Q1\n\nlower_bound = Q1 - 1.5 * IQR\nupper_bound = Q3 + 1.5 * IQR\n\nausreisser = df[(df['Preis'] &lt; lower_bound) | (df['Preis'] &gt; upper_bound)]\nprint(ausreisser)\n</code></pre> <ul> <li>Z-Score:  Der Z-Score ist ein statistisches Ma\u00df, das angibt, wie viele Standardabweichungen ein Datenpunkt vom Mittelwert entfernt ist. Werte mit einem absoluten Z-Score gr\u00f6\u00dfer als 3 werden oft als Ausrei\u00dfer betrachtet.</li> </ul> <p>Um den Z-Score f\u00fcr alle Werte zu berechnen und die Ausrei\u00dfer zu finden, kann folgender Python-Code verwendet werden:</p> <pre><code>```python\nfrom scipy.stats import zscore\ndf['z_score'] = zscore(df['Preis'])\nausreisser = df[abs(df['z_score']) &gt; 3]\nprint(ausreisser)\n```\n</code></pre> <p>In diesem Code wird die Funktion zscore aus dem Modul scipy.stats verwendet, um den Z-Score der Spalte Preis in einem DataFrame df zu berechnen. Anschlie\u00dfend werden alle Zeilen, deren absoluter Z-Score gr\u00f6\u00dfer als 3 ist, als Ausrei\u00dfer identifiziert und ausgegeben.</p>"},{"location":"KW49-50/data_processing/outliers/#umgang-mit-ausreiern_1","title":"Umgang mit Ausrei\u00dfern","text":""},{"location":"KW49-50/data_processing/outliers/#entfernen-von-ausreiern","title":"Entfernen von Ausrei\u00dfern","text":"<p>Die IQR-Methode (Interquartilsabstand) ist eine g\u00e4ngige statistische Methode zur Identifizierung und Entfernung von Ausrei\u00dfern in einem Datensatz. Der Interquartilsabstand ist der Bereich zwischen dem ersten Quartil (Q1) und dem dritten Quartil (Q3) eines Datensatzes. Ausrei\u00dfer sind Werte, die entweder unterhalb von Q1 - 1.5 * IQR oder oberhalb von Q3 + 1.5 * IQR liegen.</p> <p>Um problematische Werte zu entfernen, wenn sie Fehler oder nicht relevant sind, kann die IQR-Methode verwendet werden. Zuerst werden die Quartile Q1 und Q3 berechnet, dann wird der IQR als die Differenz zwischen Q3 und Q1 bestimmt. Schlie\u00dflich werden die unteren und oberen Grenzen festgelegt, um Ausrei\u00dfer zu identifizieren und zu entfernen. Der bereinigte Datensatz enth\u00e4lt nur die Werte, die innerhalb dieser Grenzen liegen.    <code>python     Q1 = df['Preis'].quantile(0.25)     Q3 = df['Preis'].quantile(0.75)     IQR = Q3 - Q1     lower_bound = Q1 - 1.5 * IQR     upper_bound = Q3 + 1.5 * IQR</code> df_cleaned = df[(df['Preis'] &gt;= lower_bound) &amp; (df['Preis'] &lt;= upper_bound)]</p> <p>In diesem Beispiel wird der bereinigte Datensatz df_cleaned erstellt, indem alle Ausrei\u00dfer basierend auf der IQR-Methode entfernt werden.</p>"},{"location":"KW49-50/data_processing/outliers/#transformation-von-daten","title":"Transformation von Daten","text":"<p>Die Log-Transformation wird verwendet, um extreme Werte zu gl\u00e4tten. Dies geschieht, indem die Werte einer Variable mit der Logarithmusfunktion transformiert werden. Hier ist ein Beispiel in Python:     <code>python     df['Preis_log'] = np.log1p(df['Preis'])</code> Die Log-Transformation reduziert die Skala der Daten, insbesondere bei gro\u00dfen Werten. Dies liegt daran, dass der Logarithmus einer Zahl langsamer w\u00e4chst als die Zahl selbst. Dadurch werden gro\u00dfe Werte komprimiert und die Verteilung der Daten wird symmetrischer. Dies kann hilfreich sein, um die Auswirkungen von Ausrei\u00dfern zu minimieren und die Datenanalyse zu verbessern.</p>"},{"location":"KW49-50/data_processing/outliers/#winsorizing","title":"Winsorizing","text":"<p>Um Extremwerte zu begrenzen, k\u00f6nnen wir die Werte auf vordefinierte Schwellen beschr\u00e4nken. In dieser Aufgabe sollen die Preise auf das 5. und 95. Perzentil begrenzt werden.</p> <p>Perzentile sind Ma\u00dfeinheiten, die die Verteilung von Daten in 100 gleiche Teile unterteilen. Das 5. Perzentil bedeutet, dass 5% der Daten unter diesem Wert liegen, w\u00e4hrend das 95. Perzentil bedeutet, dass 95% der Daten unter diesem Wert liegen.</p> <p>Im folgenden Python-Code wird dies umgesetzt:     ```python     lower_percentile = df['Preis'].quantile(0.05)     upper_percentile = df['Preis'].quantile(0.95)</p> <pre><code>df['Preis_winsorized'] = np.clip(df['Preis'], lower_percentile, upper_percentile)\n```\n</code></pre> <p>Hier berechnen wir zun\u00e4chst das 5. und 95. Perzentil der Preisdaten. Anschlie\u00dfend verwenden wir die np.clip-Funktion, um die Preise auf diese Perzentile zu begrenzen. Dies bedeutet, dass alle Preise, die unter dem 5. Perzentil liegen, auf das 5. Perzentil gesetzt werden, und alle Preise, die \u00fcber dem 95. Perzentil liegen, auf das 95. Perzentil gesetzt werden.</p>"},{"location":"KW49-50/data_processing/outliers/#beibehalten-von-ausreiern","title":"Beibehalten von Ausrei\u00dfern","text":"<p>In manchen F\u00e4llen sollten Ausrei\u00dfer bewusst beibehalten werden, insbesondere wenn sie f\u00fcr das Problem relevant sind, wie zum Beispiel bei der Betrugserkennung.</p>"},{"location":"KW49-50/data_processing/outliers/#best-practices","title":"Best Practices","text":"<ul> <li>Analysiere den Ursprung: Pr\u00fcfe, ob der Ausrei\u00dfer ein Messfehler, ein Dateneingabefehler oder ein echtes Signal ist.</li> <li>Dom\u00e4nenwissen einbeziehen: Besprich Auff\u00e4lligkeiten mit Experten aus dem jeweiligen Bereich.</li> <li>Dokumentiere deine Entscheidungen: Notiere, welche Schritte unternommen wurden und warum.</li> </ul>"},{"location":"KW49-50/data_processing/type_conversion/","title":"Typenkonvertierungen (Wird noch behandelt)","text":"<p>Typenkonvertierungen sind ein essenzieller Bestandteil des Data Cleanings. H\u00e4ufig stammen Daten aus unterschiedlichen Quellen, was zu inkonsistenten Datentypen f\u00fchren kann. Beispielsweise kann ein numerischer Wert als Text gespeichert sein oder ein Datum in einem un\u00fcblichen Format vorliegen. Um Analysen durchf\u00fchren zu k\u00f6nnen, m\u00fcssen solche Werte in geeignete Datentypen konvertiert werden.</p>"},{"location":"KW49-50/data_processing/type_conversion/#warum-typenkonvertierungen-wichtig-sind","title":"Warum Typenkonvertierungen wichtig sind","text":"<ul> <li>Korrekte Verarbeitung: Viele Analyse- und Modellierungsmethoden erfordern spezifische Datentypen (z. B. numerische Werte f\u00fcr Berechnungen).</li> <li>Effizienz: Die Arbeit mit korrekt typisierten Daten ist meist schneller und ressourcenschonender.</li> <li>Fehlervermeidung: Inkompatible Typen k\u00f6nnen zu Laufzeitfehlern oder falschen Ergebnissen f\u00fchren.</li> </ul>"},{"location":"KW49-50/data_processing/type_conversion/#typische-herausforderungen","title":"Typische Herausforderungen","text":"<p>Numerische Werte als Text:    Daten wie \"1000\" oder \"3,14\" werden oft als Zeichenketten gespeichert, m\u00fcssen aber f\u00fcr Berechnungen in numerische Werte konvertiert werden.</p> <p>Datumsangaben:    Formate wie \"12/31/2024\" oder \"31-12-2024\" erfordern eine einheitliche Konvertierung in ein Datumsobjekt.</p> <p>Kategorische Daten:    Kategorien wie \"Ja\" und \"Nein\" oder \"True\" und \"False\" sollten in standardisierte Werte umgewandelt werden.</p> <p>Fehlende Werte:    Leere Felder oder Platzhalter wie \"N/A\" oder \"-\" m\u00fcssen erkannt und behandelt werden.</p>"},{"location":"KW49-50/data_processing/type_conversion/#methoden-zur-typenkonvertierung","title":"Methoden zur Typenkonvertierung","text":""},{"location":"KW49-50/data_processing/type_conversion/#numerische-werte","title":"Numerische Werte","text":"<p>Mit Bibliotheken wie Pandas in Python k\u00f6nnen Zeichenketten in numerische Werte umgewandelt werden:</p> <pre><code>import pandas as pd\n\n# Beispiel-Daten\ndata = {\"Werte\": [\"100\", \"200\", \"300\"]}\ndf = pd.DataFrame(data)\n\n# Typenkonvertierung\ndf[\"Werte\"] = pd.to_numeric(df[\"Werte\"])\n</code></pre> <p>Falls die Konvertierung fehlschl\u00e4gt, kann der Parameter <code>errors='coerce'</code> verwendet werden, um problematische Werte in <code>NaN</code> umzuwandeln.</p>"},{"location":"KW49-50/data_processing/type_conversion/#datumsangaben","title":"Datumsangaben","text":"<p>Datumswerte lassen sich mit <code>pd.to_datetime</code> standardisieren:</p> <pre><code># Beispiel-Daten\ndata = {\"Datum\": [\"31/12/2024\", \"01-01-2025\", \"2024.12.30\"]}\ndf = pd.DataFrame(data)\n\n# Typenkonvertierung\ndf[\"Datum\"] = pd.to_datetime(df[\"Datum\"], dayfirst=True)\n</code></pre> <p>Der Parameter <code>dayfirst=True</code> stellt sicher, dass das europ\u00e4ische Datumsformat korrekt interpretiert wird.</p>"},{"location":"KW49-50/data_processing/type_conversion/#kategorische-daten","title":"Kategorische Daten","text":"<p>Kategorische Daten k\u00f6nnen in numerische Codes oder <code>category</code>-Typen umgewandelt werden:</p> <pre><code># Beispiel-Daten\ndata = {\"Antworten\": [\"Ja\", \"Nein\", \"Ja\"]}\ndf = pd.DataFrame(data)\n\n# Typenkonvertierung\ndf[\"Antworten\"] = df[\"Antworten\"].astype(\"category\")\n</code></pre> <p>Kategorien haben den Vorteil, weniger Speicherplatz zu beanspruchen und effizienter verarbeitet zu werden.</p>"},{"location":"KW49-50/data_processing/type_conversion/#fehlende-werte-behandeln","title":"Fehlende Werte behandeln","text":"<p>Vor der Typenkonvertierung m\u00fcssen Platzhalter wie \"N/A\" entfernt oder ersetzt werden:</p> <pre><code># Beispiel-Daten\ndata = {\"Werte\": [\"100\", \"N/A\", \"200\"]}\ndf = pd.DataFrame(data)\n\n# Fehlende Werte ersetzen und konvertieren\ndf[\"Werte\"] = pd.to_numeric(df[\"Werte\"], errors='coerce')\n</code></pre>"},{"location":"KW51/goals/","title":"Ziele","text":"<p>Ich kann...</p> <p>... aus bestehenden Features neue Features generieren. ... Kateogrien in numerische Werte umwandeln. ... mit one hot encoding umgehen. ... mit label encoding umgehen. ... erste Methoden zum Transformieren von Daten beschreiben. ... erste Methoden zum Skalieren von Daten beschreiben.</p>"},{"location":"KW51/feature_engineering/encoding/","title":"Encoding","text":"<p>Encoding ist der Prozess, kategorische Daten in numerische Werte umzuwandeln, damit maschinelle Lernalgorithmen mit ihnen arbeiten k\u00f6nnen. Das ist notwendig, da viele Algorithmen nur mit numerischen Daten umgehen k\u00f6nnen und kategoriale Werte \u2013 wie \"rot\", \"blau\", \"gr\u00fcn\" \u2013 nicht direkt verarbeiten k\u00f6nnen.</p> <p>Ein Beispiel ist das One-Hot-Encoding, bei dem jede Kategorie in eine bin\u00e4re Spalte umgewandelt wird. Wenn es zum Beispiel drei Farben gibt (\u201erot\u201c, \u201eblau\u201c, \u201egr\u00fcn\u201c), werden daraus drei Spalten: \u201erot\u201c, \u201eblau\u201c und \u201egr\u00fcn\u201c, in denen jede Zeile entweder eine 1 (aktiv) oder 0 (nicht aktiv) enth\u00e4lt.</p> <p>Beispiel in Python:</p> <pre><code>import pandas as pd\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Beispiel-Daten\nfarben = pd.DataFrame({'Farbe': ['rot', 'blau', 'gr\u00fcn', 'rot']})\n\n# One-Hot-Encoding\nencoder = OneHotEncoder(sparse=False)\nencoded = encoder.fit_transform(farben[['Farbe']])\n\n# Ergebnis anzeigen\nencoded_df = pd.DataFrame(encoded, columns=encoder.get_feature_names_out())\nprint(encoded_df)\n</code></pre> <p>Wann brauche ich das? - Wenn dein Datensatz kategorische Variablen enth\u00e4lt. - Besonders wichtig bei Algorithmen wie linearen Modellen, die nicht mit Kategorien arbeiten k\u00f6nnen.</p>"},{"location":"KW51/feature_engineering/feature_extraction/","title":"Feature Extraction","text":"<p>Feature Extraction bezeichnet die Ableitung relevanter Informationen aus komplexen Daten wie Text, Bildern oder Zeitreihen. Ziel ist es, die wichtigsten Eigenschaften (Features) aus den Rohdaten zu gewinnen, um die Komplexit\u00e4t zu reduzieren.</p> <p>Ein Beispiel ist das Extrahieren von Textl\u00e4nge aus einem Textfeld oder die Frequenzanalyse in Zeitreihendaten.</p> <p>Beispiel in Python:</p> <pre><code># Beispiel: Textl\u00e4nge berechnen\nimport pandas as pd\n\n# Beispiel-Daten\ntexte = pd.DataFrame({'Text': ['Hallo Welt', 'Data Science ist spannend', 'Python macht Spa\u00df']})\n\n# Feature: Textl\u00e4nge\ntexte['Textl\u00e4nge'] = texte['Text'].apply(len)\nprint(texte)\n</code></pre> <p>Wann brauche ich das? - Wenn deine Rohdaten unstrukturiert sind (z. B. Text, Bilder). - Um spezifische Eigenschaften zu extrahieren, die f\u00fcr Modelle relevant sind.</p>"},{"location":"KW51/feature_engineering/new_features/","title":"Erstellung neuer Features","text":"<p>Die Erstellung neuer Features (Feature Engineering) ist der Prozess, aus vorhandenen Daten neue, aussagekr\u00e4ftige Variablen zu generieren. Ziel ist es, die Beziehung zwischen Features und Zielvariablen zu verbessern.</p> <p>Ein Beispiel ist die Berechnung von Geschwindigkeiten aus Distanz- und Zeitdaten oder die Aggregation von Transaktionsdaten zu monatlichen Summen.</p> <p>Beispiel in Python:</p> <pre><code># Beispiel: Geschwindigkeit berechnen\nimport pandas as pd\n\n# Beispiel-Daten\ndaten = pd.DataFrame({'Distanz_km': [100, 200, 150], 'Zeit_h': [2, 4, 3]})\n\n# Neues Feature: Geschwindigkeit\ndaten['Geschwindigkeit_kmh'] = daten['Distanz_km'] / daten['Zeit_h']\nprint(daten)\n</code></pre> <p>Wann brauche ich das? - Wenn du die Leistung deiner Modelle verbessern m\u00f6chtest. - Um verborgene Zusammenh\u00e4nge in den Daten aufzudecken.</p>"},{"location":"KW51/feature_engineering/scaling/","title":"Scaling","text":"<p>Scaling (Skalierung) wird verwendet, um numerische Features in einen einheitlichen Wertebereich zu bringen. Dies ist besonders wichtig f\u00fcr Algorithmen, die empfindlich auf die Skala der Eingabedaten reagieren, wie z. B. lineare Regression oder k-Nearest Neighbors.</p> <p>Die zwei h\u00e4ufigsten Methoden sind: - Standardisierung: Zentriert die Daten auf Mittelwert 0 und Standardabweichung 1. - Min-Max-Skalierung: Skaliert Werte in einen Bereich zwischen 0 und 1.</p> <p>Beispiel in Python:</p> <pre><code>import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Beispiel-Daten\ndaten = pd.DataFrame({'Alter': [25, 35, 45, 55], 'Einkommen': [40000, 50000, 60000, 80000]})\n\n# Min-Max-Skalierung\nscaler = MinMaxScaler()\nskalierte_daten = scaler.fit_transform(daten)\n\n# Ergebnis anzeigen\nskalierte_df = pd.DataFrame(skalierte_daten, columns=['Alter', 'Einkommen'])\nprint(skalierte_df)\n</code></pre> <p>Wann brauche ich das? - Wenn deine Features unterschiedliche Skalen haben (z. B. Alter in Jahren und Einkommen in Euro). - Besonders wichtig bei Algorithmen wie k-Means, kNN oder neuronalen Netzen.</p>"},{"location":"KW51/feature_engineering/transformation/","title":"Transformation","text":"<p>Transformationen werden genutzt, um die Verteilung der Daten zu \u00e4ndern, z. B. um Schiefe zu korrigieren oder lineare Zusammenh\u00e4nge zu verst\u00e4rken. Eine g\u00e4ngige Methode ist die logarithmische Transformation, die oft bei schiefen Daten angewendet wird.</p> <p>Beispiel in Python:</p> <pre><code>import numpy as np\nimport pandas as pd\n\n# Beispiel-Daten\nwerte = pd.DataFrame({'Umsatz': [100, 500, 1000, 5000, 10000]})\n\n# Log-Transformation\nwerte['Log-Umsatz'] = np.log(werte['Umsatz'])\nprint(werte)\n</code></pre> <p>Wann brauche ich das? - Wenn deine Daten stark schief verteilt sind. - Wenn du exponentielle Trends oder Skaleneffekte linear darstellen m\u00f6chtest.</p>"},{"location":"data_analysis/data_visualization/","title":"Visualisierungen in der Deskriptiven Statistik","text":"<p>Visualisierungen sind ein grundlegendes Werkzeug, um Daten verst\u00e4ndlich darzustellen und erste Einsichten zu gewinnen. Im Folgenden werden verschiedene Graphen vorgestellt, die sich f\u00fcr unterschiedliche Anwendungsf\u00e4lle eignen.</p>"},{"location":"data_analysis/data_visualization/#1-histogramm","title":"1. Histogramm","text":"<ul> <li>Was zeichnet es aus: Ein Histogramm zeigt die Verteilung einer numerischen Variable, indem es die Daten in aufeinanderfolgende Intervalle unterteilt und die H\u00e4ufigkeit dieser Intervalle als Balken darstellt. Die H\u00f6he eines Balkens repr\u00e4sentiert die Anzahl der Datenpunkte in diesem Intervall.</li> <li>Wann kann ich es besonders gut anwenden: Histogramme sind ideal, um die Verteilung von Daten, insbesondere deren Form (z. B. Normalverteilung, Schiefe) und Streuung, zu visualisieren.</li> <li>Was w\u00e4re eine Alternative: Alternativen zum Histogramm sind Dichteplots oder Boxplots, die ebenfalls Verteilungen darstellen, jedoch mit anderer Betonung.</li> <li>Wie erstelle ich es:<ul> <li>Plotly: Histogram</li> <li>Seaborn: histplot</li> <li>Matplotlib: hist</li> </ul> </li> </ul>"},{"location":"data_analysis/data_visualization/#2-boxplot","title":"2. Boxplot","text":"<ul> <li>Was zeichnet es aus: Ein Boxplot veranschaulicht die Verteilung der Daten durch ein Diagramm, das den Median, die oberen und unteren Quartile sowie potenzielle Ausrei\u00dfer anzeigt. Die \u201eBox\u201c repr\u00e4sentiert den Interquartilsabstand, und \u201eWhiskers\u201c zeigen die Variabilit\u00e4t au\u00dferhalb der Quartile.</li> <li>Wann kann ich es besonders gut anwenden: Boxplots sind besonders n\u00fctzlich, um die Verteilung und die Symmetrie von Daten sowie m\u00f6gliche Ausrei\u00dfer darzustellen. Ideal f\u00fcr den Vergleich von Kategorien oder Gruppen.</li> <li>Was w\u00e4re eine Alternative: Alternativen sind Violinplots, die die Dichte der Verteilung detaillierter darstellen.</li> <li>Wie erstelle ich es:<ul> <li>Plotly: Boxplot</li> <li>Seaborn: boxplot</li> <li>Matplotlib: boxplot</li> </ul> </li> </ul>"},{"location":"data_analysis/data_visualization/#3-streudiagramm-scatter-plot","title":"3. Streudiagramm (Scatter Plot)","text":"<ul> <li>Was zeichnet es aus: Streudiagramme visualisieren die Beziehung zwischen zwei numerischen Variablen. Jeder Punkt auf dem Diagramm repr\u00e4sentiert einen Datenpunkt und die Achsen die Werte der jeweiligen Variablen.</li> <li>Wann kann ich es besonders gut anwenden: Ideal, um Korrelationen oder Muster zwischen zwei Variablen zu erkennen (z. B. ob ein linearer Zusammenhang besteht).</li> <li>Was w\u00e4re eine Alternative: Alternativen sind Liniendiagramme (wenn eine Reihenfolge wichtig ist) oder Heatmaps f\u00fcr dichtere Datens\u00e4tze.</li> <li>Wie erstelle ich es:<ul> <li>Plotly: Scatter Plot</li> <li>Seaborn: scatterplot</li> <li>Matplotlib: scatter</li> </ul> </li> </ul>"},{"location":"data_analysis/data_visualization/#4-liniendiagramm","title":"4. Liniendiagramm","text":"<ul> <li>Was zeichnet es aus: Ein Liniendiagramm verbindet Datenpunkte mittels Linien und veranschaulicht so den Verlauf einer oder mehrerer Reihen \u00fcber eine geordnete Achse (h\u00e4ufig Zeit).</li> <li>Wann kann ich es besonders gut anwenden: Liniendiagramme eignen sich gut f\u00fcr Zeitreihenanalysen und um Trends oder Zyklen in Daten darzustellen.</li> <li>Was w\u00e4re eine Alternative: Eine Alternative w\u00e4re ein Fl\u00e4chendiagramm (Area Chart), das die gleiche Information zeigt, jedoch mit gef\u00fcllten Bereichen.</li> <li>Wie erstelle ich es:<ul> <li>Plotly: Line Chart</li> <li>Seaborn: lineplot</li> <li>Matplotlib: plot</li> </ul> </li> </ul>"},{"location":"data_analysis/data_visualization/#5-violinplot","title":"5. Violinplot","text":"<ul> <li>Was zeichnet es aus: Der Violinplot kombiniert Elemente von Boxplots und Dichteplots und zeigt die Verteilung der Daten sowie deren Dichte entlang einer Achse.</li> <li>Wann kann ich es besonders gut anwenden: Violinplots sind n\u00fctzlich, wenn man die Verteilung und die Dichte einer Variablen gleichzeitig darstellen und dabei symmetrische oder bimodale Muster aufzeigen m\u00f6chte.</li> <li>Was w\u00e4re eine Alternative: Alternativen sind der Boxplot (f\u00fcr eine weniger detaillierte Darstellung der Dichte) oder der Dichteplot.</li> <li>Wie erstelle ich es:<ul> <li>Plotly: Violin Plot</li> <li>Seaborn: violinplot</li> <li>Matplotlib: violinplot</li> </ul> </li> </ul>"},{"location":"data_analysis/data_visualization/#6-heatmap","title":"6. Heatmap","text":"<ul> <li>Was zeichnet es aus: Heatmaps verwenden Farbschattierungen, um die Intensit\u00e4t von Werten in einer Matrix zu visualisieren. H\u00e4ufig werden sie verwendet, um Korrelationen zwischen Variablen darzustellen.</li> <li>Wann kann ich es besonders gut anwenden: Heatmaps sind besonders n\u00fctzlich, um Korrelationen zwischen vielen Variablen darzustellen, etwa bei der Analyse von Korrelationen in gro\u00dfen Datens\u00e4tzen.</li> <li>Was w\u00e4re eine Alternative: Eine Alternative zur Heatmap ist das Paarplot, das Korrelationen in Scatterplot-Matrizen zeigt.</li> <li>Wie erstelle ich es:<ul> <li>Plotly: Heatmap</li> <li>Seaborn: heatmap</li> <li>Matplotlib: imshow</li> </ul> </li> </ul>"},{"location":"data_analysis/data_visualization/#7-paarplot-pair-plot","title":"7. Paarplot (Pair Plot)","text":"<ul> <li>Was zeichnet es aus: Ein Paarplot visualisiert alle Kombinationen der Variablenpaare in einem Datensatz als Streudiagramme und eignet sich hervorragend zur Erkennung von Korrelationen und Zusammenh\u00e4ngen in multidimensionalen Datens\u00e4tzen.</li> <li>Wann kann ich es besonders gut anwenden: Wenn es darum geht, Korrelationen und Beziehungen zwischen mehreren Variablen gleichzeitig zu analysieren, ist der Paarplot das Mittel der Wahl.</li> <li>Was w\u00e4re eine Alternative: Alternativen sind Scatterplot-Matrizen oder einzelne Heatmaps.</li> <li>Wie erstelle ich es:<ul> <li>Seaborn: pairplot</li> <li>Plotly: scatter_matrix</li> </ul> </li> </ul>"},{"location":"data_analysis/data_visualization/#8-balkendiagramm-bar-chart","title":"8. Balkendiagramm (Bar Chart)","text":"<ul> <li>Was zeichnet es aus: Ein Balkendiagramm stellt kategorische Daten durch rechteckige Balken dar, deren L\u00e4nge proportional zur H\u00e4ufigkeit oder zum Wert einer Kategorie ist.</li> <li>Wann kann ich es besonders gut anwenden: Balkendiagramme sind ideal, um die Gr\u00f6\u00dfenverh\u00e4ltnisse verschiedener Kategorien direkt miteinander zu vergleichen.</li> <li>Was w\u00e4re eine Alternative: Eine Alternative zum Balkendiagramm ist das S\u00e4ulendiagramm, das \u00e4hnliche Daten vertikal darstellt.</li> <li>Wie erstelle ich es:<ul> <li>Plotly: Bar Chart</li> <li>Seaborn: barplot</li> <li>Matplotlib: bar</li> </ul> </li> </ul>"},{"location":"data_analysis/data_visualization/#9-kreisdiagramm-pie-chart","title":"9, Kreisdiagramm (Pie Chart)","text":"<ul> <li>Was zeichnet es aus: Ein Kreisdiagramm teilt einen Kreis in Segmente, die jeweils eine Kategorie repr\u00e4sentieren, und zeigt deren Anteile am Gesamtwert.</li> <li>Wann kann ich es besonders gut anwenden: Kreisdiagramme sind gut geeignet, um den Anteil einer Kategorie im Verh\u00e4ltnis zum Ganzen darzustellen, allerdings nicht bei mehr als f\u00fcnf Kategorien.</li> <li>Was w\u00e4re eine Alternative: Alternativen sind Donut Charts oder gestapelte Balkendiagramme, die Verh\u00e4ltnisse auch darstellen k\u00f6nnen.</li> <li>Wie erstelle ich es:<ul> <li>Plotly: Pie Chart</li> <li>Matplotlib: pie</li> </ul> </li> </ul>"},{"location":"data_analysis/data_visualization/#10-flachendiagramm-area-chart","title":"10. Fl\u00e4chendiagramm (Area Chart)","text":"<ul> <li>Was zeichnet es aus: Ein Fl\u00e4chendiagramm ist eine Variation des Liniendiagramms, bei dem der Bereich unter der Linie ausgef\u00fcllt ist, um den Wert \u00fcber eine geordnete Dimension (h\u00e4ufig Zeit) zu verdeutlichen.</li> <li>Wann kann ich es besonders gut anwenden: Fl\u00e4chendiagramme eignen sich besonders, um kumulative Daten oder Zeitreihendaten zu visualisieren, bei denen der Bereich unter der Linie eine Bedeutung hat.</li> <li>Was w\u00e4re eine Alternative: Liniendiagramme (ohne ausgef\u00fcllte Fl\u00e4che) oder Stapeldiagramme, wenn mehrere Kategorien dargestellt werden sollen.</li> <li>Wie erstelle ich es:<ul> <li>Plotly: Area Chart</li> <li>Matplotlib: fill_between</li> </ul> </li> </ul>"},{"location":"data_analysis/data_visualization/#11-blasendiagramm-bubble-chart","title":"11. Blasendiagramm (Bubble Chart)","text":"<ul> <li>Was zeichnet es aus: Ein Blasendiagramm ist ein erweitertes Streudiagramm, bei dem die Gr\u00f6\u00dfe der Punkte eine zus\u00e4tzliche Dimension repr\u00e4sentiert, meist einen numerischen Wert.</li> <li>Wann kann ich es besonders gut anwenden: Blasendiagramme sind n\u00fctzlich, wenn man drei Dimensionen (zwei f\u00fcr die Achsen und eine f\u00fcr die Punktgr\u00f6\u00dfe) auf einmal visualisieren m\u00f6chte.</li> <li>Was w\u00e4re eine Alternative: Alternativen sind einfache Streudiagramme (f\u00fcr zwei Dimensionen) oder Heatmaps f\u00fcr Korrelationen zwischen mehreren Variablen.</li> <li>Wie erstelle ich es:<ul> <li>Plotly: Bubble Chart</li> <li>Matplotlib: scatter (mit Gr\u00f6\u00dfenparameter)</li> </ul> </li> </ul>"},{"location":"data_analysis/data_visualization/#12-gestapeltes-balkendiagramm-stacked-bar-chart","title":"12. Gestapeltes Balkendiagramm (Stacked Bar Chart)","text":"<ul> <li>Was zeichnet es aus: Ein gestapeltes Balkendiagramm zeigt mehrere Kategorien \u00fcbereinander in einem Balken, wodurch die Gesamtmenge sowie die Verteilung der Teilkategorien auf einen Blick erkennbar sind.</li> <li>Wann kann ich es besonders gut anwenden: Diese Darstellung eignet sich f\u00fcr Daten, bei denen man sowohl die Gesamtwerte als auch die Anteile der Unterkategorien gleichzeitig zeigen m\u00f6chte.</li> <li>Was w\u00e4re eine Alternative: Eine Alternative ist das 100%-gestapelte Balkendiagramm, bei dem die Balken normalisiert werden, sodass sie immer bis 100% reichen.</li> <li>Wie erstelle ich es:<ul> <li>Plotly: Stacked Bar Chart</li> <li>Matplotlib: bar (mit stacked=True)</li> </ul> </li> </ul>"},{"location":"data_analysis/eda_steps/","title":"Deskriptive Datenanalyse","text":""},{"location":"data_analysis/eda_steps/#einfuhrung-in-die-explorative-datenanalyse-eda","title":"Einf\u00fchrung in die Explorative Datenanalyse (EDA)","text":"<p>In der explorativen Datenanalyse (EDA) werfen wir einen ersten Blick auf unsere Daten, um die wichtigsten Merkmale des Datensatzes zu erfassen und zu visualisieren. Diese Schritte helfen uns, die Daten besser zu verstehen, Hypothesen zu formulieren, und schlie\u00dflich die Voraussetzungen f\u00fcr eine fundierte Analyse oder ein Modell zu schaffen. Im Folgenden f\u00fchren wir dich Schritt f\u00fcr Schritt durch die f\u00fcnf wesentlichen Schritte einer EDA.</p>"},{"location":"data_analysis/eda_steps/#schritt-1-verstehen-der-datenstruktur","title":"Schritt 1: Verstehen der Datenstruktur","text":""},{"location":"data_analysis/eda_steps/#11-datentypen-und-strukturen","title":"1.1 Datentypen und Strukturen","text":"<ul> <li>Daten werden meist in numerische, kategorische und Zeitreihen-Daten unterteilt, und jeder Typ braucht besondere Techniken und Darstellungen.</li> <li>Nutze Datenstrukturen wie DataFrames (Pandas in Python) und Arrays (NumPy), um Daten zu laden und effizient zu verarbeiten.</li> </ul>"},{"location":"data_analysis/eda_steps/#12-kontext-und-quelle-der-daten","title":"1.2 Kontext und Quelle der Daten","text":"<ul> <li>Woher kommen die Daten? Wann wurden sie gesammelt? Notiere den Kontext, um den Zweck der Daten zu verstehen und m\u00f6gliche Verzerrungen zu erkennen.</li> </ul>"},{"location":"data_analysis/eda_steps/#13-metadaten-und-dokumentation","title":"1.3 Metadaten und Dokumentation","text":"<ul> <li>Wenn verf\u00fcgbar, \u00fcberpr\u00fcfe die Metadaten und die Dokumentation zum Datensatz. Das liefert oft wichtige Hinweise zu den Variablen und der Datenqualit\u00e4t.</li> </ul>"},{"location":"data_analysis/eda_steps/#schritt-2-datenbereinigung-und-vorverarbeitung","title":"Schritt 2: Datenbereinigung und Vorverarbeitung","text":""},{"location":"data_analysis/eda_steps/#21-fehlende-werte-behandeln","title":"2.1 Fehlende Werte behandeln","text":"<ul> <li>Optionen: Zeilen oder Spalten entfernen, fehlende Werte mit Durchschnitt oder Median auff\u00fcllen, oder Vorhersagemodelle nutzen, um die Werte zu sch\u00e4tzen.</li> </ul>"},{"location":"data_analysis/eda_steps/#22-duplikate-entfernen","title":"2.2 Duplikate entfernen","text":"<ul> <li>Doppelte Eintr\u00e4ge in deinem Datensatz verf\u00e4lschen Ergebnisse. Entferne sie, um die Integrit\u00e4t deiner Daten zu sichern.</li> </ul>"},{"location":"data_analysis/eda_steps/#23-transformation-der-daten","title":"2.3 Transformation der Daten","text":"<ul> <li>Normalisierung: Skalierung der Daten f\u00fcr eine einheitliche Verteilung.</li> <li>Kodierung: Umwandlung von Kategorien in numerische Werte (z.B. One-Hot-Codierung).</li> <li>Zeitreihen: Konvertiere Datumsangaben f\u00fcr eine korrekte Analyse von Zeitdaten.</li> </ul>"},{"location":"data_analysis/eda_steps/#24-ausreier-identifizieren-und-behandeln","title":"2.4 Ausrei\u00dfer identifizieren und behandeln","text":"<ul> <li>Verwende Diagramme wie Boxplots oder statistische Tests (z.B. Z-Scores), um Ausrei\u00dfer zu finden. Entschlie\u00dfe, ob du diese entfernst oder transformierst.</li> </ul>"},{"location":"data_analysis/eda_steps/#schritt-3-univariate-analyse-einzelne-variablen","title":"Schritt 3: Univariate Analyse (Einzelne Variablen)","text":""},{"location":"data_analysis/eda_steps/#31-deskriptive-statistik","title":"3.1 Deskriptive Statistik","text":"<ul> <li>F\u00fchre grundlegende statistische Berechnungen durch (Mittelwert, Median, Modus), um zentrale Tendenzen und Streuung der Variablen zu verstehen.</li> </ul>"},{"location":"data_analysis/eda_steps/#32-visualisierungen-univariate","title":"3.2 Visualisierungen (Univariate)","text":"<ul> <li>Histogramme: Zum Veranschaulichen der Verteilung.</li> <li>Boxplots: Zeigen Verteilung und Ausrei\u00dfer.</li> <li>Balkendiagramme: F\u00fcr die H\u00e4ufigkeit von Kategorien.</li> </ul>"},{"location":"data_analysis/eda_steps/#schritt-4-bivariate-und-multivariate-analyse-zusammenhange","title":"Schritt 4: Bivariate und multivariate Analyse (Zusammenh\u00e4nge)","text":""},{"location":"data_analysis/eda_steps/#41-bivariate-analyse","title":"4.1 Bivariate Analyse","text":"<ul> <li>Scatterplots: Visualisiert die Beziehung zwischen zwei numerischen Variablen.</li> <li>Korrelationsmatrix: Berechnet Korrelationen zwischen Variablen.</li> </ul>"},{"location":"data_analysis/eda_steps/#42-multivariate-analyse","title":"4.2 Multivariate Analyse","text":"<ul> <li>Paarplots und Heatmaps: Zeigen Beziehungen mehrerer Variablen.</li> <li>PCA (Principal Component Analysis): Reduziert die Komplexit\u00e4t der Daten.</li> <li>Clustering: Zum Gruppieren von Datenpunkten basierend auf \u00c4hnlichkeiten.</li> </ul>"},{"location":"data_analysis/eda_steps/#43-wechselwirkungen-erkennen","title":"4.3 Wechselwirkungen erkennen","text":"<ul> <li>Achte auf Wechselwirkungen zwischen Variablen, die m\u00f6glicherweise auf komplexe Abh\u00e4ngigkeiten oder Multikollinearit\u00e4t hinweisen.</li> </ul>"},{"location":"data_analysis/eda_steps/#schritt-5-einblicke-und-schlussfolgerungen-ziehen","title":"Schritt 5: Einblicke und Schlussfolgerungen ziehen","text":""},{"location":"data_analysis/eda_steps/#51-wichtige-erkenntnisse-zusammenfassen","title":"5.1 Wichtige Erkenntnisse zusammenfassen","text":"<ul> <li>Notiere die wichtigsten Muster und Ausrei\u00dfer. Welche Einblicke bieten deine Analysen?</li> </ul>"},{"location":"data_analysis/eda_steps/#52-visuelles-storytelling","title":"5.2 Visuelles Storytelling","text":"<ul> <li>Nutze ansprechende Diagramme, um deine Ergebnisse zu kommunizieren. Erstelle klare Visualisierungen, die die Daten einfach und verst\u00e4ndlich zusammenfassen.</li> </ul>"},{"location":"data_analysis/eda_steps/#53-entscheidungen-treffen","title":"5.3 Entscheidungen treffen","text":"<ul> <li>Verwende die Erkenntnisse der EDA, um Hypothesen zu formulieren, strategische Entscheidungen zu treffen oder n\u00e4chste Schritte f\u00fcr Modellierungen festzulegen.</li> </ul>"},{"location":"data_analysis/eda_steps/#54-dokumentation","title":"5.4 Dokumentation","text":"<ul> <li>Dokumentiere die EDA-Schritte, Methoden und Ergebnisse vollst\u00e4ndig, damit der Prozess nachvollziehbar und wiederholbar bleibt.</li> </ul>"},{"location":"data_analysis/intro/","title":"Data Analysis","text":"<p>Die Datenanalyse ist ein wesentlicher Bestandteil der Data Science und bildet die Grundlage f\u00fcr datengetriebene Entscheidungen und Erkenntnisse. Sie umfasst verschiedene Analyseans\u00e4tze, die uns helfen, aus Daten Informationen zu gewinnen und diese zielgerichtet zu nutzen. Im Wesentlichen l\u00e4sst sich die Datenanalyse in vier Hauptarten unterteilen, die jeweils unterschiedliche Fragen beantworten und unterschiedliche Methoden anwenden.</p>"},{"location":"data_analysis/intro/#die-vier-arten-der-datenanalyse","title":"Die vier Arten der Datenanalyse","text":""},{"location":"data_analysis/intro/#1-deskriptive-analyse","title":"1. Deskriptive Analyse","text":"<p>Die deskriptive Analyse stellt die Daten so dar, wie sie sind, und beantwortet grundlegende Fragen wie \u201eWas ist passiert?\u201c. Sie wird h\u00e4ufig als erster Schritt verwendet, um Muster, Trends und Verteilungen zu verstehen. Hierbei kommen statistische Kennzahlen wie Mittelwerte, H\u00e4ufigkeiten und Standardabweichungen zum Einsatz, um ein klares Bild der Daten zu gewinnen.</p>"},{"location":"data_analysis/intro/#2-diagnostische-analyse","title":"2. Diagnostische Analyse","text":"<p>Die diagnostische Analyse geht tiefer und versucht zu erkl\u00e4ren, \u201eWarum ist etwas passiert?\u201c. Sie analysiert Zusammenh\u00e4nge und identifiziert Muster, die auf Ursachen f\u00fcr bestimmte Beobachtungen hinweisen. Mit Methoden wie Hypothesentests und Regressionsanalysen lassen sich Korrelationen und Abh\u00e4ngigkeiten aufzeigen, die helfen, den Hintergrund eines Ph\u00e4nomens zu verstehen.</p>"},{"location":"data_analysis/intro/#3-pradiktive-analyse","title":"3. Pr\u00e4diktive Analyse","text":"<p>Die pr\u00e4diktive Analyse richtet den Blick in die Zukunft und versucht zu prognostizieren, \u201eWas wird wahrscheinlich passieren?\u201c. Durch die Analyse historischer Daten und das Erkennen von Mustern lassen sich Modelle entwickeln, die Vorhersagen erm\u00f6glichen. H\u00e4ufige Werkzeuge sind hier maschinelle Lernverfahren wie Regression, Entscheidungsb\u00e4ume und Zeitreihenanalysen, die bei der Ermittlung zuk\u00fcnftiger Trends und Entwicklungen unterst\u00fctzen.</p>"},{"location":"data_analysis/intro/#4-praskriptive-analyse","title":"4. Pr\u00e4skriptive Analyse","text":"<p>Die pr\u00e4skriptive Analyse baut auf den Erkenntnissen der pr\u00e4diktiven Analyse auf und stellt die Frage \u201eWas sollte getan werden?\u201c. Sie nutzt die erlangten Einsichten, um Handlungsempfehlungen zu geben und optimale Entscheidungen zu treffen. Typische Methoden umfassen Optimierungsalgorithmen und Entscheidungsmodelle, die konkrete Vorschl\u00e4ge zur Vorgehensweise bieten.</p> <p>Diese vier Analysearten bilden den Rahmen der Datenanalyse und stehen oft miteinander in Verbindung: Deskriptive und diagnostische Analysen geben Einblick in bestehende Daten, w\u00e4hrend pr\u00e4diktive und pr\u00e4skriptive Analysen darauf abzielen, zuk\u00fcnftige Ergebnisse vorherzusagen und zu beeinflussen. Gemeinsam erm\u00f6glichen sie eine umfassende Datenanalyse und er\u00f6ffnen wertvolle Perspektiven f\u00fcr datenbasierte Entscheidungen. </p> <p>Um die Datenlage verst\u00e4ndlich darzustellen und erste wertvolle Erkenntnisse zu gewinnen, werden in der deskriptiven Statistik grundlegende Methoden und Visualisierungen angewendet. Dieser n\u00e4chste Abschnitt konzentriert sich darauf, wie Datenmuster erkennbar gemacht und wichtige Verteilungen sowie Beziehungen visualisiert werden k\u00f6nnen.</p> <p>Im Fokus stehen dabei die verschiedenen Arten von Diagrammen und Graphen, die in der deskriptiven Statistik g\u00e4ngige Praxis sind, wie etwa Histogramme, Boxplots und Streudiagramme. Zus\u00e4tzlich betrachten wir einige der wichtigsten Bibliotheken in Python \u2013 darunter Matplotlib, Seaborn und Plotly.</p>"},{"location":"data_analysis/tasks/","title":"Aufgaben","text":""},{"location":"data_analysis/tasks/#stromerzeugung-in-deutschland-in-2024","title":"Stromerzeugung in Deutschland in 2024","text":"<ol> <li> <p>\u00dcberpr\u00fcfe die nachfolgenden Hypothesen</p> <ul> <li>Am Tag gibt es mehr Sonnenenergie als Nachts</li> <li>Solar erzeugt viel Strom, wenn Wind wenig Strom erzeugt</li> <li>Die Zeitumstellung hat einen Einfluss auf die Energieerzeugung (Zusatz)</li> <li>Im Jahr liefern die erneuerbaren Energien mehr Strom als die konventionellen Energien</li> <li>Die Energieerzeugung durch M\u00fcll ist die meiste Zeit konstant</li> <li>Kohle und Gas werden vor allem dann ben\u00f6tigt, wenn man eine hohe Last hat</li> <li>Wenn viel Energie aus erneuerbaren Energiequellen gewonnen wird, wird weniger Energie konventionell gewonnen</li> <li>In den D\u00e4mmerungen wird mehr Windstrom produziert, als in der Mittags- und Mitternachts-zeit (Zusatz)</li> </ul> </li> <li> <p>Erstelle eine weitere Hypothese. Suche dir einen Partner, welcher diese Hypothese \u00fcberpr\u00fcft.</p> </li> </ol>"},{"location":"data_analysis/understand_datasets/","title":"Datens\u00e4tze verstehen","text":""},{"location":"data_analysis/understand_datasets/#einstieg-in-einen-neuen-datensatz-tipps-zur-orientierung","title":"Einstieg in einen neuen Datensatz: Tipps zur Orientierung","text":"<p>Bevor wir mit einer tiefgehenden Analyse beginnen, ist es hilfreich, uns zuerst einen \u00dcberblick \u00fcber den Datensatz zu verschaffen. Ein strukturierter Ansatz sorgt daf\u00fcr, dass wir die Eigenschaften, St\u00e4rken und eventuelle Herausforderungen der Daten fr\u00fchzeitig erkennen. Hier sind einige wichtige Schritte und Ans\u00e4tze, die dir dabei helfen, dich in einem neuen Datensatz zurechtzufinden:</p>"},{"location":"data_analysis/understand_datasets/#1-datenquellen-und-kontext-prufen","title":"1. Datenquellen und Kontext pr\u00fcfen","text":"<ul> <li>Datenquelle und Kontext verstehen: Woher kommen die Daten? Stammt der Datensatz aus einer verl\u00e4sslichen Quelle? Was ist das Ziel oder der Anwendungsfall der Daten? </li> <li>Datenfelder und Bedeutung: Kl\u00e4re die Bedeutung der einzelnen Felder. Die Dokumentation oder begleitende Metadaten enthalten oft wertvolle Informationen zu den Variablen.</li> </ul>"},{"location":"data_analysis/understand_datasets/#2-uberblick-uber-die-struktur-des-datensatzes-gewinnen","title":"2. \u00dcberblick \u00fcber die Struktur des Datensatzes gewinnen","text":"<ul> <li>Spalten und Datentypen inspizieren: Nutze Python-Methoden wie <code>df.info()</code> oder <code>df.dtypes</code>, um schnell festzustellen, welche Datentypen (numerisch, kategorisch, zeitbasiert) vorhanden sind.</li> <li>Erste Einblicke in die Werte: Verwende <code>df.head()</code> und <code>df.tail()</code>, um dir die ersten und letzten Zeilen anzuschauen und ein Gef\u00fchl f\u00fcr die Dateninhalte zu bekommen. So erkennst du auch direkt m\u00f6gliche Anomalien.</li> </ul>"},{"location":"data_analysis/understand_datasets/#3-zentrale-kennzahlen-berechnen","title":"3. Zentrale Kennzahlen berechnen","text":"<ul> <li>Deskriptive Statistik: Nutze Methoden wie <code>df.describe()</code>, um erste statistische Einblicke zu gewinnen. Kennzahlen wie Mittelwert, Minimum, Maximum und Standardabweichung zeigen wichtige Eigenschaften der numerischen Variablen.</li> <li>Verteilung der Daten pr\u00fcfen: Einfache Histogramme oder Boxplots helfen, die Verteilung der Daten besser zu verstehen und Ausrei\u00dfer oder Verzerrungen zu identifizieren.</li> </ul>"},{"location":"data_analysis/understand_datasets/#4-datenqualitat-beurteilen","title":"4. Datenqualit\u00e4t beurteilen","text":"<ul> <li>Fehlende Werte analysieren: Pr\u00fcfe, ob und wo Werte fehlen. Methoden wie <code>df.isnull().sum()</code> zeigen, wie h\u00e4ufig fehlende Werte in jeder Spalte auftreten.</li> <li>Duplikate finden und bewerten: Mit <code>df.duplicated().sum()</code> erkennst du doppelte Eintr\u00e4ge, die du ggf. bereinigen solltest.</li> </ul>"},{"location":"data_analysis/understand_datasets/#5-erste-zusammenhange-und-muster-erkennen","title":"5. Erste Zusammenh\u00e4nge und Muster erkennen","text":"<ul> <li>Korrelationen untersuchen: Ein schneller Blick auf die Korrelationen numerischer Variablen (z.B. mit <code>df.corr()</code>) kann n\u00fctzliche Hinweise auf Beziehungen zwischen Variablen geben.</li> <li>Kategoriale und numerische Variablen analysieren: Einfache Kreuztabellen (<code>pd.crosstab()</code>) oder Gruppenstatistiken (<code>df.groupby()</code>) helfen, Verteilungen und Zusammenh\u00e4nge zu verstehen.</li> </ul>"},{"location":"data_analysis/understand_datasets/#6-erste-visualisierungen-nutzen","title":"6. Erste Visualisierungen nutzen","text":"<ul> <li>Visualisierung der wichtigsten Felder: Nutze Diagramme wie Balkendiagramme, Histogramme oder Boxplots, um zentrale Eigenschaften der Variablen visuell darzustellen. Dies hilft, Muster zu erkennen und eine Grundlage f\u00fcr detailliertere Analysen zu schaffen.</li> </ul>"},{"location":"fastapi/data_models/","title":"Datenmodelle mit Pydantic in FastAPI","text":"<p>Datenmodelle sind ein wesentlicher Bestandteil von APIs, da sie definieren, welche Art von Daten zwischen dem Server und den Clients ausgetauscht werden kann. In FastAPI \u00fcbernimmt die Bibliothek Pydantic diese Aufgabe, indem sie Python-Datenstrukturen in valide JSON-Objekte verwandelt. Pydantic bietet starke Validierungs- und Typisierungsfunktionen und erm\u00f6glicht es uns, Daten mit minimalem Aufwand zu strukturieren und zu validieren.</p>"},{"location":"fastapi/data_models/#warum-pydantic","title":"Warum Pydantic?","text":"<p>Pydantic ist eine zentrale Bibliothek in FastAPI, da sie eine einfache M\u00f6glichkeit bietet, Daten zu validieren und zu serialisieren. Mit Pydantic k\u00f6nnen wir sicherstellen, dass die Daten, die von einem Client gesendet werden oder an diesen zur\u00fcckgegeben werden, den gew\u00fcnschten Typen entsprechen. Dies verringert potenzielle Fehler und macht den Code robuster und lesbarer.</p>"},{"location":"fastapi/data_models/#typisierung-und-validierung","title":"Typisierung und Validierung","text":"<p>Pydantic nutzt Python\u2019s Typannotation, um die Struktur der Daten zu definieren und die Validierung basierend auf den angegebenen Datentypen durchzuf\u00fchren. Wenn du also ein Modell mit Feldern wie <code>str</code>, <code>int</code> oder <code>EmailStr</code> erstellst, stellt Pydantic sicher, dass die Daten den entsprechenden Typen entsprechen.</p> <p>Ein Beispiel f\u00fcr eine einfache Typisierung:</p> <pre><code>from pydantic import BaseModel, EmailStr\n\nclass User(BaseModel):\n    name: str\n    email: EmailStr\n    age: int\n</code></pre> <p>Hier sorgt Pydantic daf\u00fcr, dass die <code>email</code>-Adresse als g\u00fcltige E-Mail-Adresse formatiert ist, w\u00e4hrend der Name ein String und das Alter eine Ganzzahl ist.</p>"},{"location":"fastapi/data_models/#unterstutzte-datentypen","title":"Unterst\u00fctzte Datentypen","text":"<p>Pydantic unterst\u00fctzt eine breite Palette von Datentypen, um unterschiedliche Anwendungsf\u00e4lle abzudecken. Zu den wichtigsten geh\u00f6ren:</p> <ul> <li>Primitive Typen: <code>str</code>, <code>int</code>, <code>float</code>, <code>bool</code></li> <li>Spezielle Typen: <code>EmailStr</code>, <code>UUID</code>, <code>IPv4</code>, <code>IPv6</code></li> <li>Optionale Felder: <code>Optional[T]</code>, um Felder als optional zu kennzeichnen</li> <li>Listen und Tupel: <code>List[T]</code>, <code>Tuple[T, ...]</code></li> <li>Datum und Uhrzeit: <code>datetime</code>, <code>date</code>, <code>time</code></li> </ul>"},{"location":"fastapi/data_models/#erstellen-eines-einfachen-datenmodells","title":"Erstellen eines einfachen Datenmodells","text":"<p>Schauen wir uns ein einfaches Beispiel an: Wir m\u00f6chten eine API, die Benutzerdaten wie <code>name</code>, <code>email</code> und <code>age</code> verarbeitet. Mit Pydantic definieren wir die Struktur der zu verarbeitenden Daten.</p> <pre><code>from pydantic import BaseModel, EmailStr\n\nclass User(BaseModel):\n    name: str\n    email: EmailStr\n    age: int\n</code></pre> <p>In diesem Modell wird <code>name</code> als <code>str</code>, <code>email</code> als <code>EmailStr</code> und <code>age</code> als <code>int</code> validiert. Die <code>EmailStr</code>-Typisierung stellt sicher, dass die Eingabe eine g\u00fcltige E-Mail-Adresse ist.</p>"},{"location":"fastapi/data_models/#aufgabe","title":"Aufgabe","text":"<p>Erweitere das <code>User</code>-Modell, indem du ein optionales Feld f\u00fcr <code>city</code> hinzuf\u00fcgst, das einen Standardwert hat, zum Beispiel <code>\"Unbekannt\"</code>. F\u00fcge au\u00dferdem ein Feld <code>is_active</code> hinzu, das einen <code>bool</code>-Wert erwartet und standardm\u00e4\u00dfig auf <code>True</code> gesetzt ist.</p>"},{"location":"fastapi/data_models/#modell-in-fastapi-einbinden","title":"Modell in FastAPI einbinden","text":"<p>Nachdem wir das Datenmodell mit Pydantic erstellt haben, wollen wir es in einer FastAPI-Anwendung verwenden. FastAPI erm\u00f6glicht es uns, Pydantic-Modelle direkt in Routen zu integrieren. So k\u00f6nnen wir sicherstellen, dass alle eingehenden Daten den definierten Anforderungen entsprechen.</p> <pre><code>from fastapi import FastAPI\nfrom pydantic import BaseModel, EmailStr\nfrom typing import Optional\n\napp = FastAPI()\n\nclass User(BaseModel):\n    name: str\n    email: EmailStr\n    age: int\n    city: Optional[str] = \"Unbekannt\"\n    is_active: bool = True\n\n@app.post(\"/users/\")\nasync def create_user(user: User):\n    return {\"user\": user}\n</code></pre> <p>In diesem Beispiel haben wir die Route <code>/users/</code> erstellt, die einen <code>POST</code>-Request erwartet. FastAPI \u00fcbernimmt die Validierung der Daten und gibt die g\u00fcltigen Daten als Antwort zur\u00fcck.</p>"},{"location":"fastapi/data_models/#aufgabe_1","title":"Aufgabe","text":"<p>Teste die <code>/users/</code>-Route mit einem API-Client wie Postman. Sende ein JSON-Objekt, das die Felder <code>name</code>, <code>email</code> und <code>age</code> enth\u00e4lt, und \u00fcberpr\u00fcfe, wie FastAPI das Modell validiert und zur\u00fcckgibt.</p>"},{"location":"fastapi/data_models/#validierung-und-fehlerbehandlung","title":"Validierung und Fehlerbehandlung","text":"<p>FastAPI und Pydantic bieten umfangreiche M\u00f6glichkeiten, Daten zu validieren. Wenn Daten nicht den Anforderungen entsprechen, gibt FastAPI eine detaillierte Fehlermeldung zur\u00fcck, die den Entwickler genau dar\u00fcber informiert, welcher Wert nicht g\u00fcltig ist.</p>"},{"location":"fastapi/data_models/#beispiel-fur-eine-validierung","title":"Beispiel f\u00fcr eine Validierung:","text":"<pre><code>from pydantic import BaseModel, Field\n\nclass User(BaseModel):\n    name: str = Field(..., min_length=2, max_length=50)\n    email: EmailStr\n    age: int = Field(..., gt=0, le=120)  # Altersbeschr\u00e4nkung\n    city: Optional[str] = \"Unbekannt\"\n    is_active: bool = True\n</code></pre> <p>In diesem Beispiel haben wir Einschr\u00e4nkungen f\u00fcr das Feld <code>name</code> (minimale und maximale L\u00e4nge) und <code>age</code> (gr\u00f6\u00dfer als 0 und maximal 120 Jahre) definiert.</p> <p>Wenn jemand beispielsweise eine ung\u00fcltige E-Mail-Adresse sendet oder das Alter zu hoch ist, gibt FastAPI eine Fehlerantwort zur\u00fcck.</p>"},{"location":"fastapi/data_models/#aufgabe_2","title":"Aufgabe","text":"<p>Teste das Modell in einer einfachen <code>POST</code>-Route, indem du es in FastAPI integrierst. \u00dcbermittle verschiedene <code>name</code>- und <code>age</code>-Werte und beobachte die Fehlermeldungen, die FastAPI automatisch generiert, wenn die Validierungen fehlschlagen.</p>"},{"location":"fastapi/data_models/#eigene-validierungen","title":"Eigene Validierungen","text":"<p>Pydantic erm\u00f6glicht es, benutzerdefinierte Validierungen hinzuzuf\u00fcgen. Wenn du komplexere Anforderungen hast, die \u00fcber die Standardvalidierung hinausgehen, kannst du eigene Validierungsfunktionen implementieren.</p> <p>Ein Beispiel f\u00fcr eine benutzerdefinierte Validierung:</p> <pre><code>from pydantic import validator\n\nclass User(BaseModel):\n    name: str\n    email: EmailStr\n    age: int\n\n    @validator(\"name\")\n    def name_must_not_contain_numbers(cls, v):\n        if any(char.isdigit() for char in v):\n            raise ValueError(\"Name darf keine Zahlen enthalten\")\n        return v\n</code></pre> <p>In diesem Beispiel wird eine Validierung f\u00fcr das Feld <code>name</code> hinzugef\u00fcgt, die sicherstellt, dass der Name keine Zahlen enth\u00e4lt.</p>"},{"location":"fastapi/data_models/#aufgabe_3","title":"Aufgabe","text":"<p>Erweitere den <code>name</code>-Validator, sodass er auch sicherstellt, dass der Name keine Sonderzeichen enth\u00e4lt. Teste den Validator, indem du Namen wie \u201eJohn@Doe\u201c und \u201e123Peter\u201c eingibst und \u00fcberpr\u00fcfe die Fehlermeldungen.</p>"},{"location":"fastapi/first_steps/","title":"Erste Schritte mit FastAPI: HTTP-Routen","text":"<p>In diesem Abschnitt sehen wir uns die Grundlagen von HTTP-Routen und die Erstellung von API-Endpunkte in fastapi an. HTTP-Routen sind das Herzst\u00fcck jeder API. Sie definieren die verschiedenen Wege, auf denen Clients (wie Webbrowser oder mobile Apps) mit unserem Server kommunizieren k\u00f6nnen. FastAPI macht es besonders einfach, diese Routen zu erstellen und f\u00fcr verschiedene Anfragen zu konfigurieren.</p>"},{"location":"fastapi/first_steps/#unsere-erste-route-get","title":"Unsere erste Route: <code>GET</code>","text":"<p>Beginnen wir noch einmal mit einer grundlegenden Route, die eine Nachricht an den Client zur\u00fcckgibt. Der <code>GET</code>-Anfragetyp ist der einfachste und am h\u00e4ufigsten verwendete HTTP-Methodentyp \u2013 er ruft einfach Daten ab, ohne dass eine \u00c4nderung am Server oder in der Datenbank vorgenommen wird.</p> <pre><code>from fastapi import FastAPI\n\napp = FastAPI()\n\n@app.get(\"/\")\nasync def read_root():\n    return {\"message\": \"Hello, World!\"}\n</code></pre> <p>In diesem Beispiel erstellen wir eine FastAPI-Instanz namens <code>app</code>. Dann definieren wir eine <code>GET</code>-Route mit dem Endpunkt <code>/</code>, die einfach \u201eHello, World!\u201c zur\u00fcckgibt. Der <code>@app.get(\"/\")</code>-Dekorator sagt FastAPI, dass dieser Endpunkt auf <code>GET</code>-Anfragen wartet.</p>"},{"location":"fastapi/first_steps/#aufgabe","title":"Aufgabe","text":"<p>Starte den Server und rufe <code>http://127.0.0.1:8000/</code> in deinem Browser oder einem API-Client wie Postman auf. Siehst du die Nachricht? Experimentiere, indem du den Text \u00e4nderst. Teste, was passiert, wenn du den R\u00fcckgabewert ver\u00e4nderst \u2013 z. B. durch eine andere Nachricht oder eine Zahl.</p>"},{"location":"fastapi/first_steps/#parameter-in-der-url","title":"Parameter in der URL","text":"<p>Ein h\u00e4ufiges Szenario ist, dass wir Daten dynamisch basierend auf der Anfrage bereitstellen wollen. Nehmen wir an, wir m\u00f6chten eine Nachricht zur\u00fcckgeben, die den Namen des Benutzers enth\u00e4lt. Dazu f\u00fcgen wir einen URL-Parameter hinzu, der in die Route integriert wird.</p> <pre><code>@app.get(\"/hello/{name}\")\nasync def read_item(name: str):\n    return {\"message\": f\"Hello, {name}!\"}\n</code></pre> <p>Hier erstellen wir eine <code>GET</code>-Route mit einem dynamischen Segment <code>{name}</code>, das wir im Funktionsparameter <code>name</code> auffangen. Wenn wir <code>http://127.0.0.1:8000/hello/Alex</code> aufrufen, erhalten wir die Antwort: \u201eHello, Alex!\u201c</p>"},{"location":"fastapi/first_steps/#aufgabe_1","title":"Aufgabe","text":"<p>Erweitere die Route, um eine zweite Variable wie <code>age</code> oder <code>city</code> aufzunehmen. Erstelle eine Antwort, die beide Parameter in einem Begr\u00fc\u00dfungssatz verwendet. Teste verschiedene Namen und Werte, um zu sehen, wie FastAPI die Eingaben verarbeitet.</p>"},{"location":"fastapi/first_steps/#verwendung-von-http-methoden-post","title":"Verwendung von HTTP-Methoden: <code>POST</code>","text":"<p>Neben <code>GET</code> gibt es noch weitere HTTP-Methoden wie <code>POST</code>, <code>PUT</code> und <code>DELETE</code>, die alle unterschiedliche Zwecke erf\u00fcllen. <code>POST</code>-Anfragen werden typischerweise verwendet, um Daten an den Server zu senden, z. B. zum Erstellen eines neuen Eintrags.</p> <p>Angenommen, wir m\u00f6chten eine einfache Route erstellen, bei der der Benutzer eine Nachricht an den Server senden kann. Dabei nutzen wir die Methode <code>POST</code>, um die Nachricht vom Client entgegenzunehmen und eine Best\u00e4tigung zur\u00fcckzugeben.</p> <pre><code>from pydantic import BaseModel\n\nclass Message(BaseModel):\n    content: str\n\n@app.post(\"/send-message/\")\nasync def create_message(message: Message):\n    return {\"received_message\": message.content}\n</code></pre> <p>In diesem Beispiel erstellen wir ein Modell <code>Message</code> mit dem Attribut <code>content</code>, das eine Zeichenkette ist. Das <code>@app.post(\"/send-message/\")</code> zeigt FastAPI, dass dieser Endpunkt eine <code>POST</code>-Anfrage erwartet. Der Inhalt wird in Form eines JSON-Objekts vom Client gesendet und in das <code>message</code>-Objekt des Typs <code>Message</code> umgewandelt. Anschlie\u00dfend geben wir die empfangene Nachricht als Best\u00e4tigung zur\u00fcck.</p>"},{"location":"fastapi/first_steps/#aufgabe_2","title":"Aufgabe","text":"<p>Teste diese <code>POST</code>-Route mit einem API-Client wie Postman oder durch einen Browser-Extension. Sende eine JSON-Nachricht wie <code>{\"content\": \"Dies ist meine erste Nachricht\"}</code>. Experimentiere mit verschiedenen Nachrichten und \u00fcberpr\u00fcfe, wie FastAPI die Antwort generiert.</p>"},{"location":"fastapi/first_steps/#arbeiten-mit-query-parametern","title":"Arbeiten mit Query-Parametern","text":"<p>Neben Routenparametern und <code>POST</code>-Daten bietet FastAPI die M\u00f6glichkeit, Query-Parameter zu verwenden. Diese Art von Parametern befindet sich in der URL nach einem <code>?</code> und wird h\u00e4ufig f\u00fcr zus\u00e4tzliche, optionale Informationen genutzt. Beispielsweise m\u00f6chten wir eine Route erstellen, bei der der Benutzer seinen Namen als Query-Parameter senden kann, ohne ihn in der URL selbst zu definieren.</p> <pre><code>@app.get(\"/greet/\")\nasync def greet_user(name: str = \"Gast\"):\n    return {\"message\": f\"Hallo, {name}!\"}\n</code></pre> <p>In dieser Route verwenden wir den Query-Parameter <code>name</code>, der standardm\u00e4\u00dfig \u201eGast\u201c ist, falls kein Wert \u00fcbergeben wird. Wenn wir <code>http://127.0.0.1:8000/greet/?name=Lisa</code> aufrufen, erhalten wir die Antwort \u201eHallo, Lisa!\u201c.</p>"},{"location":"fastapi/first_steps/#aufgabe_3","title":"Aufgabe","text":"<p>Experimentiere mit der URL und dem <code>name</code>-Parameter. Probiere verschiedene Namen und teste, was passiert, wenn du den Parameter wegl\u00e4sst. Erweitere das Beispiel, indem du weitere optionale Query-Parameter hinzuf\u00fcgst, etwa <code>age</code> oder <code>city</code>.</p>"},{"location":"fastapi/intro/","title":"Einf\u00fchrung in FastAPI","text":""},{"location":"fastapi/intro/#was-ist-fastapi","title":"Was ist FastAPI?","text":"<p>FastAPI ist ein Web-Framework zur Entwicklung von APIs mit Python. Es wurde f\u00fcr den Einsatz in produktionskritischen Anwendungen entwickelt und zeichnet sich durch eine hohe Performance und einfache Handhabung aus.</p>"},{"location":"fastapi/intro/#kernmerkmale-von-fastapi","title":"Kernmerkmale von FastAPI","text":"<ul> <li>Automatische Dokumentation: FastAPI generiert automatisch interaktive API-Dokumentationen (Swagger UI, Redoc) auf Basis des OpenAPI-Standards.</li> <li>Asynchrone Verarbeitung: Unterst\u00fctzung f\u00fcr <code>async</code> und <code>await</code> erleichtert das Handling von asynchronen Aufgaben und macht FastAPI ideal f\u00fcr Anwendungen mit hohen Anforderungen an die Skalierbarkeit.</li> <li>Einfache Validierung: FastAPI verwendet Pydantic zur Validierung und Serialisierung von Daten, wodurch die Datenintegrit\u00e4t automatisch gesichert wird.</li> </ul>"},{"location":"fastapi/intro/#beispiel-einer-fastapi-anwendung","title":"Beispiel einer FastAPI-Anwendung","text":"<p>Hier eine grundlegende FastAPI-Anwendung, die einen \"Hello World\"-Endpunkt bereitstellt:</p> <pre><code>from fastapi import FastAPI\n\napp = FastAPI()\n\n@app.get(\"/\")\nasync def root():\n    return {\"message\": \"Hello, World!\"}\n</code></pre>"},{"location":"fastapi/intro/#installation","title":"Installation","text":"<p>FastAPI kann \u00fcber <code>pip</code> installiert werden:</p> <pre><code>pip install fastapi[all]\n</code></pre> <p>Zus\u00e4tzlich wird ein ASGI-Server wie uvicorn ben\u00f6tigt, um die Anwendung zu starten:</p> <pre><code>pip install uvicorn\nuvicorn main:app --reload\n</code></pre>"},{"location":"fastapi/intro/#fastapi-vs-flask","title":"FastAPI vs. Flask","text":"<p>FastAPI und Flask sind beide beliebte Python-Frameworks zur API-Entwicklung, unterscheiden sich jedoch erheblich in ihrer Funktionsweise und ihrem Anwendungsbereich.</p>"},{"location":"fastapi/intro/#hauptunterschiede","title":"Hauptunterschiede","text":"Merkmal Flask FastAPI Asynchronit\u00e4t Unterst\u00fctzung nur mit zus\u00e4tzlichen Bibliotheken wie <code>flask-async</code> Eingebaute Unterst\u00fctzung f\u00fcr <code>async</code> und <code>await</code> Performance Moderate Geschwindigkeit Hohe Performance durch asynchrone Architektur Datenvalidierung Keine eingebaute Validierung, zus\u00e4tzliche Bibliotheken wie <code>marshmallow</code> erforderlich Integrierte Validierung mit Pydantic Dokumentation Keine automatische Dokumentation Automatische Generierung von Swagger UI und Redoc Ideal f\u00fcr Einfache APIs und Anwendungen APIs mit hoher Leistung und komplexen Datenvalidierungsanforderungen"},{"location":"fastapi/intro/#wann-man-fastapi-verwenden-sollte","title":"Wann man FastAPI verwenden sollte","text":"<ul> <li>Wenn hohe Performance und Skalierbarkeit gefordert sind.</li> <li>F\u00fcr APIs, die asynchrone Verarbeitung ben\u00f6tigen.</li> <li>Bei Projekten, in denen die automatische Dokumentation n\u00fctzlich ist.</li> </ul>"},{"location":"fastapi/intro/#wann-flask-geeigneter-ist","title":"Wann Flask geeigneter ist","text":"<ul> <li>Bei einfachen Anwendungen oder Prototypen, die nur grundlegende Funktionalit\u00e4ten ben\u00f6tigen.</li> <li>Wenn Asynchronit\u00e4t und Performance keine kritischen Anforderungen sind.</li> </ul>"},{"location":"fastapi/sqlmodel/","title":"Arbeiten mit SQLModel in FastAPI","text":"<p>SQLModel ist eine Bibliothek, die SQLAlchemy und Pydantic kombiniert. Sie erm\u00f6glicht es dir, Datenbankmodelle in FastAPI zu integrieren, ohne auf umfangreiche ORM-Definitionen verzichten zu m\u00fcssen. SQLModel baut auf SQLAlchemy auf und verwendet Pydantic zur Validierung der Daten, sodass du Datenbankmodelle erstellen und gleichzeitig die Vorteile der Datenvalidierung von Pydantic nutzen kannst.</p>"},{"location":"fastapi/sqlmodel/#was-ist-sqlmodel","title":"Was ist SQLModel?","text":"<p>SQLModel ist eine Bibliothek, die es erm\u00f6glicht, SQL-Datenbanken in FastAPI-Anwendungen zu integrieren. Sie stellt eine einfache Schnittstelle zur Verf\u00fcgung, die es dir erm\u00f6glicht, sowohl Datenbankmodelle zu definieren als auch mit der Datenbank zu interagieren \u2013 alles in einer sauberen und einheitlichen API.</p> <p>SQLModel erm\u00f6glicht die Definition von Pydantic-Modellen, die gleichzeitig auch Datenbankmodelle sind. Das bedeutet, dass du nur ein Modell schreiben musst, um sowohl mit der Datenbank als auch mit FastAPI zu arbeiten.</p>"},{"location":"fastapi/sqlmodel/#installation","title":"Installation","text":"<pre><code>pip install sqlmodel\n</code></pre>"},{"location":"fastapi/sqlmodel/#erstellen-eines-einfachen-sqlmodel-datenmodells","title":"Erstellen eines einfachen SQLModel-Datenmodells","text":"<p>Ein SQLModel-Datenmodell ist sehr \u00e4hnlich wie ein Pydantic-Modell, aber mit zus\u00e4tzlichen SQLAlchemy-Features wie <code>Field</code> und <code>Relationship</code> f\u00fcr die Datenbankinteraktion. </p> <pre><code>from sqlmodel import Field, SQLModel\n\nclass User(SQLModel, table=True):\n    id: int = Field(default=None, primary_key=True)\n    name: str\n    email: str\n    age: int\n</code></pre> <p>In diesem Beispiel haben wir ein einfaches User-Modell erstellt, das die Felder <code>id</code>, <code>name</code>, <code>email</code> und <code>age</code> enth\u00e4lt. Das <code>table=True</code>-Attribut signalisiert, dass dieses Modell eine Tabelle in der Datenbank repr\u00e4sentiert.</p>"},{"location":"fastapi/sqlmodel/#aufgabe","title":"Aufgabe","text":"<p>Erstelle ein weiteres Datenmodell, das <code>Product</code>-Daten mit den Feldern <code>name</code>, <code>price</code> und <code>description</code> speichert. Vergiss nicht, die <code>id</code> als Prim\u00e4rschl\u00fcssel hinzuzuf\u00fcgen.</p>"},{"location":"fastapi/sqlmodel/#datenbankverbindungen-und-sessions","title":"Datenbankverbindungen und Sessions","text":"<p>Eine der wichtigsten Aufgaben beim Arbeiten mit SQLModel ist die Verbindung zur Datenbank und die Verwaltung von Sessions, um Transaktionen durchzuf\u00fchren. SQLModel baut auf SQLAlchemy auf, sodass wir die gleiche Methode verwenden, um die Verbindung zur Datenbank herzustellen.</p>"},{"location":"fastapi/sqlmodel/#verbindung-zur-datenbank-herstellen","title":"Verbindung zur Datenbank herstellen","text":"<p>Du kannst SQLModel mit einer SQLite-Datenbank oder einer anderen unterst\u00fctzten Datenbank wie PostgreSQL oder MySQL verwenden. Hier ist ein einfaches Beispiel f\u00fcr die Verbindung zu einer SQLite-Datenbank:</p> <pre><code>from sqlmodel import create_engine, Session\n\n# Erstelle die Verbindung zur SQLite-Datenbank\nengine = create_engine(\"sqlite:///database.db\")\n\n# Erstelle die Tabellen in der Datenbank (falls sie noch nicht existieren)\nSQLModel.metadata.create_all(engine)\n</code></pre>"},{"location":"fastapi/sqlmodel/#arbeiten-mit-sessions","title":"Arbeiten mit Sessions","text":"<p>Um mit der Datenbank zu interagieren, musst du eine Session erstellen, die es dir erm\u00f6glicht, Datens\u00e4tze zu lesen, zu schreiben und zu aktualisieren.</p> <pre><code>from sqlmodel import Session\n\n# \u00d6ffne eine Session, um mit der Datenbank zu interagieren\nwith Session(engine) as session:\n    # Beispiel: Hinzuf\u00fcgen eines neuen Benutzers\n    user = User(name=\"John Doe\", email=\"johndoe@example.com\", age=30)\n    session.add(user)\n    session.commit()\n</code></pre>"},{"location":"fastapi/sqlmodel/#aufgabe_1","title":"Aufgabe","text":"<p>Erstelle eine neue <code>Product</code>-Instanz, f\u00fcge sie zur Datenbank hinzu und best\u00e4tige die Transaktion mit <code>session.commit()</code>.</p>"},{"location":"fastapi/sqlmodel/#crud-operationen-mit-sqlmodel","title":"CRUD-Operationen mit SQLModel","text":"<p>SQLModel macht es einfach, grundlegende CRUD-Operationen (Create, Read, Update, Delete) auf Datenbanken durchzuf\u00fchren. Nachdem du die Datenbankverbindung und das Modell eingerichtet hast, kannst du Datens\u00e4tze einfach erstellen und abfragen.</p>"},{"location":"fastapi/sqlmodel/#erstellen-eines-datensatzes-create","title":"Erstellen eines Datensatzes (Create)","text":"<pre><code>with Session(engine) as session:\n    new_user = User(name=\"Jane Doe\", email=\"janedoe@example.com\", age=25)\n    session.add(new_user)\n    session.commit()\n</code></pre>"},{"location":"fastapi/sqlmodel/#lesen-von-datensatzen-read","title":"Lesen von Datens\u00e4tzen (Read)","text":"<pre><code>with Session(engine) as session:\n    user = session.query(User).filter(User.name == \"Jane Doe\").first()\n    print(user)\n</code></pre>"},{"location":"fastapi/sqlmodel/#aktualisieren-von-datensatzen-update","title":"Aktualisieren von Datens\u00e4tzen (Update)","text":"<pre><code>with Session(engine) as session:\n    user = session.query(User).filter(User.name == \"Jane Doe\").first()\n    if user:\n        user.age = 26\n        session.commit()\n</code></pre>"},{"location":"fastapi/sqlmodel/#loschen-von-datensatzen-delete","title":"L\u00f6schen von Datens\u00e4tzen (Delete)","text":"<pre><code>with Session(engine) as session:\n    user = session.query(User).filter(User.name == \"Jane Doe\").first()\n    if user:\n        session.delete(user)\n        session.commit()\n</code></pre>"},{"location":"fastapi/sqlmodel/#aufgabe_2","title":"Aufgabe","text":"<p>Erweitere das <code>User</code>-Modell um ein <code>address</code>-Feld und f\u00fchre eine <code>UPDATE</code>-Operation durch, um die Adresse eines bestimmten Benutzers zu \u00e4ndern.</p>"},{"location":"fastapi/sqlmodel/#integration-von-sqlmodel-mit-fastapi","title":"Integration von SQLModel mit FastAPI","text":"<p>Ein gro\u00dfer Vorteil von SQLModel ist, dass es nahtlos mit FastAPI integriert werden kann. Du kannst SQLModel-Modelle direkt in deine FastAPI-Routen einbinden, um API-Endpunkte zu erstellen, die mit der Datenbank interagieren.</p>"},{"location":"fastapi/sqlmodel/#beispiel-fur-einen-fastapi-endpunkt-mit-sqlmodel","title":"Beispiel f\u00fcr einen FastAPI-Endpunkt mit SQLModel","text":"<pre><code>from fastapi import FastAPI\nfrom sqlmodel import Session, select\n\napp = FastAPI()\n\n@app.post(\"/users/\")\nasync def create_user(user: User):\n    with Session(engine) as session:\n        session.add(user)\n        session.commit()\n    return {\"user\": user}\n</code></pre> <p>In diesem Beispiel haben wir eine <code>POST</code>-Route erstellt, die ein <code>User</code>-Objekt entgegennimmt und es in die Datenbank speichert.</p>"},{"location":"fastapi/sqlmodel/#aufgabe_3","title":"Aufgabe","text":"<p>Erstelle eine <code>GET</code>-Route, die alle Benutzer aus der Datenbank abruft und zur\u00fcckgibt.</p>"},{"location":"fastapi/task/","title":"Vorhersage des Einkommens basierend auf dem Adult Income Dataset","text":"<p>In dieser Aufgabe wirst du das Adult Income Dataset verwenden, um ein einfaches API-Modell zu erstellen, das die Wahrscheinlichkeit vorhersagt, ob jemand mehr als 50.000 USD j\u00e4hrlich verdient, basierend auf verschiedenen demografischen Merkmalen. Du wirst SQLModel f\u00fcr das Speichern der Daten in einer Datenbank und FastAPI f\u00fcr die API-Erstellung verwenden.</p>"},{"location":"fastapi/task/#aufgabe-1-datenvorbereitung-und-verstandnis","title":"Aufgabe 1: Datenvorbereitung und -verst\u00e4ndnis","text":"<p>Lade das Adult Income Dataset herunter und analysiere die Spalten. Erstelle ein Streamlit Dashboard oder Jupyter Notebook, um den Datensatz visuell aufzubereiten. Der Datensatz enth\u00e4lt folgende Merkmale:</p> <ul> <li><code>age</code>: Alter der Person</li> <li><code>workclass</code>: Arbeitsverh\u00e4ltnis (z.B. privat, \u00f6ffentlich)</li> <li><code>fnlwgt</code>: Gewicht (repr\u00e4sentiert die Anzahl der Menschen, die in der Stichprobe repr\u00e4sentiert sind)</li> <li><code>education</code>: H\u00f6chster Bildungsabschluss</li> <li><code>education-num</code>: Bildung in numerischer Form</li> <li><code>marital-status</code>: Familienstand</li> <li><code>occupation</code>: Beruf</li> <li><code>relationship</code>: Beziehung zum Haushaltsvorstand</li> <li><code>race</code>: Rasse</li> <li><code>sex</code>: Geschlecht</li> <li><code>capital-gain</code>: Kapitalgewinne</li> <li><code>capital-loss</code>: Kapitalverlust</li> <li><code>hours-per-week</code>: Arbeitsstunden pro Woche</li> <li><code>native-country</code>: Geburtsland</li> <li><code>income</code>: Einkommen (mehr als 50K oder weniger)</li> </ul>"},{"location":"fastapi/task/#aufgabe-2-erstellen-eines-sqlmodel-datenmodells","title":"Aufgabe 2: Erstellen eines SQLModel-Datenmodells","text":"<p>Erstelle ein <code>User</code>-Datenmodell mit SQLModel, das die oben genannten Merkmale repr\u00e4sentiert. Definiere auch den Typ der Merkmale (z. B. <code>age</code> als Integer, <code>education</code> als String). </p>"},{"location":"fastapi/task/#aufgabe-3-api-erstellung-mit-fastapi","title":"Aufgabe 3: API-Erstellung mit FastAPI","text":"<p>Erstelle eine FastAPI-Anwendung, die folgende Funktionalit\u00e4ten bietet:</p> <ol> <li> <p>Daten hinzuf\u00fcgen: Erstelle eine POST-Route, die es erm\u00f6glicht, neue Benutzerdaten hinzuzuf\u00fcgen. Die Route sollte alle Merkmale des Datensatzes akzeptieren.</p> </li> <li> <p>Daten abfragen: Erstelle eine GET-Route, um alle gespeicherten Benutzerdaten abzurufen.</p> </li> <li> <p>Einkommensvorhersage: Erstelle eine weitere POST-Route, die basierend auf den eingegebenen demografischen Merkmalen vorhersagt, ob das Einkommen mehr als 50.000 USD betr\u00e4gt oder nicht. Diese Route sollte ein Modell wie z. B. einen einfachen Entscheidungsbaum verwenden (der zuvor auf den Daten trainiert wurde), um die Vorhersage zu treffen.</p> </li> </ol>"},{"location":"fastapi/task/#aufgabe-4-testen-der-api-mit-postman-oder-einem-anderen-api-client","title":"Aufgabe 4: Testen der API mit Postman oder einem anderen API-Client","text":"<ol> <li> <p>Daten hinzuf\u00fcgen: Verwende die POST-Route <code>/add-user</code> (oder eine benannte Route) zum Hinzuf\u00fcgen eines neuen Datensatzes.</p> </li> <li> <p>Daten abfragen: Verwende die GET-Route <code>/users</code>, um alle gespeicherten Benutzer abzurufen.</p> </li> <li> <p>Einkommensvorhersage: Teste die <code>/predict-income/</code>-Route, indem du verschiedene Werte f\u00fcr <code>age</code>, <code>hours-per-week</code> und andere Merkmale eingibst.</p> </li> </ol>"},{"location":"fastapi/task/#aufgabe-5-zusatz-analyse-und-modellverbesserung","title":"Aufgabe 5 (Zusatz): Analyse und Modellverbesserung","text":"<p>Nachdem du die API erstellt hast, analysiere die Vorhersageergebnisse und \u00fcberlege, wie du das Modell verbessern kannst</p>"},{"location":"week10/goals/","title":"Ziele","text":"<p>Ich kann...</p>"},{"location":"week11/goals/","title":"Ziele","text":"<p>Ich kann...</p> <p>... erkl\u00e4ren, was mit ETL Prozessen gemeint ist. ... eine Datenpipeline erstellen und verwalten. ... zwischen Triggern, Webhooks und CRON Jobs unterscheiden. ... erkl\u00e4ren, was eine Datenpipeline ist. ... Datenpipelines in Python erstellen. ... eine Datenpipeline in der Cloud hosten. ... eine eigene NoSQL Datenbank anlegen. ... Datenbanken in der Cloud erstellen.</p>"},{"location":"week6/goals/","title":"Ziele","text":"<p>Ich kann...</p> <p>... Daten aus einer SQL Datenbank abrufen. ... Abfragen, Filter und Joins auf SQL Datenbanken schreiben. ... zwischen den Begriffen Datenbank und Data warehouse unterscheiden. ... Grundlagen der Datenmodellierung beschreiben. ... zwischen relationalen und nicht-relationalen Datenbanken unterscheiden. ... Abfragen, Filter und Joins auf einer MongoDB schreiben. ... eine eigene NoSQL Datenbank anlegen. ... Datenbanken in der Cloud erstellen.</p>"},{"location":"week7/goals/","title":"Ziele","text":"<p>Ich kann...</p> <p>... Daten aus einer SQL Datenbank abrufen. ... Abfragen, Filter und Joins auf SQL Datenbanken schreiben. ... zwischen den Begriffen Datenbank und Data warehouse unterscheiden. ... Grundlagen der Datenmodellierung beschreiben. ... zwischen relationalen und nicht-relationalen Datenbanken unterscheiden. ... Abfragen, Filter und Joins auf einer MongoDB schreiben. ... eine eigene NoSQL Datenbank anlegen. ... Datenbanken in der Cloud erstellen.</p>"},{"location":"week8/goals/","title":"Ziele","text":"<p>Ich kann...</p> <p>... Daten aus einer SQL Datenbank abrufen. ... Abfragen, Filter und Joins auf SQL Datenbanken schreiben. ... zwischen den Begriffen Datenbank und Data warehouse unterscheiden. ... Grundlagen der Datenmodellierung beschreiben. ... zwischen relationalen und nicht-relationalen Datenbanken unterscheiden. ... Abfragen, Filter und Joins auf einer MongoDB schreiben. ... eine eigene NoSQL Datenbank anlegen. ... Datenbanken in der Cloud erstellen.</p>"},{"location":"week9/goals/","title":"Ziele","text":"<p>Ich kann...</p>"}]}