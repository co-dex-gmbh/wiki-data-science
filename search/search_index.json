{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Kurs\u00fcbersicht","text":""},{"location":"#kursinhalte","title":"Kursinhalte:","text":"Thema Beschreibung FastAPI Einf\u00fchrung in FastAPI: Erstellung von APIs und Routen. Pydantic Datenvalidierung und Modellierung mit Pydantic. SQLModel Integration von SQLModel f\u00fcr den Umgang mit Datenbanken in FastAPI. Visualisierungen und Plots \u00dcbersicht zu Grafiken und die Erstellung in Python."},{"location":"#projekte","title":"Projekte:","text":"<p>Hier ist Platz f\u00fcr gemeinsame Projekte, welche in diesem Kurs bearbeitet werden:</p>"},{"location":"data_analysis/data_visualization/","title":"Visualisierungen in der Deskriptiven Statistik","text":"<p>Visualisierungen sind ein grundlegendes Werkzeug, um Daten verst\u00e4ndlich darzustellen und erste Einsichten zu gewinnen. Im Folgenden werden verschiedene Graphen vorgestellt, die sich f\u00fcr unterschiedliche Anwendungsf\u00e4lle eignen.</p>"},{"location":"data_analysis/data_visualization/#1-histogramm","title":"1. Histogramm","text":"<ul> <li>Was zeichnet es aus: Ein Histogramm zeigt die Verteilung einer numerischen Variable, indem es die Daten in aufeinanderfolgende Intervalle unterteilt und die H\u00e4ufigkeit dieser Intervalle als Balken darstellt. Die H\u00f6he eines Balkens repr\u00e4sentiert die Anzahl der Datenpunkte in diesem Intervall.</li> <li>Wann kann ich es besonders gut anwenden: Histogramme sind ideal, um die Verteilung von Daten, insbesondere deren Form (z. B. Normalverteilung, Schiefe) und Streuung, zu visualisieren.</li> <li>Was w\u00e4re eine Alternative: Alternativen zum Histogramm sind Dichteplots oder Boxplots, die ebenfalls Verteilungen darstellen, jedoch mit anderer Betonung.</li> <li>Wie erstelle ich es:<ul> <li>Plotly: Histogram</li> <li>Seaborn: histplot</li> <li>Matplotlib: hist</li> </ul> </li> </ul>"},{"location":"data_analysis/data_visualization/#2-boxplot","title":"2. Boxplot","text":"<ul> <li>Was zeichnet es aus: Ein Boxplot veranschaulicht die Verteilung der Daten durch ein Diagramm, das den Median, die oberen und unteren Quartile sowie potenzielle Ausrei\u00dfer anzeigt. Die \u201eBox\u201c repr\u00e4sentiert den Interquartilsabstand, und \u201eWhiskers\u201c zeigen die Variabilit\u00e4t au\u00dferhalb der Quartile.</li> <li>Wann kann ich es besonders gut anwenden: Boxplots sind besonders n\u00fctzlich, um die Verteilung und die Symmetrie von Daten sowie m\u00f6gliche Ausrei\u00dfer darzustellen. Ideal f\u00fcr den Vergleich von Kategorien oder Gruppen.</li> <li>Was w\u00e4re eine Alternative: Alternativen sind Violinplots, die die Dichte der Verteilung detaillierter darstellen.</li> <li>Wie erstelle ich es:<ul> <li>Plotly: Boxplot</li> <li>Seaborn: boxplot</li> <li>Matplotlib: boxplot</li> </ul> </li> </ul>"},{"location":"data_analysis/data_visualization/#3-streudiagramm-scatter-plot","title":"3. Streudiagramm (Scatter Plot)","text":"<ul> <li>Was zeichnet es aus: Streudiagramme visualisieren die Beziehung zwischen zwei numerischen Variablen. Jeder Punkt auf dem Diagramm repr\u00e4sentiert einen Datenpunkt und die Achsen die Werte der jeweiligen Variablen.</li> <li>Wann kann ich es besonders gut anwenden: Ideal, um Korrelationen oder Muster zwischen zwei Variablen zu erkennen (z. B. ob ein linearer Zusammenhang besteht).</li> <li>Was w\u00e4re eine Alternative: Alternativen sind Liniendiagramme (wenn eine Reihenfolge wichtig ist) oder Heatmaps f\u00fcr dichtere Datens\u00e4tze.</li> <li>Wie erstelle ich es:<ul> <li>Plotly: Scatter Plot</li> <li>Seaborn: scatterplot</li> <li>Matplotlib: scatter</li> </ul> </li> </ul>"},{"location":"data_analysis/data_visualization/#4-liniendiagramm","title":"4. Liniendiagramm","text":"<ul> <li>Was zeichnet es aus: Ein Liniendiagramm verbindet Datenpunkte mittels Linien und veranschaulicht so den Verlauf einer oder mehrerer Reihen \u00fcber eine geordnete Achse (h\u00e4ufig Zeit).</li> <li>Wann kann ich es besonders gut anwenden: Liniendiagramme eignen sich gut f\u00fcr Zeitreihenanalysen und um Trends oder Zyklen in Daten darzustellen.</li> <li>Was w\u00e4re eine Alternative: Eine Alternative w\u00e4re ein Fl\u00e4chendiagramm (Area Chart), das die gleiche Information zeigt, jedoch mit gef\u00fcllten Bereichen.</li> <li>Wie erstelle ich es:<ul> <li>Plotly: Line Chart</li> <li>Seaborn: lineplot</li> <li>Matplotlib: plot</li> </ul> </li> </ul>"},{"location":"data_analysis/data_visualization/#5-violinplot","title":"5. Violinplot","text":"<ul> <li>Was zeichnet es aus: Der Violinplot kombiniert Elemente von Boxplots und Dichteplots und zeigt die Verteilung der Daten sowie deren Dichte entlang einer Achse.</li> <li>Wann kann ich es besonders gut anwenden: Violinplots sind n\u00fctzlich, wenn man die Verteilung und die Dichte einer Variablen gleichzeitig darstellen und dabei symmetrische oder bimodale Muster aufzeigen m\u00f6chte.</li> <li>Was w\u00e4re eine Alternative: Alternativen sind der Boxplot (f\u00fcr eine weniger detaillierte Darstellung der Dichte) oder der Dichteplot.</li> <li>Wie erstelle ich es:<ul> <li>Plotly: Violin Plot</li> <li>Seaborn: violinplot</li> <li>Matplotlib: violinplot</li> </ul> </li> </ul>"},{"location":"data_analysis/data_visualization/#6-heatmap","title":"6. Heatmap","text":"<ul> <li>Was zeichnet es aus: Heatmaps verwenden Farbschattierungen, um die Intensit\u00e4t von Werten in einer Matrix zu visualisieren. H\u00e4ufig werden sie verwendet, um Korrelationen zwischen Variablen darzustellen.</li> <li>Wann kann ich es besonders gut anwenden: Heatmaps sind besonders n\u00fctzlich, um Korrelationen zwischen vielen Variablen darzustellen, etwa bei der Analyse von Korrelationen in gro\u00dfen Datens\u00e4tzen.</li> <li>Was w\u00e4re eine Alternative: Eine Alternative zur Heatmap ist das Paarplot, das Korrelationen in Scatterplot-Matrizen zeigt.</li> <li>Wie erstelle ich es:<ul> <li>Plotly: Heatmap</li> <li>Seaborn: heatmap</li> <li>Matplotlib: imshow</li> </ul> </li> </ul>"},{"location":"data_analysis/data_visualization/#7-paarplot-pair-plot","title":"7. Paarplot (Pair Plot)","text":"<ul> <li>Was zeichnet es aus: Ein Paarplot visualisiert alle Kombinationen der Variablenpaare in einem Datensatz als Streudiagramme und eignet sich hervorragend zur Erkennung von Korrelationen und Zusammenh\u00e4ngen in multidimensionalen Datens\u00e4tzen.</li> <li>Wann kann ich es besonders gut anwenden: Wenn es darum geht, Korrelationen und Beziehungen zwischen mehreren Variablen gleichzeitig zu analysieren, ist der Paarplot das Mittel der Wahl.</li> <li>Was w\u00e4re eine Alternative: Alternativen sind Scatterplot-Matrizen oder einzelne Heatmaps.</li> <li>Wie erstelle ich es:<ul> <li>Seaborn: pairplot</li> <li>Plotly: scatter_matrix</li> </ul> </li> </ul>"},{"location":"data_analysis/data_visualization/#8-balkendiagramm-bar-chart","title":"8. Balkendiagramm (Bar Chart)","text":"<ul> <li>Was zeichnet es aus: Ein Balkendiagramm stellt kategorische Daten durch rechteckige Balken dar, deren L\u00e4nge proportional zur H\u00e4ufigkeit oder zum Wert einer Kategorie ist.</li> <li>Wann kann ich es besonders gut anwenden: Balkendiagramme sind ideal, um die Gr\u00f6\u00dfenverh\u00e4ltnisse verschiedener Kategorien direkt miteinander zu vergleichen.</li> <li>Was w\u00e4re eine Alternative: Eine Alternative zum Balkendiagramm ist das S\u00e4ulendiagramm, das \u00e4hnliche Daten vertikal darstellt.</li> <li>Wie erstelle ich es:<ul> <li>Plotly: Bar Chart</li> <li>Seaborn: barplot</li> <li>Matplotlib: bar</li> </ul> </li> </ul>"},{"location":"data_analysis/data_visualization/#9-kreisdiagramm-pie-chart","title":"9, Kreisdiagramm (Pie Chart)","text":"<ul> <li>Was zeichnet es aus: Ein Kreisdiagramm teilt einen Kreis in Segmente, die jeweils eine Kategorie repr\u00e4sentieren, und zeigt deren Anteile am Gesamtwert.</li> <li>Wann kann ich es besonders gut anwenden: Kreisdiagramme sind gut geeignet, um den Anteil einer Kategorie im Verh\u00e4ltnis zum Ganzen darzustellen, allerdings nicht bei mehr als f\u00fcnf Kategorien.</li> <li>Was w\u00e4re eine Alternative: Alternativen sind Donut Charts oder gestapelte Balkendiagramme, die Verh\u00e4ltnisse auch darstellen k\u00f6nnen.</li> <li>Wie erstelle ich es:<ul> <li>Plotly: Pie Chart</li> <li>Matplotlib: pie</li> </ul> </li> </ul>"},{"location":"data_analysis/data_visualization/#10-flachendiagramm-area-chart","title":"10. Fl\u00e4chendiagramm (Area Chart)","text":"<ul> <li>Was zeichnet es aus: Ein Fl\u00e4chendiagramm ist eine Variation des Liniendiagramms, bei dem der Bereich unter der Linie ausgef\u00fcllt ist, um den Wert \u00fcber eine geordnete Dimension (h\u00e4ufig Zeit) zu verdeutlichen.</li> <li>Wann kann ich es besonders gut anwenden: Fl\u00e4chendiagramme eignen sich besonders, um kumulative Daten oder Zeitreihendaten zu visualisieren, bei denen der Bereich unter der Linie eine Bedeutung hat.</li> <li>Was w\u00e4re eine Alternative: Liniendiagramme (ohne ausgef\u00fcllte Fl\u00e4che) oder Stapeldiagramme, wenn mehrere Kategorien dargestellt werden sollen.</li> <li>Wie erstelle ich es:<ul> <li>Plotly: Area Chart</li> <li>Matplotlib: fill_between</li> </ul> </li> </ul>"},{"location":"data_analysis/data_visualization/#11-blasendiagramm-bubble-chart","title":"11. Blasendiagramm (Bubble Chart)","text":"<ul> <li>Was zeichnet es aus: Ein Blasendiagramm ist ein erweitertes Streudiagramm, bei dem die Gr\u00f6\u00dfe der Punkte eine zus\u00e4tzliche Dimension repr\u00e4sentiert, meist einen numerischen Wert.</li> <li>Wann kann ich es besonders gut anwenden: Blasendiagramme sind n\u00fctzlich, wenn man drei Dimensionen (zwei f\u00fcr die Achsen und eine f\u00fcr die Punktgr\u00f6\u00dfe) auf einmal visualisieren m\u00f6chte.</li> <li>Was w\u00e4re eine Alternative: Alternativen sind einfache Streudiagramme (f\u00fcr zwei Dimensionen) oder Heatmaps f\u00fcr Korrelationen zwischen mehreren Variablen.</li> <li>Wie erstelle ich es:<ul> <li>Plotly: Bubble Chart</li> <li>Matplotlib: scatter (mit Gr\u00f6\u00dfenparameter)</li> </ul> </li> </ul>"},{"location":"data_analysis/data_visualization/#12-gestapeltes-balkendiagramm-stacked-bar-chart","title":"12. Gestapeltes Balkendiagramm (Stacked Bar Chart)","text":"<ul> <li>Was zeichnet es aus: Ein gestapeltes Balkendiagramm zeigt mehrere Kategorien \u00fcbereinander in einem Balken, wodurch die Gesamtmenge sowie die Verteilung der Teilkategorien auf einen Blick erkennbar sind.</li> <li>Wann kann ich es besonders gut anwenden: Diese Darstellung eignet sich f\u00fcr Daten, bei denen man sowohl die Gesamtwerte als auch die Anteile der Unterkategorien gleichzeitig zeigen m\u00f6chte.</li> <li>Was w\u00e4re eine Alternative: Eine Alternative ist das 100%-gestapelte Balkendiagramm, bei dem die Balken normalisiert werden, sodass sie immer bis 100% reichen.</li> <li>Wie erstelle ich es:<ul> <li>Plotly: Stacked Bar Chart</li> <li>Matplotlib: bar (mit stacked=True)</li> </ul> </li> </ul>"},{"location":"data_analysis/eda_steps/","title":"Deskriptive Datenanalyse","text":""},{"location":"data_analysis/eda_steps/#einfuhrung-in-die-explorative-datenanalyse-eda","title":"Einf\u00fchrung in die Explorative Datenanalyse (EDA)","text":"<p>In der explorativen Datenanalyse (EDA) werfen wir einen ersten Blick auf unsere Daten, um die wichtigsten Merkmale des Datensatzes zu erfassen und zu visualisieren. Diese Schritte helfen uns, die Daten besser zu verstehen, Hypothesen zu formulieren, und schlie\u00dflich die Voraussetzungen f\u00fcr eine fundierte Analyse oder ein Modell zu schaffen. Im Folgenden f\u00fchren wir dich Schritt f\u00fcr Schritt durch die f\u00fcnf wesentlichen Schritte einer EDA.</p>"},{"location":"data_analysis/eda_steps/#schritt-1-verstehen-der-datenstruktur","title":"Schritt 1: Verstehen der Datenstruktur","text":""},{"location":"data_analysis/eda_steps/#11-datentypen-und-strukturen","title":"1.1 Datentypen und Strukturen","text":"<ul> <li>Daten werden meist in numerische, kategorische und Zeitreihen-Daten unterteilt, und jeder Typ braucht besondere Techniken und Darstellungen.</li> <li>Nutze Datenstrukturen wie DataFrames (Pandas in Python) und Arrays (NumPy), um Daten zu laden und effizient zu verarbeiten.</li> </ul>"},{"location":"data_analysis/eda_steps/#12-kontext-und-quelle-der-daten","title":"1.2 Kontext und Quelle der Daten","text":"<ul> <li>Woher kommen die Daten? Wann wurden sie gesammelt? Notiere den Kontext, um den Zweck der Daten zu verstehen und m\u00f6gliche Verzerrungen zu erkennen.</li> </ul>"},{"location":"data_analysis/eda_steps/#13-metadaten-und-dokumentation","title":"1.3 Metadaten und Dokumentation","text":"<ul> <li>Wenn verf\u00fcgbar, \u00fcberpr\u00fcfe die Metadaten und die Dokumentation zum Datensatz. Das liefert oft wichtige Hinweise zu den Variablen und der Datenqualit\u00e4t.</li> </ul>"},{"location":"data_analysis/eda_steps/#schritt-2-datenbereinigung-und-vorverarbeitung","title":"Schritt 2: Datenbereinigung und Vorverarbeitung","text":""},{"location":"data_analysis/eda_steps/#21-fehlende-werte-behandeln","title":"2.1 Fehlende Werte behandeln","text":"<ul> <li>Optionen: Zeilen oder Spalten entfernen, fehlende Werte mit Durchschnitt oder Median auff\u00fcllen, oder Vorhersagemodelle nutzen, um die Werte zu sch\u00e4tzen.</li> </ul>"},{"location":"data_analysis/eda_steps/#22-duplikate-entfernen","title":"2.2 Duplikate entfernen","text":"<ul> <li>Doppelte Eintr\u00e4ge in deinem Datensatz verf\u00e4lschen Ergebnisse. Entferne sie, um die Integrit\u00e4t deiner Daten zu sichern.</li> </ul>"},{"location":"data_analysis/eda_steps/#23-transformation-der-daten","title":"2.3 Transformation der Daten","text":"<ul> <li>Normalisierung: Skalierung der Daten f\u00fcr eine einheitliche Verteilung.</li> <li>Kodierung: Umwandlung von Kategorien in numerische Werte (z.B. One-Hot-Codierung).</li> <li>Zeitreihen: Konvertiere Datumsangaben f\u00fcr eine korrekte Analyse von Zeitdaten.</li> </ul>"},{"location":"data_analysis/eda_steps/#24-ausreier-identifizieren-und-behandeln","title":"2.4 Ausrei\u00dfer identifizieren und behandeln","text":"<ul> <li>Verwende Diagramme wie Boxplots oder statistische Tests (z.B. Z-Scores), um Ausrei\u00dfer zu finden. Entschlie\u00dfe, ob du diese entfernst oder transformierst.</li> </ul>"},{"location":"data_analysis/eda_steps/#schritt-3-univariate-analyse-einzelne-variablen","title":"Schritt 3: Univariate Analyse (Einzelne Variablen)","text":""},{"location":"data_analysis/eda_steps/#31-deskriptive-statistik","title":"3.1 Deskriptive Statistik","text":"<ul> <li>F\u00fchre grundlegende statistische Berechnungen durch (Mittelwert, Median, Modus), um zentrale Tendenzen und Streuung der Variablen zu verstehen.</li> </ul>"},{"location":"data_analysis/eda_steps/#32-visualisierungen-univariate","title":"3.2 Visualisierungen (Univariate)","text":"<ul> <li>Histogramme: Zum Veranschaulichen der Verteilung.</li> <li>Boxplots: Zeigen Verteilung und Ausrei\u00dfer.</li> <li>Balkendiagramme: F\u00fcr die H\u00e4ufigkeit von Kategorien.</li> </ul>"},{"location":"data_analysis/eda_steps/#schritt-4-bivariate-und-multivariate-analyse-zusammenhange","title":"Schritt 4: Bivariate und multivariate Analyse (Zusammenh\u00e4nge)","text":""},{"location":"data_analysis/eda_steps/#41-bivariate-analyse","title":"4.1 Bivariate Analyse","text":"<ul> <li>Scatterplots: Visualisiert die Beziehung zwischen zwei numerischen Variablen.</li> <li>Korrelationsmatrix: Berechnet Korrelationen zwischen Variablen.</li> </ul>"},{"location":"data_analysis/eda_steps/#42-multivariate-analyse","title":"4.2 Multivariate Analyse","text":"<ul> <li>Paarplots und Heatmaps: Zeigen Beziehungen mehrerer Variablen.</li> <li>PCA (Principal Component Analysis): Reduziert die Komplexit\u00e4t der Daten.</li> <li>Clustering: Zum Gruppieren von Datenpunkten basierend auf \u00c4hnlichkeiten.</li> </ul>"},{"location":"data_analysis/eda_steps/#43-wechselwirkungen-erkennen","title":"4.3 Wechselwirkungen erkennen","text":"<ul> <li>Achte auf Wechselwirkungen zwischen Variablen, die m\u00f6glicherweise auf komplexe Abh\u00e4ngigkeiten oder Multikollinearit\u00e4t hinweisen.</li> </ul>"},{"location":"data_analysis/eda_steps/#schritt-5-einblicke-und-schlussfolgerungen-ziehen","title":"Schritt 5: Einblicke und Schlussfolgerungen ziehen","text":""},{"location":"data_analysis/eda_steps/#51-wichtige-erkenntnisse-zusammenfassen","title":"5.1 Wichtige Erkenntnisse zusammenfassen","text":"<ul> <li>Notiere die wichtigsten Muster und Ausrei\u00dfer. Welche Einblicke bieten deine Analysen?</li> </ul>"},{"location":"data_analysis/eda_steps/#52-visuelles-storytelling","title":"5.2 Visuelles Storytelling","text":"<ul> <li>Nutze ansprechende Diagramme, um deine Ergebnisse zu kommunizieren. Erstelle klare Visualisierungen, die die Daten einfach und verst\u00e4ndlich zusammenfassen.</li> </ul>"},{"location":"data_analysis/eda_steps/#53-entscheidungen-treffen","title":"5.3 Entscheidungen treffen","text":"<ul> <li>Verwende die Erkenntnisse der EDA, um Hypothesen zu formulieren, strategische Entscheidungen zu treffen oder n\u00e4chste Schritte f\u00fcr Modellierungen festzulegen.</li> </ul>"},{"location":"data_analysis/eda_steps/#54-dokumentation","title":"5.4 Dokumentation","text":"<ul> <li>Dokumentiere die EDA-Schritte, Methoden und Ergebnisse vollst\u00e4ndig, damit der Prozess nachvollziehbar und wiederholbar bleibt.</li> </ul>"},{"location":"data_analysis/intro/","title":"Data Analysis","text":"<p>Die Datenanalyse ist ein wesentlicher Bestandteil der Data Science und bildet die Grundlage f\u00fcr datengetriebene Entscheidungen und Erkenntnisse. Sie umfasst verschiedene Analyseans\u00e4tze, die uns helfen, aus Daten Informationen zu gewinnen und diese zielgerichtet zu nutzen. Im Wesentlichen l\u00e4sst sich die Datenanalyse in vier Hauptarten unterteilen, die jeweils unterschiedliche Fragen beantworten und unterschiedliche Methoden anwenden.</p>"},{"location":"data_analysis/intro/#die-vier-arten-der-datenanalyse","title":"Die vier Arten der Datenanalyse","text":""},{"location":"data_analysis/intro/#1-deskriptive-analyse","title":"1. Deskriptive Analyse","text":"<p>Die deskriptive Analyse stellt die Daten so dar, wie sie sind, und beantwortet grundlegende Fragen wie \u201eWas ist passiert?\u201c. Sie wird h\u00e4ufig als erster Schritt verwendet, um Muster, Trends und Verteilungen zu verstehen. Hierbei kommen statistische Kennzahlen wie Mittelwerte, H\u00e4ufigkeiten und Standardabweichungen zum Einsatz, um ein klares Bild der Daten zu gewinnen.</p>"},{"location":"data_analysis/intro/#2-diagnostische-analyse","title":"2. Diagnostische Analyse","text":"<p>Die diagnostische Analyse geht tiefer und versucht zu erkl\u00e4ren, \u201eWarum ist etwas passiert?\u201c. Sie analysiert Zusammenh\u00e4nge und identifiziert Muster, die auf Ursachen f\u00fcr bestimmte Beobachtungen hinweisen. Mit Methoden wie Hypothesentests und Regressionsanalysen lassen sich Korrelationen und Abh\u00e4ngigkeiten aufzeigen, die helfen, den Hintergrund eines Ph\u00e4nomens zu verstehen.</p>"},{"location":"data_analysis/intro/#3-pradiktive-analyse","title":"3. Pr\u00e4diktive Analyse","text":"<p>Die pr\u00e4diktive Analyse richtet den Blick in die Zukunft und versucht zu prognostizieren, \u201eWas wird wahrscheinlich passieren?\u201c. Durch die Analyse historischer Daten und das Erkennen von Mustern lassen sich Modelle entwickeln, die Vorhersagen erm\u00f6glichen. H\u00e4ufige Werkzeuge sind hier maschinelle Lernverfahren wie Regression, Entscheidungsb\u00e4ume und Zeitreihenanalysen, die bei der Ermittlung zuk\u00fcnftiger Trends und Entwicklungen unterst\u00fctzen.</p>"},{"location":"data_analysis/intro/#4-praskriptive-analyse","title":"4. Pr\u00e4skriptive Analyse","text":"<p>Die pr\u00e4skriptive Analyse baut auf den Erkenntnissen der pr\u00e4diktiven Analyse auf und stellt die Frage \u201eWas sollte getan werden?\u201c. Sie nutzt die erlangten Einsichten, um Handlungsempfehlungen zu geben und optimale Entscheidungen zu treffen. Typische Methoden umfassen Optimierungsalgorithmen und Entscheidungsmodelle, die konkrete Vorschl\u00e4ge zur Vorgehensweise bieten.</p> <p>Diese vier Analysearten bilden den Rahmen der Datenanalyse und stehen oft miteinander in Verbindung: Deskriptive und diagnostische Analysen geben Einblick in bestehende Daten, w\u00e4hrend pr\u00e4diktive und pr\u00e4skriptive Analysen darauf abzielen, zuk\u00fcnftige Ergebnisse vorherzusagen und zu beeinflussen. Gemeinsam erm\u00f6glichen sie eine umfassende Datenanalyse und er\u00f6ffnen wertvolle Perspektiven f\u00fcr datenbasierte Entscheidungen. </p> <p>Um die Datenlage verst\u00e4ndlich darzustellen und erste wertvolle Erkenntnisse zu gewinnen, werden in der deskriptiven Statistik grundlegende Methoden und Visualisierungen angewendet. Dieser n\u00e4chste Abschnitt konzentriert sich darauf, wie Datenmuster erkennbar gemacht und wichtige Verteilungen sowie Beziehungen visualisiert werden k\u00f6nnen.</p> <p>Im Fokus stehen dabei die verschiedenen Arten von Diagrammen und Graphen, die in der deskriptiven Statistik g\u00e4ngige Praxis sind, wie etwa Histogramme, Boxplots und Streudiagramme. Zus\u00e4tzlich betrachten wir einige der wichtigsten Bibliotheken in Python \u2013 darunter Matplotlib, Seaborn und Plotly.</p>"},{"location":"data_analysis/tasks/","title":"Aufgaben","text":""},{"location":"data_analysis/tasks/#stromerzeugung-in-deutschland-in-2024","title":"Stromerzeugung in Deutschland in 2024","text":"<ol> <li> <p>\u00dcberpr\u00fcfe die nachfolgenden Hypothesen</p> <ul> <li>Am Tag gibt es mehr Sonnenenergie als Nachts</li> <li>Solar erzeugt viel Strom, wenn Wind wenig Strom erzeugt</li> <li>Die Zeitumstellung hat einen Einfluss auf die Energieerzeugung (Zusatz)</li> <li>Im Jahr liefern die erneuerbaren Energien mehr Strom als die konventionellen Energien</li> <li>Die Energieerzeugung durch M\u00fcll ist die meiste Zeit konstant</li> <li>Kohle und Gas werden vor allem dann ben\u00f6tigt, wenn man eine hohe Last hat</li> <li>Wenn viel Energie aus erneuerbaren Energiequellen gewonnen wird, wird weniger Energie konventionell gewonnen</li> <li>In den D\u00e4mmerungen wird mehr Windstrom produziert, als in der Mittags- und Mitternachts-zeit (Zusatz)</li> </ul> </li> <li> <p>Erstelle eine weitere Hypothese. Suche dir einen Partner, welcher diese Hypothese \u00fcberpr\u00fcft.</p> </li> </ol>"},{"location":"data_analysis/understand_datasets/","title":"Datens\u00e4tze verstehen","text":""},{"location":"data_analysis/understand_datasets/#einstieg-in-einen-neuen-datensatz-tipps-zur-orientierung","title":"Einstieg in einen neuen Datensatz: Tipps zur Orientierung","text":"<p>Bevor wir mit einer tiefgehenden Analyse beginnen, ist es hilfreich, uns zuerst einen \u00dcberblick \u00fcber den Datensatz zu verschaffen. Ein strukturierter Ansatz sorgt daf\u00fcr, dass wir die Eigenschaften, St\u00e4rken und eventuelle Herausforderungen der Daten fr\u00fchzeitig erkennen. Hier sind einige wichtige Schritte und Ans\u00e4tze, die dir dabei helfen, dich in einem neuen Datensatz zurechtzufinden:</p>"},{"location":"data_analysis/understand_datasets/#1-datenquellen-und-kontext-prufen","title":"1. Datenquellen und Kontext pr\u00fcfen","text":"<ul> <li>Datenquelle und Kontext verstehen: Woher kommen die Daten? Stammt der Datensatz aus einer verl\u00e4sslichen Quelle? Was ist das Ziel oder der Anwendungsfall der Daten? </li> <li>Datenfelder und Bedeutung: Kl\u00e4re die Bedeutung der einzelnen Felder. Die Dokumentation oder begleitende Metadaten enthalten oft wertvolle Informationen zu den Variablen.</li> </ul>"},{"location":"data_analysis/understand_datasets/#2-uberblick-uber-die-struktur-des-datensatzes-gewinnen","title":"2. \u00dcberblick \u00fcber die Struktur des Datensatzes gewinnen","text":"<ul> <li>Spalten und Datentypen inspizieren: Nutze Python-Methoden wie <code>df.info()</code> oder <code>df.dtypes</code>, um schnell festzustellen, welche Datentypen (numerisch, kategorisch, zeitbasiert) vorhanden sind.</li> <li>Erste Einblicke in die Werte: Verwende <code>df.head()</code> und <code>df.tail()</code>, um dir die ersten und letzten Zeilen anzuschauen und ein Gef\u00fchl f\u00fcr die Dateninhalte zu bekommen. So erkennst du auch direkt m\u00f6gliche Anomalien.</li> </ul>"},{"location":"data_analysis/understand_datasets/#3-zentrale-kennzahlen-berechnen","title":"3. Zentrale Kennzahlen berechnen","text":"<ul> <li>Deskriptive Statistik: Nutze Methoden wie <code>df.describe()</code>, um erste statistische Einblicke zu gewinnen. Kennzahlen wie Mittelwert, Minimum, Maximum und Standardabweichung zeigen wichtige Eigenschaften der numerischen Variablen.</li> <li>Verteilung der Daten pr\u00fcfen: Einfache Histogramme oder Boxplots helfen, die Verteilung der Daten besser zu verstehen und Ausrei\u00dfer oder Verzerrungen zu identifizieren.</li> </ul>"},{"location":"data_analysis/understand_datasets/#4-datenqualitat-beurteilen","title":"4. Datenqualit\u00e4t beurteilen","text":"<ul> <li>Fehlende Werte analysieren: Pr\u00fcfe, ob und wo Werte fehlen. Methoden wie <code>df.isnull().sum()</code> zeigen, wie h\u00e4ufig fehlende Werte in jeder Spalte auftreten.</li> <li>Duplikate finden und bewerten: Mit <code>df.duplicated().sum()</code> erkennst du doppelte Eintr\u00e4ge, die du ggf. bereinigen solltest.</li> </ul>"},{"location":"data_analysis/understand_datasets/#5-erste-zusammenhange-und-muster-erkennen","title":"5. Erste Zusammenh\u00e4nge und Muster erkennen","text":"<ul> <li>Korrelationen untersuchen: Ein schneller Blick auf die Korrelationen numerischer Variablen (z.B. mit <code>df.corr()</code>) kann n\u00fctzliche Hinweise auf Beziehungen zwischen Variablen geben.</li> <li>Kategoriale und numerische Variablen analysieren: Einfache Kreuztabellen (<code>pd.crosstab()</code>) oder Gruppenstatistiken (<code>df.groupby()</code>) helfen, Verteilungen und Zusammenh\u00e4nge zu verstehen.</li> </ul>"},{"location":"data_analysis/understand_datasets/#6-erste-visualisierungen-nutzen","title":"6. Erste Visualisierungen nutzen","text":"<ul> <li>Visualisierung der wichtigsten Felder: Nutze Diagramme wie Balkendiagramme, Histogramme oder Boxplots, um zentrale Eigenschaften der Variablen visuell darzustellen. Dies hilft, Muster zu erkennen und eine Grundlage f\u00fcr detailliertere Analysen zu schaffen.</li> </ul>"},{"location":"fastapi/data_models/","title":"Datenmodelle mit Pydantic in FastAPI","text":"<p>Datenmodelle sind ein wesentlicher Bestandteil von APIs, da sie definieren, welche Art von Daten zwischen dem Server und den Clients ausgetauscht werden kann. In FastAPI \u00fcbernimmt die Bibliothek Pydantic diese Aufgabe, indem sie Python-Datenstrukturen in valide JSON-Objekte verwandelt. Pydantic bietet starke Validierungs- und Typisierungsfunktionen und erm\u00f6glicht es uns, Daten mit minimalem Aufwand zu strukturieren und zu validieren.</p>"},{"location":"fastapi/data_models/#warum-pydantic","title":"Warum Pydantic?","text":"<p>Pydantic ist eine zentrale Bibliothek in FastAPI, da sie eine einfache M\u00f6glichkeit bietet, Daten zu validieren und zu serialisieren. Mit Pydantic k\u00f6nnen wir sicherstellen, dass die Daten, die von einem Client gesendet werden oder an diesen zur\u00fcckgegeben werden, den gew\u00fcnschten Typen entsprechen. Dies verringert potenzielle Fehler und macht den Code robuster und lesbarer.</p>"},{"location":"fastapi/data_models/#typisierung-und-validierung","title":"Typisierung und Validierung","text":"<p>Pydantic nutzt Python\u2019s Typannotation, um die Struktur der Daten zu definieren und die Validierung basierend auf den angegebenen Datentypen durchzuf\u00fchren. Wenn du also ein Modell mit Feldern wie <code>str</code>, <code>int</code> oder <code>EmailStr</code> erstellst, stellt Pydantic sicher, dass die Daten den entsprechenden Typen entsprechen.</p> <p>Ein Beispiel f\u00fcr eine einfache Typisierung:</p> <pre><code>from pydantic import BaseModel, EmailStr\n\nclass User(BaseModel):\n    name: str\n    email: EmailStr\n    age: int\n</code></pre> <p>Hier sorgt Pydantic daf\u00fcr, dass die <code>email</code>-Adresse als g\u00fcltige E-Mail-Adresse formatiert ist, w\u00e4hrend der Name ein String und das Alter eine Ganzzahl ist.</p>"},{"location":"fastapi/data_models/#unterstutzte-datentypen","title":"Unterst\u00fctzte Datentypen","text":"<p>Pydantic unterst\u00fctzt eine breite Palette von Datentypen, um unterschiedliche Anwendungsf\u00e4lle abzudecken. Zu den wichtigsten geh\u00f6ren:</p> <ul> <li>Primitive Typen: <code>str</code>, <code>int</code>, <code>float</code>, <code>bool</code></li> <li>Spezielle Typen: <code>EmailStr</code>, <code>UUID</code>, <code>IPv4</code>, <code>IPv6</code></li> <li>Optionale Felder: <code>Optional[T]</code>, um Felder als optional zu kennzeichnen</li> <li>Listen und Tupel: <code>List[T]</code>, <code>Tuple[T, ...]</code></li> <li>Datum und Uhrzeit: <code>datetime</code>, <code>date</code>, <code>time</code></li> </ul>"},{"location":"fastapi/data_models/#erstellen-eines-einfachen-datenmodells","title":"Erstellen eines einfachen Datenmodells","text":"<p>Schauen wir uns ein einfaches Beispiel an: Wir m\u00f6chten eine API, die Benutzerdaten wie <code>name</code>, <code>email</code> und <code>age</code> verarbeitet. Mit Pydantic definieren wir die Struktur der zu verarbeitenden Daten.</p> <pre><code>from pydantic import BaseModel, EmailStr\n\nclass User(BaseModel):\n    name: str\n    email: EmailStr\n    age: int\n</code></pre> <p>In diesem Modell wird <code>name</code> als <code>str</code>, <code>email</code> als <code>EmailStr</code> und <code>age</code> als <code>int</code> validiert. Die <code>EmailStr</code>-Typisierung stellt sicher, dass die Eingabe eine g\u00fcltige E-Mail-Adresse ist.</p>"},{"location":"fastapi/data_models/#aufgabe","title":"Aufgabe","text":"<p>Erweitere das <code>User</code>-Modell, indem du ein optionales Feld f\u00fcr <code>city</code> hinzuf\u00fcgst, das einen Standardwert hat, zum Beispiel <code>\"Unbekannt\"</code>. F\u00fcge au\u00dferdem ein Feld <code>is_active</code> hinzu, das einen <code>bool</code>-Wert erwartet und standardm\u00e4\u00dfig auf <code>True</code> gesetzt ist.</p>"},{"location":"fastapi/data_models/#modell-in-fastapi-einbinden","title":"Modell in FastAPI einbinden","text":"<p>Nachdem wir das Datenmodell mit Pydantic erstellt haben, wollen wir es in einer FastAPI-Anwendung verwenden. FastAPI erm\u00f6glicht es uns, Pydantic-Modelle direkt in Routen zu integrieren. So k\u00f6nnen wir sicherstellen, dass alle eingehenden Daten den definierten Anforderungen entsprechen.</p> <pre><code>from fastapi import FastAPI\nfrom pydantic import BaseModel, EmailStr\nfrom typing import Optional\n\napp = FastAPI()\n\nclass User(BaseModel):\n    name: str\n    email: EmailStr\n    age: int\n    city: Optional[str] = \"Unbekannt\"\n    is_active: bool = True\n\n@app.post(\"/users/\")\nasync def create_user(user: User):\n    return {\"user\": user}\n</code></pre> <p>In diesem Beispiel haben wir die Route <code>/users/</code> erstellt, die einen <code>POST</code>-Request erwartet. FastAPI \u00fcbernimmt die Validierung der Daten und gibt die g\u00fcltigen Daten als Antwort zur\u00fcck.</p>"},{"location":"fastapi/data_models/#aufgabe_1","title":"Aufgabe","text":"<p>Teste die <code>/users/</code>-Route mit einem API-Client wie Postman. Sende ein JSON-Objekt, das die Felder <code>name</code>, <code>email</code> und <code>age</code> enth\u00e4lt, und \u00fcberpr\u00fcfe, wie FastAPI das Modell validiert und zur\u00fcckgibt.</p>"},{"location":"fastapi/data_models/#validierung-und-fehlerbehandlung","title":"Validierung und Fehlerbehandlung","text":"<p>FastAPI und Pydantic bieten umfangreiche M\u00f6glichkeiten, Daten zu validieren. Wenn Daten nicht den Anforderungen entsprechen, gibt FastAPI eine detaillierte Fehlermeldung zur\u00fcck, die den Entwickler genau dar\u00fcber informiert, welcher Wert nicht g\u00fcltig ist.</p>"},{"location":"fastapi/data_models/#beispiel-fur-eine-validierung","title":"Beispiel f\u00fcr eine Validierung:","text":"<pre><code>from pydantic import BaseModel, Field\n\nclass User(BaseModel):\n    name: str = Field(..., min_length=2, max_length=50)\n    email: EmailStr\n    age: int = Field(..., gt=0, le=120)  # Altersbeschr\u00e4nkung\n    city: Optional[str] = \"Unbekannt\"\n    is_active: bool = True\n</code></pre> <p>In diesem Beispiel haben wir Einschr\u00e4nkungen f\u00fcr das Feld <code>name</code> (minimale und maximale L\u00e4nge) und <code>age</code> (gr\u00f6\u00dfer als 0 und maximal 120 Jahre) definiert.</p> <p>Wenn jemand beispielsweise eine ung\u00fcltige E-Mail-Adresse sendet oder das Alter zu hoch ist, gibt FastAPI eine Fehlerantwort zur\u00fcck.</p>"},{"location":"fastapi/data_models/#aufgabe_2","title":"Aufgabe","text":"<p>Teste das Modell in einer einfachen <code>POST</code>-Route, indem du es in FastAPI integrierst. \u00dcbermittle verschiedene <code>name</code>- und <code>age</code>-Werte und beobachte die Fehlermeldungen, die FastAPI automatisch generiert, wenn die Validierungen fehlschlagen.</p>"},{"location":"fastapi/data_models/#eigene-validierungen","title":"Eigene Validierungen","text":"<p>Pydantic erm\u00f6glicht es, benutzerdefinierte Validierungen hinzuzuf\u00fcgen. Wenn du komplexere Anforderungen hast, die \u00fcber die Standardvalidierung hinausgehen, kannst du eigene Validierungsfunktionen implementieren.</p> <p>Ein Beispiel f\u00fcr eine benutzerdefinierte Validierung:</p> <pre><code>from pydantic import validator\n\nclass User(BaseModel):\n    name: str\n    email: EmailStr\n    age: int\n\n    @validator(\"name\")\n    def name_must_not_contain_numbers(cls, v):\n        if any(char.isdigit() for char in v):\n            raise ValueError(\"Name darf keine Zahlen enthalten\")\n        return v\n</code></pre> <p>In diesem Beispiel wird eine Validierung f\u00fcr das Feld <code>name</code> hinzugef\u00fcgt, die sicherstellt, dass der Name keine Zahlen enth\u00e4lt.</p>"},{"location":"fastapi/data_models/#aufgabe_3","title":"Aufgabe","text":"<p>Erweitere den <code>name</code>-Validator, sodass er auch sicherstellt, dass der Name keine Sonderzeichen enth\u00e4lt. Teste den Validator, indem du Namen wie \u201eJohn@Doe\u201c und \u201e123Peter\u201c eingibst und \u00fcberpr\u00fcfe die Fehlermeldungen.</p>"},{"location":"fastapi/first_steps/","title":"Erste Schritte mit FastAPI: HTTP-Routen","text":"<p>In diesem Abschnitt sehen wir uns die Grundlagen von HTTP-Routen und die Erstellung von API-Endpunkte in fastapi an. HTTP-Routen sind das Herzst\u00fcck jeder API. Sie definieren die verschiedenen Wege, auf denen Clients (wie Webbrowser oder mobile Apps) mit unserem Server kommunizieren k\u00f6nnen. FastAPI macht es besonders einfach, diese Routen zu erstellen und f\u00fcr verschiedene Anfragen zu konfigurieren.</p>"},{"location":"fastapi/first_steps/#unsere-erste-route-get","title":"Unsere erste Route: <code>GET</code>","text":"<p>Beginnen wir noch einmal mit einer grundlegenden Route, die eine Nachricht an den Client zur\u00fcckgibt. Der <code>GET</code>-Anfragetyp ist der einfachste und am h\u00e4ufigsten verwendete HTTP-Methodentyp \u2013 er ruft einfach Daten ab, ohne dass eine \u00c4nderung am Server oder in der Datenbank vorgenommen wird.</p> <pre><code>from fastapi import FastAPI\n\napp = FastAPI()\n\n@app.get(\"/\")\nasync def read_root():\n    return {\"message\": \"Hello, World!\"}\n</code></pre> <p>In diesem Beispiel erstellen wir eine FastAPI-Instanz namens <code>app</code>. Dann definieren wir eine <code>GET</code>-Route mit dem Endpunkt <code>/</code>, die einfach \u201eHello, World!\u201c zur\u00fcckgibt. Der <code>@app.get(\"/\")</code>-Dekorator sagt FastAPI, dass dieser Endpunkt auf <code>GET</code>-Anfragen wartet.</p>"},{"location":"fastapi/first_steps/#aufgabe","title":"Aufgabe","text":"<p>Starte den Server und rufe <code>http://127.0.0.1:8000/</code> in deinem Browser oder einem API-Client wie Postman auf. Siehst du die Nachricht? Experimentiere, indem du den Text \u00e4nderst. Teste, was passiert, wenn du den R\u00fcckgabewert ver\u00e4nderst \u2013 z. B. durch eine andere Nachricht oder eine Zahl.</p>"},{"location":"fastapi/first_steps/#parameter-in-der-url","title":"Parameter in der URL","text":"<p>Ein h\u00e4ufiges Szenario ist, dass wir Daten dynamisch basierend auf der Anfrage bereitstellen wollen. Nehmen wir an, wir m\u00f6chten eine Nachricht zur\u00fcckgeben, die den Namen des Benutzers enth\u00e4lt. Dazu f\u00fcgen wir einen URL-Parameter hinzu, der in die Route integriert wird.</p> <pre><code>@app.get(\"/hello/{name}\")\nasync def read_item(name: str):\n    return {\"message\": f\"Hello, {name}!\"}\n</code></pre> <p>Hier erstellen wir eine <code>GET</code>-Route mit einem dynamischen Segment <code>{name}</code>, das wir im Funktionsparameter <code>name</code> auffangen. Wenn wir <code>http://127.0.0.1:8000/hello/Alex</code> aufrufen, erhalten wir die Antwort: \u201eHello, Alex!\u201c</p>"},{"location":"fastapi/first_steps/#aufgabe_1","title":"Aufgabe","text":"<p>Erweitere die Route, um eine zweite Variable wie <code>age</code> oder <code>city</code> aufzunehmen. Erstelle eine Antwort, die beide Parameter in einem Begr\u00fc\u00dfungssatz verwendet. Teste verschiedene Namen und Werte, um zu sehen, wie FastAPI die Eingaben verarbeitet.</p>"},{"location":"fastapi/first_steps/#verwendung-von-http-methoden-post","title":"Verwendung von HTTP-Methoden: <code>POST</code>","text":"<p>Neben <code>GET</code> gibt es noch weitere HTTP-Methoden wie <code>POST</code>, <code>PUT</code> und <code>DELETE</code>, die alle unterschiedliche Zwecke erf\u00fcllen. <code>POST</code>-Anfragen werden typischerweise verwendet, um Daten an den Server zu senden, z. B. zum Erstellen eines neuen Eintrags.</p> <p>Angenommen, wir m\u00f6chten eine einfache Route erstellen, bei der der Benutzer eine Nachricht an den Server senden kann. Dabei nutzen wir die Methode <code>POST</code>, um die Nachricht vom Client entgegenzunehmen und eine Best\u00e4tigung zur\u00fcckzugeben.</p> <pre><code>from pydantic import BaseModel\n\nclass Message(BaseModel):\n    content: str\n\n@app.post(\"/send-message/\")\nasync def create_message(message: Message):\n    return {\"received_message\": message.content}\n</code></pre> <p>In diesem Beispiel erstellen wir ein Modell <code>Message</code> mit dem Attribut <code>content</code>, das eine Zeichenkette ist. Das <code>@app.post(\"/send-message/\")</code> zeigt FastAPI, dass dieser Endpunkt eine <code>POST</code>-Anfrage erwartet. Der Inhalt wird in Form eines JSON-Objekts vom Client gesendet und in das <code>message</code>-Objekt des Typs <code>Message</code> umgewandelt. Anschlie\u00dfend geben wir die empfangene Nachricht als Best\u00e4tigung zur\u00fcck.</p>"},{"location":"fastapi/first_steps/#aufgabe_2","title":"Aufgabe","text":"<p>Teste diese <code>POST</code>-Route mit einem API-Client wie Postman oder durch einen Browser-Extension. Sende eine JSON-Nachricht wie <code>{\"content\": \"Dies ist meine erste Nachricht\"}</code>. Experimentiere mit verschiedenen Nachrichten und \u00fcberpr\u00fcfe, wie FastAPI die Antwort generiert.</p>"},{"location":"fastapi/first_steps/#arbeiten-mit-query-parametern","title":"Arbeiten mit Query-Parametern","text":"<p>Neben Routenparametern und <code>POST</code>-Daten bietet FastAPI die M\u00f6glichkeit, Query-Parameter zu verwenden. Diese Art von Parametern befindet sich in der URL nach einem <code>?</code> und wird h\u00e4ufig f\u00fcr zus\u00e4tzliche, optionale Informationen genutzt. Beispielsweise m\u00f6chten wir eine Route erstellen, bei der der Benutzer seinen Namen als Query-Parameter senden kann, ohne ihn in der URL selbst zu definieren.</p> <pre><code>@app.get(\"/greet/\")\nasync def greet_user(name: str = \"Gast\"):\n    return {\"message\": f\"Hallo, {name}!\"}\n</code></pre> <p>In dieser Route verwenden wir den Query-Parameter <code>name</code>, der standardm\u00e4\u00dfig \u201eGast\u201c ist, falls kein Wert \u00fcbergeben wird. Wenn wir <code>http://127.0.0.1:8000/greet/?name=Lisa</code> aufrufen, erhalten wir die Antwort \u201eHallo, Lisa!\u201c.</p>"},{"location":"fastapi/first_steps/#aufgabe_3","title":"Aufgabe","text":"<p>Experimentiere mit der URL und dem <code>name</code>-Parameter. Probiere verschiedene Namen und teste, was passiert, wenn du den Parameter wegl\u00e4sst. Erweitere das Beispiel, indem du weitere optionale Query-Parameter hinzuf\u00fcgst, etwa <code>age</code> oder <code>city</code>.</p>"},{"location":"fastapi/intro/","title":"Einf\u00fchrung in FastAPI","text":""},{"location":"fastapi/intro/#was-ist-fastapi","title":"Was ist FastAPI?","text":"<p>FastAPI ist ein Web-Framework zur Entwicklung von APIs mit Python. Es wurde f\u00fcr den Einsatz in produktionskritischen Anwendungen entwickelt und zeichnet sich durch eine hohe Performance und einfache Handhabung aus.</p>"},{"location":"fastapi/intro/#kernmerkmale-von-fastapi","title":"Kernmerkmale von FastAPI","text":"<ul> <li>Automatische Dokumentation: FastAPI generiert automatisch interaktive API-Dokumentationen (Swagger UI, Redoc) auf Basis des OpenAPI-Standards.</li> <li>Asynchrone Verarbeitung: Unterst\u00fctzung f\u00fcr <code>async</code> und <code>await</code> erleichtert das Handling von asynchronen Aufgaben und macht FastAPI ideal f\u00fcr Anwendungen mit hohen Anforderungen an die Skalierbarkeit.</li> <li>Einfache Validierung: FastAPI verwendet Pydantic zur Validierung und Serialisierung von Daten, wodurch die Datenintegrit\u00e4t automatisch gesichert wird.</li> </ul>"},{"location":"fastapi/intro/#beispiel-einer-fastapi-anwendung","title":"Beispiel einer FastAPI-Anwendung","text":"<p>Hier eine grundlegende FastAPI-Anwendung, die einen \"Hello World\"-Endpunkt bereitstellt:</p> <pre><code>from fastapi import FastAPI\n\napp = FastAPI()\n\n@app.get(\"/\")\nasync def root():\n    return {\"message\": \"Hello, World!\"}\n</code></pre>"},{"location":"fastapi/intro/#installation","title":"Installation","text":"<p>FastAPI kann \u00fcber <code>pip</code> installiert werden:</p> <pre><code>pip install fastapi[all]\n</code></pre> <p>Zus\u00e4tzlich wird ein ASGI-Server wie uvicorn ben\u00f6tigt, um die Anwendung zu starten:</p> <pre><code>pip install uvicorn\nuvicorn main:app --reload\n</code></pre>"},{"location":"fastapi/intro/#fastapi-vs-flask","title":"FastAPI vs. Flask","text":"<p>FastAPI und Flask sind beide beliebte Python-Frameworks zur API-Entwicklung, unterscheiden sich jedoch erheblich in ihrer Funktionsweise und ihrem Anwendungsbereich.</p>"},{"location":"fastapi/intro/#hauptunterschiede","title":"Hauptunterschiede","text":"Merkmal Flask FastAPI Asynchronit\u00e4t Unterst\u00fctzung nur mit zus\u00e4tzlichen Bibliotheken wie <code>flask-async</code> Eingebaute Unterst\u00fctzung f\u00fcr <code>async</code> und <code>await</code> Performance Moderate Geschwindigkeit Hohe Performance durch asynchrone Architektur Datenvalidierung Keine eingebaute Validierung, zus\u00e4tzliche Bibliotheken wie <code>marshmallow</code> erforderlich Integrierte Validierung mit Pydantic Dokumentation Keine automatische Dokumentation Automatische Generierung von Swagger UI und Redoc Ideal f\u00fcr Einfache APIs und Anwendungen APIs mit hoher Leistung und komplexen Datenvalidierungsanforderungen"},{"location":"fastapi/intro/#wann-man-fastapi-verwenden-sollte","title":"Wann man FastAPI verwenden sollte","text":"<ul> <li>Wenn hohe Performance und Skalierbarkeit gefordert sind.</li> <li>F\u00fcr APIs, die asynchrone Verarbeitung ben\u00f6tigen.</li> <li>Bei Projekten, in denen die automatische Dokumentation n\u00fctzlich ist.</li> </ul>"},{"location":"fastapi/intro/#wann-flask-geeigneter-ist","title":"Wann Flask geeigneter ist","text":"<ul> <li>Bei einfachen Anwendungen oder Prototypen, die nur grundlegende Funktionalit\u00e4ten ben\u00f6tigen.</li> <li>Wenn Asynchronit\u00e4t und Performance keine kritischen Anforderungen sind.</li> </ul>"},{"location":"fastapi/sqlmodel/","title":"Arbeiten mit SQLModel in FastAPI","text":"<p>SQLModel ist eine Bibliothek, die SQLAlchemy und Pydantic kombiniert. Sie erm\u00f6glicht es dir, Datenbankmodelle in FastAPI zu integrieren, ohne auf umfangreiche ORM-Definitionen verzichten zu m\u00fcssen. SQLModel baut auf SQLAlchemy auf und verwendet Pydantic zur Validierung der Daten, sodass du Datenbankmodelle erstellen und gleichzeitig die Vorteile der Datenvalidierung von Pydantic nutzen kannst.</p>"},{"location":"fastapi/sqlmodel/#was-ist-sqlmodel","title":"Was ist SQLModel?","text":"<p>SQLModel ist eine Bibliothek, die es erm\u00f6glicht, SQL-Datenbanken in FastAPI-Anwendungen zu integrieren. Sie stellt eine einfache Schnittstelle zur Verf\u00fcgung, die es dir erm\u00f6glicht, sowohl Datenbankmodelle zu definieren als auch mit der Datenbank zu interagieren \u2013 alles in einer sauberen und einheitlichen API.</p> <p>SQLModel erm\u00f6glicht die Definition von Pydantic-Modellen, die gleichzeitig auch Datenbankmodelle sind. Das bedeutet, dass du nur ein Modell schreiben musst, um sowohl mit der Datenbank als auch mit FastAPI zu arbeiten.</p>"},{"location":"fastapi/sqlmodel/#installation","title":"Installation","text":"<pre><code>pip install sqlmodel\n</code></pre>"},{"location":"fastapi/sqlmodel/#erstellen-eines-einfachen-sqlmodel-datenmodells","title":"Erstellen eines einfachen SQLModel-Datenmodells","text":"<p>Ein SQLModel-Datenmodell ist sehr \u00e4hnlich wie ein Pydantic-Modell, aber mit zus\u00e4tzlichen SQLAlchemy-Features wie <code>Field</code> und <code>Relationship</code> f\u00fcr die Datenbankinteraktion. </p> <pre><code>from sqlmodel import Field, SQLModel\n\nclass User(SQLModel, table=True):\n    id: int = Field(default=None, primary_key=True)\n    name: str\n    email: str\n    age: int\n</code></pre> <p>In diesem Beispiel haben wir ein einfaches User-Modell erstellt, das die Felder <code>id</code>, <code>name</code>, <code>email</code> und <code>age</code> enth\u00e4lt. Das <code>table=True</code>-Attribut signalisiert, dass dieses Modell eine Tabelle in der Datenbank repr\u00e4sentiert.</p>"},{"location":"fastapi/sqlmodel/#aufgabe","title":"Aufgabe","text":"<p>Erstelle ein weiteres Datenmodell, das <code>Product</code>-Daten mit den Feldern <code>name</code>, <code>price</code> und <code>description</code> speichert. Vergiss nicht, die <code>id</code> als Prim\u00e4rschl\u00fcssel hinzuzuf\u00fcgen.</p>"},{"location":"fastapi/sqlmodel/#datenbankverbindungen-und-sessions","title":"Datenbankverbindungen und Sessions","text":"<p>Eine der wichtigsten Aufgaben beim Arbeiten mit SQLModel ist die Verbindung zur Datenbank und die Verwaltung von Sessions, um Transaktionen durchzuf\u00fchren. SQLModel baut auf SQLAlchemy auf, sodass wir die gleiche Methode verwenden, um die Verbindung zur Datenbank herzustellen.</p>"},{"location":"fastapi/sqlmodel/#verbindung-zur-datenbank-herstellen","title":"Verbindung zur Datenbank herstellen","text":"<p>Du kannst SQLModel mit einer SQLite-Datenbank oder einer anderen unterst\u00fctzten Datenbank wie PostgreSQL oder MySQL verwenden. Hier ist ein einfaches Beispiel f\u00fcr die Verbindung zu einer SQLite-Datenbank:</p> <pre><code>from sqlmodel import create_engine, Session\n\n# Erstelle die Verbindung zur SQLite-Datenbank\nengine = create_engine(\"sqlite:///database.db\")\n\n# Erstelle die Tabellen in der Datenbank (falls sie noch nicht existieren)\nSQLModel.metadata.create_all(engine)\n</code></pre>"},{"location":"fastapi/sqlmodel/#arbeiten-mit-sessions","title":"Arbeiten mit Sessions","text":"<p>Um mit der Datenbank zu interagieren, musst du eine Session erstellen, die es dir erm\u00f6glicht, Datens\u00e4tze zu lesen, zu schreiben und zu aktualisieren.</p> <pre><code>from sqlmodel import Session\n\n# \u00d6ffne eine Session, um mit der Datenbank zu interagieren\nwith Session(engine) as session:\n    # Beispiel: Hinzuf\u00fcgen eines neuen Benutzers\n    user = User(name=\"John Doe\", email=\"johndoe@example.com\", age=30)\n    session.add(user)\n    session.commit()\n</code></pre>"},{"location":"fastapi/sqlmodel/#aufgabe_1","title":"Aufgabe","text":"<p>Erstelle eine neue <code>Product</code>-Instanz, f\u00fcge sie zur Datenbank hinzu und best\u00e4tige die Transaktion mit <code>session.commit()</code>.</p>"},{"location":"fastapi/sqlmodel/#crud-operationen-mit-sqlmodel","title":"CRUD-Operationen mit SQLModel","text":"<p>SQLModel macht es einfach, grundlegende CRUD-Operationen (Create, Read, Update, Delete) auf Datenbanken durchzuf\u00fchren. Nachdem du die Datenbankverbindung und das Modell eingerichtet hast, kannst du Datens\u00e4tze einfach erstellen und abfragen.</p>"},{"location":"fastapi/sqlmodel/#erstellen-eines-datensatzes-create","title":"Erstellen eines Datensatzes (Create)","text":"<pre><code>with Session(engine) as session:\n    new_user = User(name=\"Jane Doe\", email=\"janedoe@example.com\", age=25)\n    session.add(new_user)\n    session.commit()\n</code></pre>"},{"location":"fastapi/sqlmodel/#lesen-von-datensatzen-read","title":"Lesen von Datens\u00e4tzen (Read)","text":"<pre><code>with Session(engine) as session:\n    user = session.query(User).filter(User.name == \"Jane Doe\").first()\n    print(user)\n</code></pre>"},{"location":"fastapi/sqlmodel/#aktualisieren-von-datensatzen-update","title":"Aktualisieren von Datens\u00e4tzen (Update)","text":"<pre><code>with Session(engine) as session:\n    user = session.query(User).filter(User.name == \"Jane Doe\").first()\n    if user:\n        user.age = 26\n        session.commit()\n</code></pre>"},{"location":"fastapi/sqlmodel/#loschen-von-datensatzen-delete","title":"L\u00f6schen von Datens\u00e4tzen (Delete)","text":"<pre><code>with Session(engine) as session:\n    user = session.query(User).filter(User.name == \"Jane Doe\").first()\n    if user:\n        session.delete(user)\n        session.commit()\n</code></pre>"},{"location":"fastapi/sqlmodel/#aufgabe_2","title":"Aufgabe","text":"<p>Erweitere das <code>User</code>-Modell um ein <code>address</code>-Feld und f\u00fchre eine <code>UPDATE</code>-Operation durch, um die Adresse eines bestimmten Benutzers zu \u00e4ndern.</p>"},{"location":"fastapi/sqlmodel/#integration-von-sqlmodel-mit-fastapi","title":"Integration von SQLModel mit FastAPI","text":"<p>Ein gro\u00dfer Vorteil von SQLModel ist, dass es nahtlos mit FastAPI integriert werden kann. Du kannst SQLModel-Modelle direkt in deine FastAPI-Routen einbinden, um API-Endpunkte zu erstellen, die mit der Datenbank interagieren.</p>"},{"location":"fastapi/sqlmodel/#beispiel-fur-einen-fastapi-endpunkt-mit-sqlmodel","title":"Beispiel f\u00fcr einen FastAPI-Endpunkt mit SQLModel","text":"<pre><code>from fastapi import FastAPI\nfrom sqlmodel import Session, select\n\napp = FastAPI()\n\n@app.post(\"/users/\")\nasync def create_user(user: User):\n    with Session(engine) as session:\n        session.add(user)\n        session.commit()\n    return {\"user\": user}\n</code></pre> <p>In diesem Beispiel haben wir eine <code>POST</code>-Route erstellt, die ein <code>User</code>-Objekt entgegennimmt und es in die Datenbank speichert.</p>"},{"location":"fastapi/sqlmodel/#aufgabe_3","title":"Aufgabe","text":"<p>Erstelle eine <code>GET</code>-Route, die alle Benutzer aus der Datenbank abruft und zur\u00fcckgibt.</p>"},{"location":"fastapi/task/","title":"Vorhersage des Einkommens basierend auf dem Adult Income Dataset","text":"<p>In dieser Aufgabe wirst du das Adult Income Dataset verwenden, um ein einfaches API-Modell zu erstellen, das die Wahrscheinlichkeit vorhersagt, ob jemand mehr als 50.000 USD j\u00e4hrlich verdient, basierend auf verschiedenen demografischen Merkmalen. Du wirst SQLModel f\u00fcr das Speichern der Daten in einer Datenbank und FastAPI f\u00fcr die API-Erstellung verwenden.</p>"},{"location":"fastapi/task/#aufgabe-1-datenvorbereitung-und-verstandnis","title":"Aufgabe 1: Datenvorbereitung und -verst\u00e4ndnis","text":"<p>Lade das Adult Income Dataset herunter und analysiere die Spalten. Erstelle ein Streamlit Dashboard oder Jupyter Notebook, um den Datensatz visuell aufzubereiten. Der Datensatz enth\u00e4lt folgende Merkmale:</p> <ul> <li><code>age</code>: Alter der Person</li> <li><code>workclass</code>: Arbeitsverh\u00e4ltnis (z.B. privat, \u00f6ffentlich)</li> <li><code>fnlwgt</code>: Gewicht (repr\u00e4sentiert die Anzahl der Menschen, die in der Stichprobe repr\u00e4sentiert sind)</li> <li><code>education</code>: H\u00f6chster Bildungsabschluss</li> <li><code>education-num</code>: Bildung in numerischer Form</li> <li><code>marital-status</code>: Familienstand</li> <li><code>occupation</code>: Beruf</li> <li><code>relationship</code>: Beziehung zum Haushaltsvorstand</li> <li><code>race</code>: Rasse</li> <li><code>sex</code>: Geschlecht</li> <li><code>capital-gain</code>: Kapitalgewinne</li> <li><code>capital-loss</code>: Kapitalverlust</li> <li><code>hours-per-week</code>: Arbeitsstunden pro Woche</li> <li><code>native-country</code>: Geburtsland</li> <li><code>income</code>: Einkommen (mehr als 50K oder weniger)</li> </ul>"},{"location":"fastapi/task/#aufgabe-2-erstellen-eines-sqlmodel-datenmodells","title":"Aufgabe 2: Erstellen eines SQLModel-Datenmodells","text":"<p>Erstelle ein <code>User</code>-Datenmodell mit SQLModel, das die oben genannten Merkmale repr\u00e4sentiert. Definiere auch den Typ der Merkmale (z. B. <code>age</code> als Integer, <code>education</code> als String). </p>"},{"location":"fastapi/task/#aufgabe-3-api-erstellung-mit-fastapi","title":"Aufgabe 3: API-Erstellung mit FastAPI","text":"<p>Erstelle eine FastAPI-Anwendung, die folgende Funktionalit\u00e4ten bietet:</p> <ol> <li> <p>Daten hinzuf\u00fcgen: Erstelle eine POST-Route, die es erm\u00f6glicht, neue Benutzerdaten hinzuzuf\u00fcgen. Die Route sollte alle Merkmale des Datensatzes akzeptieren.</p> </li> <li> <p>Daten abfragen: Erstelle eine GET-Route, um alle gespeicherten Benutzerdaten abzurufen.</p> </li> <li> <p>Einkommensvorhersage: Erstelle eine weitere POST-Route, die basierend auf den eingegebenen demografischen Merkmalen vorhersagt, ob das Einkommen mehr als 50.000 USD betr\u00e4gt oder nicht. Diese Route sollte ein Modell wie z. B. einen einfachen Entscheidungsbaum verwenden (der zuvor auf den Daten trainiert wurde), um die Vorhersage zu treffen.</p> </li> </ol>"},{"location":"fastapi/task/#aufgabe-4-testen-der-api-mit-postman-oder-einem-anderen-api-client","title":"Aufgabe 4: Testen der API mit Postman oder einem anderen API-Client","text":"<ol> <li> <p>Daten hinzuf\u00fcgen: Verwende die POST-Route <code>/add-user</code> (oder eine benannte Route) zum Hinzuf\u00fcgen eines neuen Datensatzes.</p> </li> <li> <p>Daten abfragen: Verwende die GET-Route <code>/users</code>, um alle gespeicherten Benutzer abzurufen.</p> </li> <li> <p>Einkommensvorhersage: Teste die <code>/predict-income/</code>-Route, indem du verschiedene Werte f\u00fcr <code>age</code>, <code>hours-per-week</code> und andere Merkmale eingibst.</p> </li> </ol>"},{"location":"fastapi/task/#aufgabe-5-zusatz-analyse-und-modellverbesserung","title":"Aufgabe 5 (Zusatz): Analyse und Modellverbesserung","text":"<p>Nachdem du die API erstellt hast, analysiere die Vorhersageergebnisse und \u00fcberlege, wie du das Modell verbessern kannst</p>"},{"location":"week1/data_related_roles/","title":"Rollen im Zusammenhang mit Datenbasierten Projekten","text":"<p>Datenbasierte Projekte sind oft komplex und erfordern die Zusammenarbeit von Fachkr\u00e4ften mit unterschiedlichen Kompetenzen. Eine klare Definition der Rollen und Verantwortlichkeiten hilft, die Zusammenarbeit effizient zu gestalten und das Projektergebnis zu optimieren. In diesem Kapitel schauen wir uns die eine der Rollen in datenbasierten Projekten an und welche Aufgaben sie typischerweise \u00fcbernehmen.</p>"},{"location":"week1/data_related_roles/#1-data-scientist","title":"1. Data Scientist","text":"<p>Aufgaben:</p> <ul> <li>Entwicklung und Anwendung von Algorithmen zur Datenanalyse und Modellierung.</li> <li>Experimentieren mit Machine-Learning-Modellen und Auswahl der besten Ans\u00e4tze.</li> <li>Kommunikation der Ergebnisse in Form von Berichten oder Visualisierungen.</li> </ul> <p>Ben\u00f6tigte Kompetenzen:</p> <ul> <li>Starke mathematische und statistische Kenntnisse.</li> <li>Erfahrung mit Programmiersprachen wie Python oder R.</li> <li>F\u00e4higkeit, komplexe Zusammenh\u00e4nge einfach zu erkl\u00e4ren.</li> </ul>"},{"location":"week1/data_related_roles/#2-data-engineer","title":"2. Data Engineer","text":"<p>Aufgaben:</p> <ul> <li>Aufbau und Wartung der Dateninfrastruktur (z. B. Datenbanken, Pipelines).</li> <li>Sicherstellung, dass Daten f\u00fcr Analysen verf\u00fcgbar und qualitativ hochwertig sind.</li> <li>Optimierung von Datenspeichersystemen hinsichtlich Leistung und Skalierbarkeit.</li> </ul> <p>Ben\u00f6tigte Kompetenzen:</p> <ul> <li>Erfahrung mit ETL-Prozessen (Extract, Transform, Load).</li> <li>Kenntnisse in Datenbanktechnologien wie SQL, NoSQL oder Big Data Tools.</li> <li>Verst\u00e4ndnis von Cloud-Diensten wie AWS, Azure oder Google Cloud.</li> </ul>"},{"location":"week1/data_related_roles/#3-business-analyst","title":"3. Business Analyst","text":"<p>Aufgaben:</p> <ul> <li>Verstehen der gesch\u00e4ftlichen Anforderungen und \u00dcbersetzung in datenbezogene Fragestellungen.</li> <li>Interpretation der Analyseergebnisse und Ableitung gesch\u00e4ftlicher Entscheidungen.</li> <li>Identifikation von Potenzialen zur Verbesserung von Prozessen oder Produkten.</li> </ul> <p>Ben\u00f6tigte Kompetenzen:</p> <ul> <li>Kenntnisse \u00fcber die Branche und den spezifischen Gesch\u00e4ftskontext.</li> <li>F\u00e4higkeit, mit Stakeholdern zu kommunizieren und deren Anforderungen zu priorisieren.</li> <li>Grundlagen in Datenvisualisierung und Reporting-Tools (z. B. Tableau, Power BI).</li> </ul>"},{"location":"week1/data_related_roles/#4-data-analyst","title":"4. Data Analyst","text":"<p>Aufgaben:</p> <ul> <li>Analyse von Daten, um Muster, Trends und Anomalien zu erkennen.</li> <li>Erstellung von Dashboards und Berichten, die wichtige Erkenntnisse darstellen.</li> <li>Zusammenarbeit mit anderen Teams, um datengest\u00fctzte Entscheidungen zu treffen.</li> </ul> <p>Ben\u00f6tigte Kompetenzen:</p> <ul> <li>Starke Kenntnisse in Statistik und Datenanalyse.</li> <li>Erfahrung mit Tools wie Excel, SQL oder spezialisierten Analyseplattformen.</li> <li>F\u00e4higkeit, Daten effektiv zu visualisieren.</li> </ul>"},{"location":"week1/data_related_roles/#5-project-manager","title":"5. Project Manager","text":"<p>Aufgaben:</p> <ul> <li>Planung und Steuerung des Projekts, um sicherzustellen, dass es p\u00fcnktlich und im Budget bleibt.</li> <li>Koordination zwischen verschiedenen Teams und Stakeholdern.</li> <li>Identifikation und Management von Risiken.</li> </ul> <p>Ben\u00f6tigte Kompetenzen:</p> <ul> <li>Erfahrung in agilen Methoden wie Scrum oder Kanban.</li> <li>Kommunikations- und Probleml\u00f6sungsf\u00e4higkeiten.</li> <li>Verst\u00e4ndnis der grundlegenden Aspekte datenbasierter Projekte..</li> </ul>"},{"location":"week1/data_related_roles/#7-machine-learning-engineer","title":"7. Machine Learning Engineer","text":"<p>Aufgaben:</p> <ul> <li>Entwicklung, Optimierung und Deployment von Machine-Learning-Modellen.</li> <li>Integration von Modellen in produktive Systeme.</li> <li>Sicherstellung der Skalierbarkeit und Leistung von Modellen im Echtbetrieb.</li> </ul> <p>Ben\u00f6tigte Kompetenzen:</p> <ul> <li>Kenntnisse in Softwareentwicklung und Machine Learning.</li> <li>Erfahrung mit Frameworks wie TensorFlow, PyTorch oder Scikit-Learn.</li> <li>Verst\u00e4ndnis von MLOps-Tools und -Prozessen.</li> </ul>"},{"location":"week1/data_science_buzzwords/","title":"Begriffe aus dem Data-Science-Kosmos","text":"<p>In der Welt der Datenwissenschaft begegnen wir h\u00e4ufig Begriffen, die eng miteinander verwandt sind, aber unterschiedliche Bedeutungen haben. Dieses Kapitel bietet dir eine strukturierte \u00dcbersicht, um Klarheit in die wichtigsten Begriffe zu bringen und sie einzuordnen.</p>"},{"location":"week1/data_science_buzzwords/#1-grundlagenbegriffe","title":"1. Grundlagenbegriffe","text":""},{"location":"week1/data_science_buzzwords/#kunstliche-intelligenz-ki","title":"K\u00fcnstliche Intelligenz (KI)","text":"<ul> <li>Definition: KI umfasst Systeme, die menschen\u00e4hnliches Denken und Verhalten simulieren, z. B. durch Probleml\u00f6sung, Mustererkennung oder Entscheidungsfindung.</li> <li>Einsatzgebiete: Sprach\u00fcbersetzung, Bilderkennung, autonome Fahrzeuge.</li> <li>Beziehung zu anderen Begriffen: Oberbegriff f\u00fcr Machine Learning und Deep Learning.</li> </ul>"},{"location":"week1/data_science_buzzwords/#machine-learning-ml","title":"Machine Learning (ML)","text":"<ul> <li>Definition: Teilbereich der KI, der es Systemen erlaubt, aus Daten zu lernen und Vorhersagen oder Entscheidungen zu treffen, ohne explizit programmiert zu sein.</li> <li>Typen von ML:</li> <li>Supervised Learning (z. B. Regression, Klassifikation)</li> <li>Unsupervised Learning (z. B. Clustering, Dimensionsreduktion)</li> <li>Reinforcement Learning (z. B. Steuerung von Robotern)</li> <li>Beispiel: Vorhersage von Hauspreisen basierend auf historischen Daten.</li> </ul>"},{"location":"week1/data_science_buzzwords/#deep-learning-dl","title":"Deep Learning (DL)","text":"<ul> <li>Definition: Spezialisierter Bereich des Machine Learning, der auf neuronalen Netzwerken mit vielen Schichten basiert.</li> <li>Anwendung: Bilderkennung, Spracherkennung, Generative Modelle (z. B. Text-to-Image-Generierung).</li> <li>Beziehung zu ML und KI: Deep Learning ist ein Teilbereich von ML und somit auch der KI zuzuordnen.</li> </ul>"},{"location":"week1/data_science_buzzwords/#statistik","title":"Statistik","text":"<ul> <li>Definition: Wissenschaft der Datenanalyse zur Gewinnung von Informationen und zur Entscheidungsfindung.</li> <li>Rolle im Data Science: Grundlage vieler Algorithmen im ML (z. B. lineare Regression, Hypothesentests).</li> <li>Abgrenzung zu ML: Statistik betont Erkl\u00e4rbarkeit und Hypothesentestung, w\u00e4hrend ML prim\u00e4r auf Vorhersage fokussiert ist.</li> </ul>"},{"location":"week1/data_science_buzzwords/#2-datenverarbeitung-und-strukturen","title":"2. Datenverarbeitung und -strukturen","text":""},{"location":"week1/data_science_buzzwords/#datenpipeline","title":"Datenpipeline","text":"<ul> <li>Definition: Prozess, der Daten von der Sammlung \u00fcber die Verarbeitung bis zur Analyse automatisiert.</li> <li>Bestandteile: Extraktion, Transformation, Laden (ETL-Prozesse).</li> <li>Beispiel: Daten von IoT-Ger\u00e4ten werden gesammelt, bereinigt und an ein Analysemodell \u00fcbergeben.</li> </ul>"},{"location":"week1/data_science_buzzwords/#big-data","title":"Big Data","text":"<ul> <li>Definition: Verarbeitung und Analyse extrem gro\u00dfer Datenmengen, die mit traditionellen Methoden nicht effizient bearbeitet werden k\u00f6nnen.</li> <li>Merkmale (die 4 Vs): Volume, Velocity, Variety, Veracity.</li> <li>Technologien: Hadoop, Spark.</li> </ul>"},{"location":"week1/data_science_buzzwords/#cloud-computing","title":"Cloud Computing","text":"<ul> <li>Definition: Bereitstellung von Rechenleistung, Speicher und Anwendungen \u00fcber das Internet.</li> <li>Bezug zu Data Science: Erm\u00f6glicht skalierbare Speicherung und Verarbeitung gro\u00dfer Datenmengen.</li> <li>Beispiele: Amazon Web Services (AWS), Google Cloud Platform (GCP).</li> </ul>"},{"location":"week1/data_science_buzzwords/#3-analysemethoden-und-modellierung","title":"3. Analysemethoden und Modellierung","text":""},{"location":"week1/data_science_buzzwords/#explorative-datenanalyse-eda","title":"Explorative Datenanalyse (EDA)","text":"<ul> <li>Definition: Erste Analyse von Daten zur Entdeckung von Mustern, Anomalien und Zusammenh\u00e4ngen.</li> <li>Werkzeuge: Pandas, Seaborn, Matplotlib.</li> <li>Beziehung zu ML: Bereitet Daten f\u00fcr ML-Modelle vor.</li> </ul>"},{"location":"week1/data_science_buzzwords/#feature-engineering","title":"Feature Engineering","text":"<ul> <li>Definition: Erstellung neuer Variablen (Features), um die Leistung eines Modells zu verbessern.</li> <li>Beispiele: Normalisierung, One-Hot-Encoding, Feature-Auswahl.</li> <li>Rolle in ML: Entscheidender Faktor f\u00fcr Modellgenauigkeit.</li> </ul>"},{"location":"week1/data_science_buzzwords/#preprocessing","title":"Preprocessing","text":"<ul> <li>Definition: Vorbereitung der Rohdaten f\u00fcr die Modellierung.</li> <li>Beispiele: Skalierung, fehlende Werte behandeln, Kategorisierung.</li> <li>Unterschied zu Feature Engineering: Beim Preprocessing geht es um generelle Datenvorbereitung, w\u00e4hrend Feature Engineering neue Informationen aus bestehenden Daten erzeugt.</li> </ul>"},{"location":"week1/data_science_buzzwords/#data-cleaning","title":"Data Cleaning","text":"<ul> <li>Definition: Identifikation und Korrektur fehlerhafter, unvollst\u00e4ndiger oder ungenauer Daten.</li> <li>Beispiele: Entfernen von Duplikaten, Bereinigung von Textdaten, Umgang mit Ausrei\u00dfern.</li> <li>Rolle im Workflow: Grundlegender Schritt, der sicherstellt, dass Daten verl\u00e4sslich sind.</li> </ul>"},{"location":"week1/data_science_buzzwords/#4-technologische-begriffe","title":"4. Technologische Begriffe","text":""},{"location":"week1/data_science_buzzwords/#apis-application-programming-interfaces","title":"APIs (Application Programming Interfaces)","text":"<ul> <li>Definition: Schnittstellen, die den Zugriff auf Funktionen oder Daten eines Systems erlauben.</li> <li>Rolle in Projekten: Verbindung von Modellen mit Anwendungen oder Datenquellen.</li> <li>Beispiel: REST-APIs, FastAPI.</li> </ul>"},{"location":"week1/data_science_buzzwords/#datenbanken","title":"Datenbanken","text":"<ul> <li>Relational: Daten in Tabellenform (z. B. MySQL, PostgreSQL).</li> <li>NoSQL: Flexible Speicherung von unstrukturierten Daten (z. B. MongoDB, Cassandra).</li> <li>Relevanz: Speicherung, Abfrage und Analyse von Daten.</li> </ul>"},{"location":"week1/data_science_buzzwords/#containerization","title":"Containerization","text":"<ul> <li>Definition: Verpackung einer Anwendung mit ihrer gesamten Abh\u00e4ngigkeit in einem Container.</li> <li>Technologie: Docker.</li> <li>Nutzen: Erleichtert die Bereitstellung und Skalierung.</li> </ul>"},{"location":"week1/data_science_buzzwords/#5-abgrenzung-ahnlicher-begriffe","title":"5. Abgrenzung \u00e4hnlicher Begriffe","text":""},{"location":"week1/data_science_buzzwords/#ki-vs-ml-vs-dl-vs-statistik","title":"KI vs. ML vs. DL vs. Statistik","text":"<ul> <li>KI: Der umfassende Rahmen.</li> <li>ML: L\u00e4sst Maschinen aus Daten lernen.</li> <li>DL: Subdisziplin, spezialisiert auf neuronale Netzwerke.</li> <li>Statistik: Fokus auf Dateninterpretation und Erkl\u00e4rbarkeit.</li> </ul>"},{"location":"week1/data_science_buzzwords/#big-data-vs-datenanalyse","title":"Big Data vs. Datenanalyse","text":"<ul> <li>Big Data: Umgang mit der Masse an Daten.</li> <li>Datenanalyse: Fokus auf das Verstehen und die Interpretation der Daten.</li> </ul>"},{"location":"week1/data_science_buzzwords/#feature-engineering-vs-preprocessing-vs-data-cleaning","title":"Feature Engineering vs. Preprocessing vs. Data Cleaning","text":"<ul> <li>Feature Engineering: Erstellung neuer Features, die die Modellleistung verbessern (z. B. Kombination von Variablen, Erstellung von Zeitmerkmalen).</li> <li>Preprocessing: Allgemeine Datenvorbereitung wie Normalisierung, Skalierung oder Encoding.</li> <li>Data Cleaning: Beseitigung von Problemen in den Rohdaten, wie fehlerhaften Eintr\u00e4gen oder fehlenden Werten.</li> </ul>"},{"location":"week1/goals/","title":"Ziele","text":"<p>Ich kann...</p> <p>... die Schritte einer EDA beschreiben. ... missing values in einem Datensatz identifizieren. ... die Verteilung von Daten beschreiben. ... die Verteilung von Daten visualisieren. ... missing values in einem Datensatz bearbeiten. ... Ausrei\u00dfer in einem Datensatz identifizieren. ... Ausrei\u00dfer in einem Datensatz bearbeiten. ... Duplikate in einem Datensatz identifizieren. ... Duplikate in einem Datensatz bearbeiten. ... zwischen verschiedenen Berufsgruppen im Zusammenhangn mit Data Science unterscheiden. ... zwischen verschiedenen Begiffen im Zusammenhang mit Data Science unterscheiden.</p>"},{"location":"week1/overview/","title":"Grundlagen von Data Science","text":"<p>Data Science ist ein interdisziplin\u00e4res Feld, das datengetriebene Erkenntnisse gewinnt, um Entscheidungsprozesse zu verbessern und neue Muster sowie Zusammenh\u00e4nge aufzudecken. Dieses Kapitel bietet dir eine Einf\u00fchrung in die grundlegenden Konzepte und Prinzipien, die im Zentrum dieses spannenden Bereichs stehen.</p>"},{"location":"week1/overview/#was-ist-data-science","title":"Was ist Data Science?","text":"<p>Data Science kombiniert verschiedene Disziplinen, darunter:</p> <ul> <li>Statistik: Die Wissenschaft der Datenanalyse und des Schlussfolgerns.</li> <li>Informatik: Methoden zur Verarbeitung, Analyse und Visualisierung gro\u00dfer Datenmengen.</li> <li>Dom\u00e4nenwissen: Spezifisches Wissen \u00fcber den Anwendungsbereich der Datenanalyse.</li> </ul> <p>Das Ziel von Data Science ist es, aus strukturierten und unstrukturierten Daten Wissen zu generieren.</p> <p>Wir haben in der Einf\u00fchrung zur Data Analysis bereits 4 Grundlegende Arten der Datenanalyse (Descriptive Analysis, Diagnostic Analysis, Predictive Analysis, Prescriptive Analysis) kennengelernt. Die einzelnen Schritte dieser Analysen sind ein wesentlicher Bestandteil der Data Science. Dar\u00fcber hinaus gibt es weitere wichtige Konzepte und Methoden, die in der Datenwissenschaft eine Rolle spielen. In einem Kompletten Data Science Prozess werden unter anderem die folgenden Schritte durchlaufen:</p>"},{"location":"week1/overview/#1-problemdefinition","title":"1. Problemdefinition","text":"<ul> <li>Ziel: Verstehen, welches Problem gel\u00f6st werden soll.</li> <li>Fragen: Welche Entscheidungen sollen unterst\u00fctzt werden? Welche Daten sind verf\u00fcgbar?</li> </ul>"},{"location":"week1/overview/#2-datenbeschaffung","title":"2. Datenbeschaffung","text":"<ul> <li>Quellen: Daten k\u00f6nnen aus internen Systemen, externen APIs, Sensoren oder Web-Scraping stammen.</li> <li>Wichtig: Qualit\u00e4t und Verf\u00fcgbarkeit der Daten beurteilen.</li> </ul>"},{"location":"week1/overview/#3-datenaufbereitung","title":"3. Datenaufbereitung","text":"<ul> <li>Data Cleaning: Umgang mit fehlenden Werten, Duplikaten oder Ausrei\u00dfern.</li> <li>Feature Engineering: Erstellung relevanter Variablen, die Modelle verbessern.</li> <li>Preprocessing: Skalieren, Normalisieren und Kodieren von Daten.</li> <li>ETL-Prozesse: (Extract, Transform, Load) Umwandlung und Integration von Daten aus verschiedenen Quellen.</li> </ul>"},{"location":"week1/overview/#4-modellierung","title":"4. Modellierung","text":"<ul> <li>Ziel: Aufbau von pr\u00e4diktiven oder deskriptiven Modellen.</li> <li>Methoden: Klassifikation, Regression, Clustering, etc.</li> </ul>"},{"location":"week1/overview/#5-validierung","title":"5. Validierung","text":"<ul> <li>Metriken: Genauigkeit, F1-Score, RMSE.</li> <li>Cross-Validation: Pr\u00fcfung der Generalisierungsf\u00e4higkeit eines Modells.</li> </ul>"},{"location":"week1/overview/#6-deployment","title":"(6. Deployment)","text":"<ul> <li>Anwendung: Bereitstellung von Modellen in produktiven Systemen.</li> <li>Wartung: Monitoring der Modellleistung und Aktualisierung.</li> </ul>"},{"location":"week1/overview/#mathematische-optimierung-in-data-science","title":"Mathematische Optimierung in Data Science","text":"<p>Data Science setzt sich zwar prim\u00e4r aus den Statistik, Informatik und Dom\u00e4nenwissen zusammen, aber es gibt auch viele andere Disziplinen, die in diesem Bereich eine Rolle spielen. Zu diesen geh\u00f6rt insbesondere die Mathematische Optimierung. Bei vielen statistischen Methoden und Machine-Learning-Modellen handelt es sich um Optimierungsprobleme, bei denen das Ziel darin besteht, die besten Parameter f\u00fcr ein Modell zu finden. Wir gehen im Nachfolgenden auf einige Begrifflichkeiten und Konzepte der Mathematischen Optimierung ein, welche uns als Data Scientisten regelm\u00e4\u00dfig begegnen werden.</p>"},{"location":"week1/overview/#1-definition","title":"1. Definition","text":"<ul> <li>Ziel: Finden der besten L\u00f6sung f\u00fcr ein Problem unter gegebenen Einschr\u00e4nkungen.</li> <li>Beispiele: Minimierung des Fehlers eines Modells, Maximierung der Genauigkeit oder Optimierung von Ressourcen.</li> </ul>"},{"location":"week1/overview/#2-gradient-descent","title":"2. Gradient Descent","text":"<ul> <li>Beschreibung: Iterativer Algorithmus, um die optimalen Parameter eines Modells zu finden.</li> <li>Funktionsweise: Bewegung in Richtung des steilsten Gradienten der Fehlerfunktion, um das Minimum zu erreichen.</li> <li>Einsatzgebiete: Training neuronaler Netzwerke, lineare Regression.</li> </ul>"},{"location":"week1/overview/#3-regularisierung","title":"3. Regularisierung","text":"<ul> <li>Ziel: Verhindern von Overfitting durch Hinzuf\u00fcgen von Straftermen in die Optimierungsfunktion.</li> <li>Beispiele: L1-Regularisierung (Lasso), L2-Regularisierung (Ridge).</li> </ul>"},{"location":"week1/overview/#4-optimierungsprobleme-im-kontext-von-data-science","title":"4. Optimierungsprobleme im Kontext von Data Science","text":"<ul> <li>Lineare Programmierung: Optimierung einer linearen Zielfunktion unter linearen Nebenbedingungen.</li> <li>Nichtlineare Optimierung: Behandlung komplexerer Probleme, bei denen Zielfunktion oder Nebenbedingungen nicht linear sind.</li> <li>Kombinatorische Optimierung: L\u00f6sung diskreter Probleme, wie z. B. die Auswahl der besten Feature-Kombination.</li> </ul> <p>Vor einigen Jahren wurde unter dem Begriff \"Data Science\" nahezu alles zusammengefasst, was mit Daten zu tun hatte. Heute hat sich das Feld weiterentwickelt und spezialisiert, wobei verschiedene Rollen und Aufgabenbereiche entstanden sind. Im Folgenden werden einige wichtige Rollen in Data Science vorgestellt, die in Unternehmen und Organisationen eine zentrale Rolle spielen.</p>"},{"location":"week1/overview/#wichtige-rollen-in-data-science","title":"Wichtige Rollen in Data Science","text":""},{"location":"week1/overview/#data-scientist","title":"Data Scientist","text":"<ul> <li>Verantwortlich f\u00fcr die Analyse und Modellierung der Daten.</li> <li>Kombiniert Statistik, Programmierung und Dom\u00e4nenwissen.</li> </ul>"},{"location":"week1/overview/#data-engineer","title":"Data Engineer","text":"<ul> <li>Entwickelt und pflegt Datenpipelines und Infrastruktur.</li> <li>Sicherstellung, dass Daten effizient und skalierbar verarbeitet werden.</li> </ul>"},{"location":"week1/overview/#data-analyst","title":"Data Analyst","text":"<ul> <li>Fokus auf Datenvisualisierung und Berichterstattung.</li> <li>Ziel: Gesch\u00e4ftsrelevante Erkenntnisse kommunizieren.</li> </ul>"},{"location":"week1/overview/#machine-learning-engineer","title":"Machine Learning Engineer","text":"<ul> <li>Spezialisiert auf die Entwicklung und Implementierung von ML-Modellen.</li> <li>Arbeitet eng mit Data Scientists zusammen, um Modelle in Produktionsumgebungen zu \u00fcberf\u00fchren.</li> </ul>"},{"location":"week1/overview/#business-analyst","title":"Business Analyst","text":"<ul> <li>\u00dcbersetzt Gesch\u00e4ftsanforderungen in datenbasierte L\u00f6sungen.</li> <li>Fokussiert auf die Anwendung von Daten zur Optimierung von Gesch\u00e4ftsprozessen.</li> </ul>"},{"location":"week1/overview/#data-architect","title":"Data Architect","text":"<ul> <li>Entwirft und implementiert Datenbanken und -architekturen.</li> <li>Sicherstellung, dass Daten effizient gespeichert und abgerufen werden k\u00f6nnen.</li> </ul> <p>In der Umgangssprache werden datengbasierte Projekte und teilweise sogar Software Projekte einfach als KI Projekte bezeichnet. In der Realit\u00e4t ist KI jedoch nur ein Teilbereich der Data Science. In diesem Abschnitt werden die Unterschiede und Beziehungen zwischen KI, Maschinellem Lernen, Deep Learning und Statistik erl\u00e4utert.</p>"},{"location":"week1/overview/#begriffe-aus-dem-data-science-kosmos","title":"Begriffe aus dem Data Science Kosmos","text":"<p>In der Welt von Data Science gibt es viele Begriffe, die oft synonym oder missverst\u00e4ndlich verwendet werden. Wir f\u00fchren hier einige der wichtigsten Begriffe auf und erkl\u00e4ren ihre Bedeutung und Beziehung zueinander. Die Berufsfelder aus dem obigen Abschnitt werden dabei nicht wiederholend aufgef\u00fchrt.</p> <ul> <li>K\u00fcnstliche Intelligenz (KI): Der \u00dcberbegriff f\u00fcr Technologien, die menschen\u00e4hnliches Denken und Handeln nachahmen.</li> <li>Maschinelles Lernen (ML): Ein Teilbereich der KI, der sich auf Algorithmen konzentriert, die aus Daten lernen k\u00f6nnen.</li> <li>Deep Learning: Ein Teilbereich des ML, der neuronale Netze mit vielen Schichten verwendet.</li> <li>Statistik: Fundamentale Methoden zur Analyse und Interpretation von Daten.</li> <li>Data Mining: Extraktion von Mustern und Informationen aus gro\u00dfen Datenmengen.</li> <li>Big Data: Verarbeitung und Analyse gro\u00dfer Datenmengen, die mit traditionellen Methoden nicht m\u00f6glich w\u00e4ren.</li> <li>Business Intelligence (BI): Technologien und Anwendungen zur Analyse von Gesch\u00e4ftsdaten.</li> </ul> <p>Wir werden uns in den n\u00e4chsten Abschnitten mit der Datenverarbeitung besch\u00e4ftigen. Genauer werden wir auf Feature Enigneering eingehen. Feature Engineering ist ein wichtiger Schritt im Data-Science-Prozess, bei dem neue, aussagekr\u00e4ftige Variablen aus vorhandenen Daten erstellt werden. Dieser Prozess erm\u00f6glicht es, relevante Informationen zu extrahieren und in einer Form bereitzustellen, die von Algorithmen optimal genutzt werden kann.</p>"},{"location":"week1/data_processing/duplicates/","title":"Umgang mit Duplikaten in Daten","text":"<p>In der Datenbereinigung stellen Duplikate ein h\u00e4ufiges Problem dar. Duplikate sind Eintr\u00e4ge in einem Datensatz, die identisch oder nahezu identisch sind und die Analyse verf\u00e4lschen k\u00f6nnen. Ein bewusster und systematischer Umgang mit Duplikaten ist entscheidend, um valide Ergebnisse zu erzielen.</p>"},{"location":"week1/data_processing/duplicates/#warum-sind-duplikate-problematisch","title":"Warum sind Duplikate problematisch?","text":"<p>Duplikate k\u00f6nnen aus verschiedenen Gr\u00fcnden auftreten:</p> <ul> <li>Fehlerhafte Datenintegration: Beim Zusammenf\u00fchren von Daten aus unterschiedlichen Quellen entstehen oft redundante Eintr\u00e4ge.</li> <li>Mehrfache Datenerfassung: Beispielsweise k\u00f6nnen Kunden ein Formular mehrfach ausgef\u00fcllt haben.</li> <li>Automatisierte Prozesse: Systeme, die ohne ausreichende Kontrollmechanismen Daten generieren, k\u00f6nnen unbeabsichtigt doppelte Werte erzeugen.</li> </ul> <p>Konsequenzen von Duplikaten sind unter anderem:</p> <ul> <li>Verzerrte Analysen durch \u00fcberrepr\u00e4sentierte Werte.</li> <li>Ineffiziente Speicher- und Rechenressourcen.</li> <li>Probleme bei Modellen, die davon ausgehen, dass jeder Datensatz einzigartig ist.</li> </ul>"},{"location":"week1/data_processing/duplicates/#arten-von-duplikaten","title":"Arten von Duplikaten","text":"<p>Es gibt verschiedene Arten von Duplikaten, die je nach Kontext unterschiedlich behandelt werden m\u00fcssen:</p> <p>Exakte Duplikate    - Diese Eintr\u00e4ge stimmen in allen Feldern \u00fcberein.    - Beispiel:      | ID  | Name       | Alter | Ort    |      | --- | ---------- | ----- | ------ |      | 001 | Anna Meier | 29    | Berlin |      | 001 | Anna Meier | 29    | Berlin |</p> <p>Nahezu Duplikate    - Diese Eintr\u00e4ge unterscheiden sich geringf\u00fcgig, z. B. durch Tippfehler oder unterschiedliche Schreibweisen.    - Beispiel:      | ID  | Name          | Alter | Ort     |      | --- | ------------- | ----- | ------- |      | 002 | Thomas Schmid | 35    | Hamburg |      | 003 | T. Schmid     | 35    | Hamburg |</p>"},{"location":"week1/data_processing/duplicates/#vorgehen-zur-identifikation-und-behandlung-von-duplikaten","title":"Vorgehen zur Identifikation und Behandlung von Duplikaten","text":""},{"location":"week1/data_processing/duplicates/#identifikation-von-duplikaten","title":"Identifikation von Duplikaten","text":"<p>Die Identifikation ist der erste Schritt und kann je nach Art der Duplikate unterschiedlich erfolgen:</p> <ul> <li>Exakte Duplikate finden:</li> <li> <p>In Python k\u00f6nnen exakte Duplikate mit Pandas identifiziert werden:     ```python     import pandas as pd</p> </li> <li> <p>Nahezu Duplikate finden:</p> </li> <li> <p>Hier helfen Ans\u00e4tze wie Fuzzy Matching, beispielsweise mit der Bibliothek <code>fuzzywuzzy</code> oder \u00e4hnlichen Tools:     ```python     from fuzzywuzzy import fuzz     from fuzzywuzzy import process</p> </li> </ul>"},{"location":"week1/data_processing/duplicates/#beispiel-datensatz","title":"Beispiel-Datensatz","text":"<p>data = {     'ID': [1, 2, 3, 1],     'Name': ['Anna Meier', 'Thomas Schmid', 'Maria Kurz', 'Anna Meier'],     'Alter': [29, 35, 40, 29] } df = pd.DataFrame(data)</p>"},{"location":"week1/data_processing/duplicates/#duplikate-finden","title":"Duplikate finden","text":"<p>duplicates = df[df.duplicated()] print(duplicates) ```</p>"},{"location":"week1/data_processing/duplicates/#beispiel-vergleich-zweier-namen","title":"Beispiel: Vergleich zweier Namen","text":"<p>score = fuzz.ratio(\"Thomas Schmid\", \"T. Schmid\") print(score)  # Ausgabe: \u00dcber 80%, also wahrscheinlich Duplikat ```</p>"},{"location":"week1/data_processing/duplicates/#behandlung-von-duplikaten","title":"Behandlung von Duplikaten","text":""},{"location":"week1/data_processing/duplicates/#exakte-duplikate-entfernen","title":"Exakte Duplikate entfernen","text":"<ul> <li>Einfaches Entfernen: <code>python   # Entfernen von exakten Duplikaten   df_cleaned = df.drop_duplicates()</code></li> <li>Erhalt bestimmter Eintr\u00e4ge: Wenn du die erste oder letzte Instanz eines Duplikats behalten m\u00f6chtest, kannst du dies steuern:   <code>python   df_cleaned = df.drop_duplicates(keep='first')  # Behalte den ersten Eintrag</code></li> </ul>"},{"location":"week1/data_processing/duplicates/#nahezu-duplikate-bereinigen","title":"Nahezu Duplikate bereinigen","text":"<ul> <li>Manuelle Pr\u00fcfung: Besonders bei kleineren Datens\u00e4tzen lohnt sich die manuelle Sichtung.</li> <li>Automatisierte Verfahren:</li> <li>Berechnung von \u00c4hnlichkeitsscores.</li> <li>Erstellung von Regeln zur Vereinheitlichung (z. B. Namenskonventionen).</li> </ul>"},{"location":"week1/data_processing/duplicates/#zusammenfuhren-von-duplikaten","title":"Zusammenf\u00fchren von Duplikaten","text":"<ul> <li>Wenn Informationen in den Duplikaten variieren, kannst du diese zusammenf\u00fchren:   <code>python   # Gruppieren und Aggregieren   df_grouped = df.groupby('ID').agg({       'Name': 'first',       'Alter': 'mean'   })</code></li> </ul>"},{"location":"week1/data_processing/duplicates/#beispiele-aus-der-praxis","title":"Beispiele aus der Praxis","text":"<ul> <li> <p>Kundenlisten bereinigen:   Ein Unternehmen hat mehrere Listen von Kunden aus verschiedenen Regionen zusammengef\u00fchrt. Dabei entstehen oft exakte und nahe Duplikate, die manuell oder mit Matching-Algorithmen behandelt werden m\u00fcssen.</p> </li> <li> <p>Web-Scraping-Daten:   Beim Scraping von Websites entstehen oft doppelte Eintr\u00e4ge, wenn dieselbe Seite mehrfach abgefragt wird. Hier hilft die Verwendung einer eindeutigen ID oder eines Hashes, um Duplikate zu identifizieren.</p> </li> </ul>"},{"location":"week1/data_processing/feature_enigneerin_ideas/","title":"Feature enigneerin ideas","text":"<ol> <li>Feature-Auswahl:</li> <li>Korrelationsanalyse: Untersuche, welche Features stark miteinander korrelieren und ob es sinnvoll ist, einige Features zu entfernen, um Multikollinearit\u00e4t zu vermeiden.</li> <li> <p>Spearman/Kendall/Korrelation: Wenn du mit nicht-linearen Beziehungen arbeitest, k\u00f6nnen diese Methoden ebenfalls n\u00fctzlich sein.</p> </li> <li> <p>Feature-Transformation:</p> </li> <li>Skalierung: Normalisierung (Min-Max) oder Standardisierung (Z-Score) von Features, um sie auf denselben Ma\u00dfstab zu bringen, besonders f\u00fcr Modelle, die auf Distanzmessungen basieren (z.B. SVM oder k-NN).</li> <li> <p>Log-Transformation: Wenn Features starke Schiefe aufweisen, kann eine Log-Transformation helfen, die Verteilung zu stabilisieren.</p> </li> <li> <p>Feature-Creation:</p> </li> <li>Kombination von Features: Erstelle neue Features durch Kombinationen bestehender (z.B. Multiplikation, Addition oder Differenz von numerischen Variablen).</li> <li>Zeitmerkmale: Wenn du mit Zeitstempeln arbeitest, extrahiere Merkmale wie Wochentag, Monat, Jahr, Stunden, Feiertage etc.</li> <li> <p>Kategorische Merkmale: Wandeln von kategorischen Variablen in numerische Werte (z.B. mit One-Hot-Encoding, Label-Encoding oder Target-Encoding).</p> </li> <li> <p>Handling von fehlenden Werten:</p> </li> <li>Imputation: Fehlende Werte mit dem Mittelwert, Median oder einem Vorhersagemodell auff\u00fcllen.</li> <li> <p>L\u00f6schen: Entferne Zeilen oder Spalten mit zu vielen fehlenden Werten, wenn die Imputation nicht sinnvoll erscheint.</p> </li> <li> <p>Feature-Engineering f\u00fcr Textdaten:</p> </li> <li>Bag of Words oder TF-IDF: Um Textdaten in eine numerische Form zu bringen.</li> <li> <p>Word Embeddings: Verwendung vortrainierter Vektoren wie Word2Vec oder GloVe, um semantische Beziehungen zu erfassen.</p> </li> <li> <p>Feature-Engineering f\u00fcr kategorische Daten:</p> </li> <li>One-Hot-Encoding: F\u00fcr nominale Kategorien, um diese in eine bin\u00e4re Form zu bringen.</li> <li> <p>Ordinal-Encoding: Wenn die Kategorien eine nat\u00fcrliche Reihenfolge haben (z.B. klein, mittel, gro\u00df).</p> </li> <li> <p>Dimensionalit\u00e4tsreduktion:</p> </li> <li>PCA (Principal Component Analysis): Reduziere die Anzahl der Features bei gleichzeitiger Beibehaltung der gr\u00f6\u00dften Varianz im Datensatz.</li> <li>t-SNE oder UMAP: Zur Visualisierung und ggf. als Vorprozessierung bei komplexen Datens\u00e4tzen.</li> </ol>"},{"location":"week1/data_processing/intro/","title":"Datenvorverareitung","text":"<p>Einer der wichtigsten Schritte in der Data Science ist das bearbeiten und aufbereiten von Daten. Unsere Modelle sind nur so gut wie die Daten, die wir ihnen geben. Daher wird die Qualit\u00e4t unser Modelle ma\u00dfgeblich von der Qualit\u00e4t unser Datenverarbeitung beeinflusst.</p> <p>Die Datenverarbeitung wird in verschiedene Untergebiete aufgeteilt. Zu diesen z\u00e4hlen unter anderem das Data Cleaning, Preprocessing und Feature Engineering.</p>"},{"location":"week1/data_processing/intro/#data-cleaning","title":"Data Cleaning","text":"<p>Data Cleaning ist der Prozess der Bereinigung von Daten, um deren Qualit\u00e4t und Konsistenz zu verbessern. Dies ist ein entscheidender Schritt, da unvollst\u00e4ndige oder fehlerhafte Daten die Leistung von Modellen erheblich beeintr\u00e4chtigen k\u00f6nnen. </p> <ul> <li>Fokus: Bereinigung der Daten.</li> <li>Beispiele:<ul> <li>Umgang mit fehlenden Werten: Fehlende Daten k\u00f6nnen durch Imputation (z. B. Mittelwert, Median) oder durch Entfernen der betroffenen Eintr\u00e4ge behandelt werden.</li> <li>Entfernung von Duplikaten: Doppelte Eintr\u00e4ge in den Daten k\u00f6nnen zu Verzerrungen f\u00fchren und sollten identifiziert und entfernt werden.</li> <li>Korrektur von Inkonsistenzen: Unterschiedliche Formate oder Schreibweisen (z. B. Datumsformate) sollten vereinheitlicht werden.</li> </ul> </li> </ul>"},{"location":"week1/data_processing/intro/#preprocessing","title":"Preprocessing","text":"<p>Preprocessing umfasst alle Schritte, die notwendig sind, um Rohdaten in ein Format zu bringen, das f\u00fcr die Modellierung geeignet ist. Dies beinhaltet die Transformation und Normalisierung der Daten, um sicherzustellen, dass sie von Algorithmen effizient verarbeitet werden k\u00f6nnen.</p> <ul> <li>Fokus: Vorbereitung der Daten f\u00fcr Modelle.</li> <li>Beispiele:<ul> <li>Skalieren von Werten: Daten k\u00f6nnen auf einen bestimmten Bereich (z. B. 0 bis 1) skaliert werden, um die Leistung von Algorithmen zu verbessern.</li> <li>Normalisierung: Anpassung der Datenverteilung, z. B. durch Z-Score-Normalisierung, um die Vergleichbarkeit der Variablen zu gew\u00e4hrleisten.</li> </ul> </li> </ul>"},{"location":"week1/data_processing/intro/#feature-engineering","title":"Feature Engineering","text":"<p>Feature Engineering ist der  Prozess der Erstellung neuer, relevanter Features aus den vorhandenen Daten. Ziel ist es, die Leistungsf\u00e4higkeit der Modelle zu steigern, indem zus\u00e4tzliche Informationen extrahiert und bereitgestellt werden. Das Feature Engineering baut auf den vorherigen Schritten auf und erm\u00f6glicht es, die Daten optimal f\u00fcr die Modellierung vorzubereiten.</p> <ul> <li>Fokus: Erstellen neuer, relevanter Features.</li> <li>Beispiele:<ul> <li>Berechnung einer neuen Variable: Erstellen neuer Features durch Kombination oder Transformation bestehender Daten, z. B. das Verh\u00e4ltnis von zwei Variablen.</li> <li>Zeit- und Ortsbezug: Extrahieren von Informationen wie Wochentagen, Uhrzeiten oder geografischen Regionen aus Datums- oder Standortdaten.</li> <li>Transformation bestehender Daten: Anwenden von mathematischen Transformationen wie Logarithmieren oder Skalieren, um die Datenverteilung zu verbessern.</li> <li>Kodierung kategorialer Daten: Umwandlung von Kategorien in numerische Werte, um sie f\u00fcr Algorithmen nutzbar zu machen.</li> </ul> </li> </ul> <p>Feature Engineering erfordert sowohl technisches Wissen als auch ein tiefes Verst\u00e4ndnis der Daten und des Anwendungsbereichs, um die relevantesten und aussagekr\u00e4ftigsten Features zu identifizieren und zu erstellen.</p>"},{"location":"week1/data_processing/intro/#7-einfuhrung-in-feature-engineering","title":"7. Einf\u00fchrung in Feature Engineering","text":"<p>Feature Engineering ist ein essenzieller Schritt im Data-Science-Prozess, der den \u00dcbergang von Rohdaten zu aussagekr\u00e4ftigen Variablen erm\u00f6glicht. Dabei geht es darum, die relevanten Informationen aus den Daten zu extrahieren und in einer Form bereitzustellen, die Algorithmen optimal nutzen k\u00f6nnen.</p>"},{"location":"week1/data_processing/intro/#einordnung-von-feature-engineering","title":"Einordnung von Feature Engineering","text":"<p>Feature Engineering geh\u00f6rt in den Bereich der Datenvorbereitung, grenzt sich jedoch durch den Fokus auf die Erstellung neuer Variablen ab. W\u00e4hrend Data Cleaning und Preprocessing die Qualit\u00e4t und Struktur der Daten verbessern, zielt Feature Engineering darauf ab, Informationen f\u00fcr Modelle nutzbar zu machen.</p>"},{"location":"week1/data_processing/intro/#warum-ist-feature-engineering-wichtig","title":"Warum ist Feature Engineering wichtig?","text":"<ul> <li>Verbesserte Modellleistung: Gut gestaltete Features erh\u00f6hen die Genauigkeit und Robustheit von Modellen.</li> <li>Dom\u00e4nenwissen einbinden: Durch spezifisches Wissen \u00fcber den Anwendungsbereich k\u00f6nnen wichtige Zusammenh\u00e4nge besser dargestellt werden.</li> <li>Reduktion von Komplexit\u00e4t: Feature Engineering kann helfen, irrelevante Informationen zu entfernen und die Analyse zu fokussieren.</li> </ul>"},{"location":"week1/data_processing/intro/#methoden-des-feature-engineering","title":"Methoden des Feature Engineering","text":"<ol> <li>Transformation bestehender Daten: Logarithmieren, Skalieren oder Normalisieren von Variablen.</li> <li>Kombination von Variablen: Erstellen neuer Features durch Kombination bestehender Variablen (z. B. Verh\u00e4ltnisse oder Differenzen).</li> <li>Ableitung von Zeit- oder Ortsbezug: Extrahieren von Wochentagen, Uhrzeiten oder geografischen Regionen aus Datums- oder Standortinformationen.</li> <li>Kodierung kategorialer Daten: Umwandlung von Kategorien in numerische Werte (z. B. One-Hot-Encoding).</li> </ol> <p>Im Nachfolgenden Abschnitt werden wir uns zun\u00e4chst mit Methoden des Data Cleaning und Preprocessing besch\u00e4ftigen, bevor wir tiefer in die Methoden des Feature Engineering eintauchen.</p>"},{"location":"week1/data_processing/missing_values/","title":"Missing Values","text":"<p>Unsere Datens\u00e4tze sind in den wengisten F\u00e4llen perfekt vollst\u00e4ndig. Beispielsweise kann es bei der Erfassung von Personaldaten vorkommen, dass nicht alle Felder ausgef\u00fcllt wurden. Diese fehlenden Werte k\u00f6nnen die Analyse und Modellierung beeintr\u00e4chtigen. Daher ist es wichtig, diese L\u00fccken zu identifizeren und zu behandeln.</p>"},{"location":"week1/data_processing/missing_values/#umgang-mit-fehlenden-werten","title":"Umgang mit fehlenden Werten","text":"<p>Es gibt verschiedene Strategien, um fehlende Werte zu behandeln. Die Wahl der Methode h\u00e4ngt von der Art der Daten und dem Kontext ab. Hier sind einige g\u00e4ngige Ans\u00e4tze:</p>"},{"location":"week1/data_processing/missing_values/#1-entfernen-von-fehlenden-werten","title":"1. Entfernen von fehlenden Werten","text":"<p>Eine einfache Methode besteht darin, alle Zeilen oder Spalten zu entfernen, die fehlende Werte enthalten. Dies kann jedoch zu einem Informationsverlust f\u00fchren, insbesondere wenn viele Daten fehlen. Generell sollten wir beim Entfernen der Daten uns vorher ansehen, ob die Daten zuf\u00e4llig fehlen oder ob es ein Muster gibt. Machmal kann es auch vorkommen, dass unser Datensatz durch das Entfernen von Daten verzerrt oder zu klein wird.</p>"},{"location":"week1/data_processing/missing_values/#2-imputation-von-fehlenden-werten","title":"2. Imputation von fehlenden Werten","text":"<p>Eine h\u00e4ufigere Methode ist die Imputation, bei der fehlende Werte durch Sch\u00e4tzungen basierend auf den verbleibenden Daten ersetzt werden. Dies kann durch den Durchschnitt, den Median oder durch Vorhersagemodelle erfolgen. Die Wahl der Methode h\u00e4ngt von der Art der Daten und dem Kontext ab. Vor der Wahl einer der Methoden f\u00fcr die Imputation m\u00fcssen wir auch hier ein Grundverst\u00e4ndnis von unseren Daten haben. </p> <p>Beobachte wir beispielsweise Wetterdaten und es Fehlt die Temperatur an einem Tag, dann k\u00f6nnten wir die Temperatur des Vortages als Sch\u00e4tzung verwenden. Auch die Verwendung eines Mittelweertes der anliegenden Tage k\u00f6nnte eine M\u00f6glichkeit sein. Haben wir jedoch ungeordnete Daten, wie beispielsweise Namen, dann macht es keinen Sinn, den Durchschnitt f\u00fcr die Spalte Alter als Ersatz f\u00fcr Missing Values zu berechnen.</p> <p>Je nach Art der Daten und des Problems ist bei der Imputation Kreativit\u00e4t und Fachwissen gefragt.</p>"},{"location":"week1/data_processing/missing_values/#3-vorhersagemodelle-fur-die-imputation","title":"3. Vorhersagemodelle f\u00fcr die Imputation","text":"<p>In einigen F\u00e4llen kann es sinnvoll sein, Vorhersagemodelle zu verwenden, um fehlende Werte zu sch\u00e4tzen. Dies kann insbesondere dann n\u00fctzlich sein, wenn die fehlenden Werte nicht zuf\u00e4llig sind und ein Muster aufweisen. Beispielsweise k\u00f6nnten wir ein Regressionsmodell trainieren, um fehlende Werte zu sch\u00e4tzen, basierend auf anderen verf\u00fcgbaren Variablen.</p>"},{"location":"week1/data_processing/missing_values/#4-kategorisierung-von-fehlenden-werten","title":"4. Kategorisierung von fehlenden Werten","text":"<p>Manchmal kann es sinnvoll sein, fehlende Werte als separate Kategorie zu behandeln, anstatt sie zu sch\u00e4tzen oder zu entfernen. Dies kann insbesondere dann n\u00fctzlich sein, wenn das Fehlen von Werten eine Bedeutung hat und Informationen enth\u00e4lt. Beispielsweise kann das fehlen eines Wertes bei der Anwesenheit eines Patienten in einem Krankenhaus darauf hinweisen, dass der Patient nicht anwesend war.</p>"},{"location":"week1/data_processing/outliers/","title":"Umgang mit Ausrei\u00dfern","text":"<p>Ausrei\u00dfer sind Datenpunkte, die erheblich von anderen Beobachtungen in einem Datensatz abweichen. Sie k\u00f6nnen wertvolle Informationen enthalten oder die Analyse st\u00f6ren, weshalb es wichtig ist, sie zu erkennen und sinnvoll zu behandeln.</p>"},{"location":"week1/data_processing/outliers/#warum-sind-ausreier-wichtig","title":"Warum sind Ausrei\u00dfer wichtig?","text":"<p>Ausrei\u00dfer k\u00f6nnen sowohl negative als auch positive Auswirkungen auf die Datenanalyse haben. Negative Effekte umfassen die Verzerrung statistischer Kennzahlen wie Mittelwert und Standardabweichung, was zu fehlerhaften Schlussfolgerungen f\u00fchren kann. Auf der anderen Seite k\u00f6nnen Ausrei\u00dfer auch positive Effekte haben, indem sie auf seltene, aber wichtige Ereignisse hinweisen, wie zum Beispiel Betrug, Fehler oder extreme Ph\u00e4nomene.</p> <p>Ein Beispiel f\u00fcr einen negativen Effekt w\u00e4re ein Monatsgehalt von 1.000.000 \u20ac in einem Datensatz von Durchschnittsgeh\u00e4ltern, das den Mittelwert stark nach oben verzerren w\u00fcrde. Ein Beispiel f\u00fcr einen positiven Effekt w\u00e4re eine Temperaturmessung von 80 \u00b0C in einer Klimadatenreihe, die auf ein au\u00dfergew\u00f6hnliches Wetterereignis hinweisen k\u00f6nnte.</p>"},{"location":"week1/data_processing/outliers/#erkennung-von-ausreiern","title":"Erkennung von Ausrei\u00dfern","text":""},{"location":"week1/data_processing/outliers/#visuelle-methoden","title":"Visuelle Methoden","text":"<p>Visuelle Methoden zur Erkennung von Ausrei\u00dfern umfassen Boxplots und Scatterplots. Boxplots zeigen potenzielle Ausrei\u00dfer oberhalb oder unterhalb der \"Whisker\". Um einen Boxplot f\u00fcr die Variable \"Preis\" in einem Datensatz zu erstellen, kann folgender Python-Code verwendet werden:</p> <pre><code>import matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.boxplot(x=df['Preis'])\nplt.show()\n</code></pre> <p>Scatterplots hingegen zeigen Ausrei\u00dfer in zweidimensionalen Daten. Um \"Gr\u00f6\u00dfe\" gegen \"Preis\" zu plotten und auff\u00e4llige Punkte zu identifizieren, kann folgender Python-Code verwendet werden:</p> <pre><code>plt.scatter(df['Gr\u00f6\u00dfe'], df['Preis'])\nplt.xlabel('Gr\u00f6\u00dfe')\nplt.ylabel('Preis')\nplt.show()\n</code></pre>"},{"location":"week1/data_processing/outliers/#statistische-methoden","title":"Statistische Methoden","text":"<p>Die IQR-Methode (Interquartilsabstand) ist eine g\u00e4ngige Methode zur Identifizierung von Ausrei\u00dfern. Werte, die au\u00dferhalb des Bereichs [Q1 - 1,5 * IQR, Q3 + 1,5 * IQR] liegen, gelten als Ausrei\u00dfer. Um die IQR-Methode zu implementieren und die Ausrei\u00dfer aufzulisten, kann folgender Python-Code verwendet werden:</p> <pre><code>Q1 = df['Preis'].quantile(0.25)\nQ3 = df['Preis'].quantile(0.75)\nIQR = Q3 - Q1\n\nlower_bound = Q1 - 1.5 * IQR\nupper_bound = Q3 + 1.5 * IQR\n\nausreisser = df[(df['Preis'] &lt; lower_bound) | (df['Preis'] &gt; upper_bound)]\nprint(ausreisser)\n</code></pre> <ul> <li>Z-Score:  Der Z-Score ist ein statistisches Ma\u00df, das angibt, wie viele Standardabweichungen ein Datenpunkt vom Mittelwert entfernt ist. Werte mit einem absoluten Z-Score gr\u00f6\u00dfer als 3 werden oft als Ausrei\u00dfer betrachtet.</li> </ul> <p>Um den Z-Score f\u00fcr alle Werte zu berechnen und die Ausrei\u00dfer zu finden, kann folgender Python-Code verwendet werden:</p> <pre><code>```python\nfrom scipy.stats import zscore\ndf['z_score'] = zscore(df['Preis'])\nausreisser = df[abs(df['z_score']) &gt; 3]\nprint(ausreisser)\n```\n</code></pre> <p>In diesem Code wird die Funktion zscore aus dem Modul scipy.stats verwendet, um den Z-Score der Spalte Preis in einem DataFrame df zu berechnen. Anschlie\u00dfend werden alle Zeilen, deren absoluter Z-Score gr\u00f6\u00dfer als 3 ist, als Ausrei\u00dfer identifiziert und ausgegeben.</p>"},{"location":"week1/data_processing/outliers/#umgang-mit-ausreiern_1","title":"Umgang mit Ausrei\u00dfern","text":""},{"location":"week1/data_processing/outliers/#entfernen-von-ausreiern","title":"Entfernen von Ausrei\u00dfern","text":"<p>Die IQR-Methode (Interquartilsabstand) ist eine g\u00e4ngige statistische Methode zur Identifizierung und Entfernung von Ausrei\u00dfern in einem Datensatz. Der Interquartilsabstand ist der Bereich zwischen dem ersten Quartil (Q1) und dem dritten Quartil (Q3) eines Datensatzes. Ausrei\u00dfer sind Werte, die entweder unterhalb von Q1 - 1.5 * IQR oder oberhalb von Q3 + 1.5 * IQR liegen.</p> <p>Um problematische Werte zu entfernen, wenn sie Fehler oder nicht relevant sind, kann die IQR-Methode verwendet werden. Zuerst werden die Quartile Q1 und Q3 berechnet, dann wird der IQR als die Differenz zwischen Q3 und Q1 bestimmt. Schlie\u00dflich werden die unteren und oberen Grenzen festgelegt, um Ausrei\u00dfer zu identifizieren und zu entfernen. Der bereinigte Datensatz enth\u00e4lt nur die Werte, die innerhalb dieser Grenzen liegen.    <code>python     Q1 = df['Preis'].quantile(0.25)     Q3 = df['Preis'].quantile(0.75)     IQR = Q3 - Q1     lower_bound = Q1 - 1.5 * IQR     upper_bound = Q3 + 1.5 * IQR</code> df_cleaned = df[(df['Preis'] &gt;= lower_bound) &amp; (df['Preis'] &lt;= upper_bound)]</p> <p>In diesem Beispiel wird der bereinigte Datensatz df_cleaned erstellt, indem alle Ausrei\u00dfer basierend auf der IQR-Methode entfernt werden.</p>"},{"location":"week1/data_processing/outliers/#transformation-von-daten","title":"Transformation von Daten","text":"<p>Die Log-Transformation wird verwendet, um extreme Werte zu gl\u00e4tten. Dies geschieht, indem die Werte einer Variable mit der Logarithmusfunktion transformiert werden. Hier ist ein Beispiel in Python:     <code>python     df['Preis_log'] = np.log1p(df['Preis'])</code> Die Log-Transformation reduziert die Skala der Daten, insbesondere bei gro\u00dfen Werten. Dies liegt daran, dass der Logarithmus einer Zahl langsamer w\u00e4chst als die Zahl selbst. Dadurch werden gro\u00dfe Werte komprimiert und die Verteilung der Daten wird symmetrischer. Dies kann hilfreich sein, um die Auswirkungen von Ausrei\u00dfern zu minimieren und die Datenanalyse zu verbessern.</p>"},{"location":"week1/data_processing/outliers/#winsorizing","title":"Winsorizing","text":"<p>Um Extremwerte zu begrenzen, k\u00f6nnen wir die Werte auf vordefinierte Schwellen beschr\u00e4nken. In dieser Aufgabe sollen die Preise auf das 5. und 95. Perzentil begrenzt werden.</p> <p>Perzentile sind Ma\u00dfeinheiten, die die Verteilung von Daten in 100 gleiche Teile unterteilen. Das 5. Perzentil bedeutet, dass 5% der Daten unter diesem Wert liegen, w\u00e4hrend das 95. Perzentil bedeutet, dass 95% der Daten unter diesem Wert liegen.</p> <p>Im folgenden Python-Code wird dies umgesetzt:     ```python     lower_percentile = df['Preis'].quantile(0.05)     upper_percentile = df['Preis'].quantile(0.95)</p> <pre><code>df['Preis_winsorized'] = np.clip(df['Preis'], lower_percentile, upper_percentile)\n```\n</code></pre> <p>Hier berechnen wir zun\u00e4chst das 5. und 95. Perzentil der Preisdaten. Anschlie\u00dfend verwenden wir die np.clip-Funktion, um die Preise auf diese Perzentile zu begrenzen. Dies bedeutet, dass alle Preise, die unter dem 5. Perzentil liegen, auf das 5. Perzentil gesetzt werden, und alle Preise, die \u00fcber dem 95. Perzentil liegen, auf das 95. Perzentil gesetzt werden.</p>"},{"location":"week1/data_processing/outliers/#beibehalten-von-ausreiern","title":"Beibehalten von Ausrei\u00dfern","text":"<p>In manchen F\u00e4llen sollten Ausrei\u00dfer bewusst beibehalten werden, insbesondere wenn sie f\u00fcr das Problem relevant sind, wie zum Beispiel bei der Betrugserkennung.</p>"},{"location":"week1/data_processing/outliers/#best-practices","title":"Best Practices","text":"<ul> <li>Analysiere den Ursprung: Pr\u00fcfe, ob der Ausrei\u00dfer ein Messfehler, ein Dateneingabefehler oder ein echtes Signal ist.</li> <li>Dom\u00e4nenwissen einbeziehen: Besprich Auff\u00e4lligkeiten mit Experten aus dem jeweiligen Bereich.</li> <li>Dokumentiere deine Entscheidungen: Notiere, welche Schritte unternommen wurden und warum.</li> </ul>"},{"location":"week1/data_processing/type_conversion/","title":"Typenkonvertierungen (Wird noch behandelt)","text":"<p>Typenkonvertierungen sind ein essenzieller Bestandteil des Data Cleanings. H\u00e4ufig stammen Daten aus unterschiedlichen Quellen, was zu inkonsistenten Datentypen f\u00fchren kann. Beispielsweise kann ein numerischer Wert als Text gespeichert sein oder ein Datum in einem un\u00fcblichen Format vorliegen. Um Analysen durchf\u00fchren zu k\u00f6nnen, m\u00fcssen solche Werte in geeignete Datentypen konvertiert werden.</p>"},{"location":"week1/data_processing/type_conversion/#warum-typenkonvertierungen-wichtig-sind","title":"Warum Typenkonvertierungen wichtig sind","text":"<ul> <li>Korrekte Verarbeitung: Viele Analyse- und Modellierungsmethoden erfordern spezifische Datentypen (z. B. numerische Werte f\u00fcr Berechnungen).</li> <li>Effizienz: Die Arbeit mit korrekt typisierten Daten ist meist schneller und ressourcenschonender.</li> <li>Fehlervermeidung: Inkompatible Typen k\u00f6nnen zu Laufzeitfehlern oder falschen Ergebnissen f\u00fchren.</li> </ul>"},{"location":"week1/data_processing/type_conversion/#typische-herausforderungen","title":"Typische Herausforderungen","text":"<p>Numerische Werte als Text:    Daten wie \"1000\" oder \"3,14\" werden oft als Zeichenketten gespeichert, m\u00fcssen aber f\u00fcr Berechnungen in numerische Werte konvertiert werden.</p> <p>Datumsangaben:    Formate wie \"12/31/2024\" oder \"31-12-2024\" erfordern eine einheitliche Konvertierung in ein Datumsobjekt.</p> <p>Kategorische Daten:    Kategorien wie \"Ja\" und \"Nein\" oder \"True\" und \"False\" sollten in standardisierte Werte umgewandelt werden.</p> <p>Fehlende Werte:    Leere Felder oder Platzhalter wie \"N/A\" oder \"-\" m\u00fcssen erkannt und behandelt werden.</p>"},{"location":"week1/data_processing/type_conversion/#methoden-zur-typenkonvertierung","title":"Methoden zur Typenkonvertierung","text":""},{"location":"week1/data_processing/type_conversion/#numerische-werte","title":"Numerische Werte","text":"<p>Mit Bibliotheken wie Pandas in Python k\u00f6nnen Zeichenketten in numerische Werte umgewandelt werden:</p> <pre><code>import pandas as pd\n\n# Beispiel-Daten\ndata = {\"Werte\": [\"100\", \"200\", \"300\"]}\ndf = pd.DataFrame(data)\n\n# Typenkonvertierung\ndf[\"Werte\"] = pd.to_numeric(df[\"Werte\"])\n</code></pre> <p>Falls die Konvertierung fehlschl\u00e4gt, kann der Parameter <code>errors='coerce'</code> verwendet werden, um problematische Werte in <code>NaN</code> umzuwandeln.</p>"},{"location":"week1/data_processing/type_conversion/#datumsangaben","title":"Datumsangaben","text":"<p>Datumswerte lassen sich mit <code>pd.to_datetime</code> standardisieren:</p> <pre><code># Beispiel-Daten\ndata = {\"Datum\": [\"31/12/2024\", \"01-01-2025\", \"2024.12.30\"]}\ndf = pd.DataFrame(data)\n\n# Typenkonvertierung\ndf[\"Datum\"] = pd.to_datetime(df[\"Datum\"], dayfirst=True)\n</code></pre> <p>Der Parameter <code>dayfirst=True</code> stellt sicher, dass das europ\u00e4ische Datumsformat korrekt interpretiert wird.</p>"},{"location":"week1/data_processing/type_conversion/#kategorische-daten","title":"Kategorische Daten","text":"<p>Kategorische Daten k\u00f6nnen in numerische Codes oder <code>category</code>-Typen umgewandelt werden:</p> <pre><code># Beispiel-Daten\ndata = {\"Antworten\": [\"Ja\", \"Nein\", \"Ja\"]}\ndf = pd.DataFrame(data)\n\n# Typenkonvertierung\ndf[\"Antworten\"] = df[\"Antworten\"].astype(\"category\")\n</code></pre> <p>Kategorien haben den Vorteil, weniger Speicherplatz zu beanspruchen und effizienter verarbeitet zu werden.</p>"},{"location":"week1/data_processing/type_conversion/#fehlende-werte-behandeln","title":"Fehlende Werte behandeln","text":"<p>Vor der Typenkonvertierung m\u00fcssen Platzhalter wie \"N/A\" entfernt oder ersetzt werden:</p> <pre><code># Beispiel-Daten\ndata = {\"Werte\": [\"100\", \"N/A\", \"200\"]}\ndf = pd.DataFrame(data)\n\n# Fehlende Werte ersetzen und konvertieren\ndf[\"Werte\"] = pd.to_numeric(df[\"Werte\"], errors='coerce')\n</code></pre>"},{"location":"week3/goals/","title":"Ziele","text":"<p>Ich kann...</p> <p>... aus bestehenden Features neue Features generieren. ... Kateogrien in numerische Werte umwandeln. ... mit one hot encoding umgehen. ... mit label encoding umgehen. ... erste Methoden zum Transformieren von Daten beschreiben. ... erste Methoden zum Skalieren von Daten beschreiben.</p>"},{"location":"week3/feature_engineering/encoding/","title":"Encoding","text":"<p>Encoding ist der Prozess, kategorische Daten in numerische Werte umzuwandeln, damit maschinelle Lernalgorithmen mit ihnen arbeiten k\u00f6nnen. Das ist notwendig, da viele Algorithmen nur mit numerischen Daten umgehen k\u00f6nnen und kategoriale Werte \u2013 wie \"rot\", \"blau\", \"gr\u00fcn\" \u2013 nicht direkt verarbeiten k\u00f6nnen.</p> <p>Ein Beispiel ist das One-Hot-Encoding, bei dem jede Kategorie in eine bin\u00e4re Spalte umgewandelt wird. Wenn es zum Beispiel drei Farben gibt (\u201erot\u201c, \u201eblau\u201c, \u201egr\u00fcn\u201c), werden daraus drei Spalten: \u201erot\u201c, \u201eblau\u201c und \u201egr\u00fcn\u201c, in denen jede Zeile entweder eine 1 (aktiv) oder 0 (nicht aktiv) enth\u00e4lt.</p> <p>Beispiel in Python:</p> <pre><code>import pandas as pd\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Beispiel-Daten\nfarben = pd.DataFrame({'Farbe': ['rot', 'blau', 'gr\u00fcn', 'rot']})\n\n# One-Hot-Encoding\nencoder = OneHotEncoder(sparse=False)\nencoded = encoder.fit_transform(farben[['Farbe']])\n\n# Ergebnis anzeigen\nencoded_df = pd.DataFrame(encoded, columns=encoder.get_feature_names_out())\nprint(encoded_df)\n</code></pre> <p>Wann brauche ich das? - Wenn dein Datensatz kategorische Variablen enth\u00e4lt. - Besonders wichtig bei Algorithmen wie linearen Modellen, die nicht mit Kategorien arbeiten k\u00f6nnen.</p>"},{"location":"week3/feature_engineering/feature_extraction/","title":"Feature Extraction","text":"<p>Feature Extraction bezeichnet die Ableitung relevanter Informationen aus komplexen Daten wie Text, Bildern oder Zeitreihen. Ziel ist es, die wichtigsten Eigenschaften (Features) aus den Rohdaten zu gewinnen, um die Komplexit\u00e4t zu reduzieren.</p> <p>Ein Beispiel ist das Extrahieren von Textl\u00e4nge aus einem Textfeld oder die Frequenzanalyse in Zeitreihendaten.</p> <p>Beispiel in Python:</p> <pre><code># Beispiel: Textl\u00e4nge berechnen\nimport pandas as pd\n\n# Beispiel-Daten\ntexte = pd.DataFrame({'Text': ['Hallo Welt', 'Data Science ist spannend', 'Python macht Spa\u00df']})\n\n# Feature: Textl\u00e4nge\ntexte['Textl\u00e4nge'] = texte['Text'].apply(len)\nprint(texte)\n</code></pre> <p>Wann brauche ich das? - Wenn deine Rohdaten unstrukturiert sind (z. B. Text, Bilder). - Um spezifische Eigenschaften zu extrahieren, die f\u00fcr Modelle relevant sind.</p>"},{"location":"week3/feature_engineering/new_features/","title":"Erstellung neuer Features","text":"<p>Die Erstellung neuer Features (Feature Engineering) ist der Prozess, aus vorhandenen Daten neue, aussagekr\u00e4ftige Variablen zu generieren. Ziel ist es, die Beziehung zwischen Features und Zielvariablen zu verbessern.</p> <p>Ein Beispiel ist die Berechnung von Geschwindigkeiten aus Distanz- und Zeitdaten oder die Aggregation von Transaktionsdaten zu monatlichen Summen.</p> <p>Beispiel in Python:</p> <pre><code># Beispiel: Geschwindigkeit berechnen\nimport pandas as pd\n\n# Beispiel-Daten\ndaten = pd.DataFrame({'Distanz_km': [100, 200, 150], 'Zeit_h': [2, 4, 3]})\n\n# Neues Feature: Geschwindigkeit\ndaten['Geschwindigkeit_kmh'] = daten['Distanz_km'] / daten['Zeit_h']\nprint(daten)\n</code></pre> <p>Wann brauche ich das? - Wenn du die Leistung deiner Modelle verbessern m\u00f6chtest. - Um verborgene Zusammenh\u00e4nge in den Daten aufzudecken.</p>"},{"location":"week3/feature_engineering/scaling/","title":"Scaling","text":"<p>Scaling (Skalierung) wird verwendet, um numerische Features in einen einheitlichen Wertebereich zu bringen. Dies ist besonders wichtig f\u00fcr Algorithmen, die empfindlich auf die Skala der Eingabedaten reagieren, wie z. B. lineare Regression oder k-Nearest Neighbors.</p> <p>Die zwei h\u00e4ufigsten Methoden sind: - Standardisierung: Zentriert die Daten auf Mittelwert 0 und Standardabweichung 1. - Min-Max-Skalierung: Skaliert Werte in einen Bereich zwischen 0 und 1.</p> <p>Beispiel in Python:</p> <pre><code>import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Beispiel-Daten\ndaten = pd.DataFrame({'Alter': [25, 35, 45, 55], 'Einkommen': [40000, 50000, 60000, 80000]})\n\n# Min-Max-Skalierung\nscaler = MinMaxScaler()\nskalierte_daten = scaler.fit_transform(daten)\n\n# Ergebnis anzeigen\nskalierte_df = pd.DataFrame(skalierte_daten, columns=['Alter', 'Einkommen'])\nprint(skalierte_df)\n</code></pre> <p>Wann brauche ich das? - Wenn deine Features unterschiedliche Skalen haben (z. B. Alter in Jahren und Einkommen in Euro). - Besonders wichtig bei Algorithmen wie k-Means, kNN oder neuronalen Netzen.</p>"},{"location":"week3/feature_engineering/transformation/","title":"Transformation","text":"<p>Transformationen werden genutzt, um die Verteilung der Daten zu \u00e4ndern, z. B. um Schiefe zu korrigieren oder lineare Zusammenh\u00e4nge zu verst\u00e4rken. Eine g\u00e4ngige Methode ist die logarithmische Transformation, die oft bei schiefen Daten angewendet wird.</p> <p>Beispiel in Python:</p> <pre><code>import numpy as np\nimport pandas as pd\n\n# Beispiel-Daten\nwerte = pd.DataFrame({'Umsatz': [100, 500, 1000, 5000, 10000]})\n\n# Log-Transformation\nwerte['Log-Umsatz'] = np.log(werte['Umsatz'])\nprint(werte)\n</code></pre> <p>Wann brauche ich das? - Wenn deine Daten stark schief verteilt sind. - Wenn du exponentielle Trends oder Skaleneffekte linear darstellen m\u00f6chtest.</p>"}]}